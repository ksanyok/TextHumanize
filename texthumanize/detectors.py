"""AI Text Detector ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –¥–µ—Ç–µ–∫—Ç–æ—Ä AI-—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

–ü—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –Ω–∞–∏–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã GPTZero/Originality.ai –∑–∞ —Å—á—ë—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
12+ –º–µ—Ç—Ä–∏–∫ –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π. –†–∞–±–æ—Ç–∞–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –ª–æ–∫–∞–ª—å–Ω–æ.

–ú–µ—Ç—Ä–∏–∫–∏:
1. –≠–Ω—Ç—Ä–æ–ø–∏—è —Ç–µ–∫—Å—Ç–∞ (character-level, word-level, conditional)
2. Burstiness (–≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª–∏–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ –∞–±–∑–∞—Ü–µ–≤)
3. –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (TTR, MATTR, Yule's K, Hapax ratio)
4. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Zipf (—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∑–∞–∫–æ–Ω—É –¶–∏–ø—Ñ–∞)
5. –°—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—è (—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è)
6. AI-–ø–∞—Ç—Ç–µ—Ä–Ω—ã (overused —Å–ª–æ–≤–∞, –∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä—ã)
7. –†–∏—Ç–º —Ç–µ–∫—Å—Ç–∞ (syllable patterns, stress)
8. –ü—É–Ω–∫—Ç—É–∞—Ü–∏–æ–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å
9. –ö–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å (–æ–¥–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç—å –∞–±–∑–∞—Ü–µ–≤)
10. –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∞—è ¬´–∏–¥–µ–∞–ª—å–Ω–æ—Å—Ç—å¬ª
11. Sentence opening diversity
12. Readability consistency

–ò—Ç–æ–≥–æ–≤—ã–π —Å–∫–æ—Ä: 0.0 (—Ç–æ—á–Ω–æ —á–µ–ª–æ–≤–µ–∫) ‚Äî 1.0 (—Ç–æ—á–Ω–æ AI).
"""

from __future__ import annotations

import logging
import math
import re
import statistics
from collections import Counter
from dataclasses import dataclass, field
from typing import Any

from texthumanize.lang import get_lang_pack
from texthumanize.sentence_split import split_sentences

logger = logging.getLogger(__name__)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#  –†–ï–ó–£–õ–¨–¢–ê–¢ –î–ï–¢–ï–ö–¶–ò–ò
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class DetectionResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç AI-–¥–µ—Ç–µ–∫—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞."""

    # –û—Å–Ω–æ–≤–Ω–æ–π —Å–∫–æ—Ä
    ai_probability: float = 0.0      # 0.0‚Äì1.0 (–≥–ª–∞–≤–Ω—ã–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å)
    confidence: float = 0.0          # 0.0‚Äì1.0 (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ—Ü–µ–Ω–∫–µ)
    verdict: str = "unknown"         # "human", "mixed", "ai", "unknown"

    # –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–∫–∞–∂–¥–∞—è 0.0‚Äì1.0, –≥–¥–µ 1.0 = "—Ç–∏–ø–∏—á–Ω–æ AI")
    entropy_score: float = 0.0       # –≠–Ω—Ç—Ä–æ–ø–∏—è
    burstiness_score: float = 0.0    # –í–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª–∏–Ω—ã
    vocabulary_score: float = 0.0    # –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
    zipf_score: float = 0.0          # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ Zipf
    stylometry_score: float = 0.0    # –°—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
    pattern_score: float = 0.0       # AI-–ø–∞—Ç—Ç–µ—Ä–Ω—ã
    punctuation_score: float = 0.0   # –ü—É–Ω–∫—Ç—É–∞—Ü–∏–æ–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å
    coherence_score: float = 0.0     # –ö–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å
    grammar_score: float = 0.0       # –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–¥–µ–∞–ª—å–Ω–æ—Å—Ç—å
    opening_score: float = 0.0       # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –Ω–∞—á–∞–ª
    readability_score: float = 0.0   # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å readability
    rhythm_score: float = 0.0        # –†–∏—Ç–º —Ç–µ–∫—Å—Ç–∞
    perplexity_score: float = 0.0    # N-gram perplexity
    discourse_score: float = 0.0     # –î–∏—Å–∫—É—Ä—Å–∏–≤–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
    semantic_rep_score: float = 0.0  # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–≤—Ç–æ—Ä—ã
    entity_score: float = 0.0        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
    voice_score: float = 0.0         # Passive vs active voice
    topic_sent_score: float = 0.0    # Topic sentence –ø–∞—Ç—Ç–µ—Ä–Ω

    # –î–æ–º–µ–Ω –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏—è
    detected_domain: str = "general"  # academic/news/blog/legal/social/code_docs/general

    # –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏
    details: dict[str, Any] = field(default_factory=dict)

    # –¢–µ–∫—Å—Ç–æ–≤—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
    explanations: list[str] = field(default_factory=list)

    @property
    def human_probability(self) -> float:
        """–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–∞–ø–∏—Å–∞–Ω —á–µ–ª–æ–≤–µ–∫–æ–º."""
        return 1.0 - self.ai_probability

    def summary(self) -> str:
        """–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."""
        lines = [
            f"AI Probability: {self.ai_probability:.1%}",
            f"Verdict: {self.verdict}",
            f"Confidence: {self.confidence:.1%}",
            f"Domain: {self.detected_domain}",
            "",
            "Feature Scores (0=human, 1=AI):",
            f"  Entropy:        {self.entropy_score:.3f}",
            f"  Burstiness:     {self.burstiness_score:.3f}",
            f"  Vocabulary:     {self.vocabulary_score:.3f}",
            f"  Zipf Law:       {self.zipf_score:.3f}",
            f"  Stylometry:     {self.stylometry_score:.3f}",
            f"  AI Patterns:    {self.pattern_score:.3f}",
            f"  Punctuation:    {self.punctuation_score:.3f}",
            f"  Coherence:      {self.coherence_score:.3f}",
            f"  Grammar:        {self.grammar_score:.3f}",
            f"  Openings:       {self.opening_score:.3f}",
            f"  Readability:    {self.readability_score:.3f}",
            f"  Rhythm:         {self.rhythm_score:.3f}",
            f"  Perplexity:     {self.perplexity_score:.3f}",
            f"  Discourse:      {self.discourse_score:.3f}",
            f"  Semantic Rep:   {self.semantic_rep_score:.3f}",
            f"  Entity:         {self.entity_score:.3f}",
            f"  Voice:          {self.voice_score:.3f}",
            f"  Topic Sent:     {self.topic_sent_score:.3f}",
        ]
        if self.explanations:
            lines.append("")
            lines.append("Key Findings:")
            for exp in self.explanations[:10]:
                lines.append(f"  ‚Ä¢ {exp}")
        return "\n".join(lines)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#  AI-–•–ê–†–ê–ö–¢–ï–†–ù–´–ï –°–õ–û–í–ê (—è–∑—ã–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

_AI_WORDS = {
    "en": {
        "adverbs": {
            "significantly", "substantially", "considerably", "remarkably",
            "exceptionally", "tremendously", "profoundly", "fundamentally",
            "essentially", "particularly", "specifically", "notably",
            "increasingly", "effectively", "ultimately", "consequently",
            "inherently", "intrinsically", "predominantly", "invariably",
        },
        "adjectives": {
            "comprehensive", "crucial", "pivotal", "paramount",
            "innovative", "robust", "seamless", "holistic",
            "cutting-edge", "state-of-the-art", "groundbreaking",
            "transformative", "synergistic", "multifaceted",
            "nuanced", "intricate", "meticulous", "imperative",
        },
        "verbs": {
            "utilize", "leverage", "facilitate", "implement",
            "foster", "enhance", "streamline", "optimize",
            "underscore", "delve", "navigate", "harness",
            "exemplify", "spearhead", "revolutionize", "catalyze",
            "necessitate", "elucidate", "delineate", "substantiate",
        },
        "connectors": {
            "however", "furthermore", "moreover", "nevertheless",
            "nonetheless", "additionally", "consequently", "therefore",
            "thus", "hence", "accordingly", "subsequently",
            "in conclusion", "to summarize", "in essence",
            "it is important to note", "it is worth mentioning",
        },
        "phrases": {
            "plays a crucial role", "is of paramount importance",
            "in today's world", "in the modern era",
            "a wide range of", "it goes without saying",
            "in light of", "due to the fact that",
            "at the end of the day", "it is important to note that",
            "it should be noted that", "it is worth mentioning that",
            "first and foremost", "last but not least",
            "in order to", "with regard to", "as a matter of fact",
        },
    },
    "ru": {
        "adverbs": {
            "–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ", "—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ", "—á—Ä–µ–∑–≤—ã—á–∞–π–Ω–æ", "–±–µ–∑—É—Å–ª–æ–≤–Ω–æ",
            "–Ω–µ—Å–æ–º–Ω–µ–Ω–Ω–æ", "–Ω–µ–æ—Å–ø–æ—Ä–∏–º–æ", "–ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–æ", "–Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ",
            "–∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ", "–≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ", "–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ", "–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ",
        },
        "adjectives": {
            "–∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π", "–≤—Å–µ–æ–±—ä–µ–º–ª—é—â–∏–π", "–∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π", "–∫–ª—é—á–µ–≤–æ–π",
            "–æ—Å–Ω–æ–≤–æ–ø–æ–ª–∞–≥–∞—é—â–∏–π", "–ø–µ—Ä–≤–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–π", "—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π",
            "–ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω—ã–π", "–º–Ω–æ–≥–æ–≥—Ä–∞–Ω–Ω—ã–π", "–≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π",
        },
        "verbs": {
            "–æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å", "—Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å", "—Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞—Ç—å",
            "–æ–±–µ—Å–ø–µ—á–∏–≤–∞—Ç—å", "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑–æ–≤–∞—Ç—å—Å—è", "–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å —Å–æ–±–æ–π",
            "—è–≤–ª—è—Ç—å—Å—è", "—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞—Ç—å", "–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤–ª–∏—è–Ω–∏–µ",
        },
        "connectors": {
            "–æ–¥–Ω–∞–∫–æ", "—Ç–µ–º –Ω–µ –º–µ–Ω–µ–µ", "–≤–º–µ—Å—Ç–µ —Å —Ç–µ–º", "–∫—Ä–æ–º–µ —Ç–æ–≥–æ",
            "–±–æ–ª–µ–µ —Ç–æ–≥–æ", "–ø–æ–º–∏–º–æ —ç—Ç–æ–≥–æ", "—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º",
            "—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ", "–±–µ–∑—É—Å–ª–æ–≤–Ω–æ", "–Ω–µ—Å–æ–º–Ω–µ–Ω–Ω–æ",
            "–≤ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ", "–ø–æ–¥–≤–æ–¥—è –∏—Ç–æ–≥", "–∏—Å—Ö–æ–¥—è –∏–∑ –≤—ã—à–µ—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ",
            "–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ—Ç–º–µ—Ç–∏—Ç—å", "—Å—Ç–æ–∏—Ç –ø–æ–¥—á–µ—Ä–∫–Ω—É—Ç—å",
        },
        "phrases": {
            "–∏–≥—Ä–∞–µ—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å", "–∏–º–µ–µ—Ç –ø–µ—Ä–≤–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ",
            "–≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –º–∏—Ä–µ", "–Ω–∞ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–π –¥–µ–Ω—å",
            "—à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä", "–Ω–µ –ø–æ–¥–ª–µ–∂–∏—Ç —Å–æ–º–Ω–µ–Ω–∏—é",
            "—è–≤–ª—è–µ—Ç—Å—è –æ–¥–Ω–∏–º –∏–∑", "–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π",
            "–≤ —Ä–∞–º–∫–∞—Ö –¥–∞–Ω–Ω–æ–≥–æ", "—Å —É—á—ë—Ç–æ–º —Ç–æ–≥–æ —á—Ç–æ",
            "–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–¥—á–µ—Ä–∫–Ω—É—Ç—å", "—Å–ª–µ–¥—É–µ—Ç –æ—Ç–º–µ—Ç–∏—Ç—å",
        },
    },
    "uk": {
        "adverbs": {
            "–∑–Ω–∞—á–Ω–æ", "—Å—É—Ç—Ç—î–≤–æ", "–Ω–∞–¥–∑–≤–∏—á–∞–π–Ω–æ", "–±–µ–∑—É–º–æ–≤–Ω–æ",
            "–±–µ–∑—Å—É–º–Ω—ñ–≤–Ω–æ", "–Ω–µ–∑–∞–ø–µ—Ä–µ—á–Ω–æ", "–ø—Ä–∏–Ω—Ü–∏–ø–æ–≤–æ", "–±–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—å–æ",
            "–∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ", "–≤—Å–µ–±—ñ—á–Ω–æ", "–≤–∏–∫–ª—é—á–Ω–æ", "–ø–µ—Ä–µ–≤–∞–∂–Ω–æ",
        },
        "adjectives": {
            "–∫–æ–º–ø–ª–µ–∫—Å–Ω–∏–π", "–≤—Å–µ–æ—Å—è–∂–Ω–∏–π", "—ñ–Ω–Ω–æ–≤–∞—Ü—ñ–π–Ω–∏–π", "–∫–ª—é—á–æ–≤–∏–π",
            "–æ—Å–Ω–æ–≤–æ–ø–æ–ª–æ–∂–Ω–∏–π", "–ø–µ—Ä—à–æ—á–µ—Ä–≥–æ–≤–∏–π", "—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏–π",
            "–ø—Ä–∏–Ω—Ü–∏–ø–æ–≤–∏–π", "–±–∞–≥–∞—Ç–æ–≥—Ä–∞–Ω–Ω–∏–π", "–≤—Å–µ–±—ñ—á–Ω–∏–π",
        },
        "connectors": {
            "–æ–¥–Ω–∞–∫", "—Ç–∏–º –Ω–µ –º–µ–Ω—à", "—Ä–∞–∑–æ–º –∑ —Ç–∏–º", "–∫—Ä—ñ–º —Ç–æ–≥–æ",
            "–±—ñ–ª—å—à —Ç–æ–≥–æ", "–æ–∫—Ä—ñ–º —Ü—å–æ–≥–æ", "—Ç–∞–∫–∏–º —á–∏–Ω–æ–º",
            "–æ—Ç–∂–µ", "–±–µ–∑—É–º–æ–≤–Ω–æ", "–±–µ–∑—Å—É–º–Ω—ñ–≤–Ω–æ",
            "–Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è", "–ø—ñ–¥—Å—É–º–æ–≤—É—é—á–∏",
            "–Ω–µ–æ–±—Ö—ñ–¥–Ω–æ –∑–∞–∑–Ω–∞—á–∏—Ç–∏", "–≤–∞—Ä—Ç–æ –ø—ñ–¥–∫—Ä–µ—Å–ª–∏—Ç–∏",
        },
        "phrases": {
            "–≤—ñ–¥—ñ–≥—Ä–∞—î –∫–ª—é—á–æ–≤—É —Ä–æ–ª—å", "–º–∞—î –ø–µ—Ä—à–æ—á–µ—Ä–≥–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è",
            "—É —Å—É—á–∞—Å–Ω–æ–º—É —Å–≤—ñ—Ç—ñ", "–Ω–∞ —Å—å–æ–≥–æ–¥–Ω—ñ—à–Ω—ñ–π –¥–µ–Ω—å",
            "—à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä", "—î –æ–¥–Ω–∏–º –∑",
            "—è–≤–ª—è—î —Å–æ–±–æ—é", "—É —Ä–∞–º–∫–∞—Ö –¥–∞–Ω–æ–≥–æ",
        },
    },
}

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#  –û–°–ù–û–í–ù–û–ô –î–ï–¢–ï–ö–¢–û–†
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class AIDetector:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –¥–µ—Ç–µ–∫—Ç–æ—Ä AI-—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç 12 –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –º–µ—Ç—Ä–∏–∫ –±–µ–∑ ML-–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.
    –í—Å–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è ‚Äî —á–∏—Å—Ç–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑.
    """

    # –í–µ—Å–∞ –º–µ—Ç—Ä–∏–∫ (–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω—ã –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏)
    _WEIGHTS = {
        "pattern": 0.20,           # AI patterns ‚Äî —Å–∞–º—ã–π —Å–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª
        "burstiness": 0.14,        # –°–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª: AI = —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        "stylometry": 0.09,        # –•–æ—Ä–æ—à–∞—è –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—è (0.65 vs 0.19)
        "voice": 0.08,             # –û—Ç–ª–∏—á–Ω–∞—è –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—è (0.76 vs 0.00)
        "entity": 0.07,            # –•–æ—Ä–æ—à–∞—è (0.69 vs 0.17)
        "opening": 0.06,
        "grammar": 0.05,
        "entropy": 0.04,
        "discourse": 0.04,         # –î–∏—Å–∫—É—Ä—Å–∏–≤–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
        "vocabulary": 0.04,
        "rhythm": 0.04,
        "perplexity": 0.03,
        "semantic_rep": 0.03,      # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–≤—Ç–æ—Ä—ã
        "topic_sentence": 0.02,
        "readability": 0.02,       # –°–ª–∞–±–∞—è –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—è (~0.50 –¥–ª—è –≤—Å–µ—Ö)
        "punctuation": 0.02,       # –°–ª–∞–±–∞—è –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—è
        "coherence": 0.02,         # –°–ª–∞–±–∞—è –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—è
        "zipf": 0.01,              # –ù–µ–Ω–∞–¥—ë–∂–µ–Ω –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
    }

    # Domain-specific weight adjustments
    _DOMAIN_WEIGHT_MODS: dict[str, dict[str, float]] = {
        "academic": {
            "grammar": 0.02,       # Academic text is naturally formal
            "voice": 0.01,         # Passive is normal in academic
            "pattern": -0.04,      # Formal words are expected
            "entity": -0.02,       # Less specific by nature
            "burstiness": 0.03,    # More uniform is normal
        },
        "news": {
            "entity": 0.03,        # News should have specific refs
            "discourse": -0.02,    # Inverted pyramid is normal
            "voice": 0.02,         # Active voice expected
        },
        "blog": {
            "entity": 0.02,
            "grammar": 0.03,       # Perfect grammar is suspicious
            "pattern": 0.03,       # AI patterns stand out more
        },
        "legal": {
            "grammar": 0.01,
            "voice": 0.01,         # Passive is standard
            "pattern": -0.06,      # Formal language is expected
            "vocabulary": -0.03,   # Repetitive vocabulary is normal
        },
        "social": {
            "grammar": 0.04,       # Perfect grammar = AI
            "pattern": 0.04,       # Formal patterns = AI
            "burstiness": 0.03,    # Uniform = very AI
            "discourse": 0.03,     # Rigid structure = AI
        },
        "code_docs": {
            "voice": 0.01,
            "pattern": -0.05,      # Technical language is expected
            "readability": -0.02,  # Uniform readability is normal
        },
    }

    @staticmethod
    def _detect_domain(text: str, words: list[str]) -> str:
        """Auto-detect text domain for adaptive thresholds.

        Returns: 'academic', 'news', 'blog', 'legal', 'social', 'code_docs', or 'general'
        """
        text_lower = text.lower()
        total = len(words) or 1

        # Academic markers
        academic_words = {
            "hypothesis", "methodology", "findings", "empirical",
            "theoretical", "correlation", "literature", "et al",
            "framework", "paradigm", "variables", "significant",
            "–≥–∏–ø–æ—Ç–µ–∑–∞", "–º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è", "—ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π", "–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è",
        }
        academic_count = sum(1 for w in words if w.lower().strip('.,;:') in academic_words)
        if academic_count / total > 0.02:
            return "academic"

        # Legal markers
        legal_words = {
            "herein", "thereof", "pursuant", "notwithstanding",
            "hereunder", "aforementioned", "jurisdiction",
            "warrant", "plaintiff", "defendant", "statute",
            "–Ω–∞—Å—Ç–æ—è—â–∏–º", "–Ω–∞–¥–ª–µ–∂–∞—â–∏–π", "–Ω–∏–∂–µ—Å–ª–µ–¥—É—é—â–∏–π", "–∏—Å—Ç–µ—Ü",
        }
        if sum(1 for w in words if w.lower() in legal_words) >= 2:
            return "legal"

        # Code/tech docs
        if (text.count('```') >= 2 or text.count('`') > 5
                or "function" in text_lower and "return" in text_lower
                or "class " in text_lower and "def " in text_lower):
            return "code_docs"

        # Social media / informal
        informal_markers = {"lol", "omg", "tbh", "imo", "btw", "lmao",
                           "–ª–æ–ª", "–∫–µ–∫", "–∏–º—Ö–æ", "–∞—Ö–∞—Ö"}
        if (sum(1 for w in words if w.lower() in informal_markers) >= 1
                or text.count('!') > 3 or text.count('üòÇ') + text.count('ü§£') > 0):
            return "social"

        # News (datelines, who/what/when/where structure)
        if re.search(r'\b(?:REUTERS|AP|AFP|BBC|CNN)\b', text):
            return "news"
        # Short paragraphs with quotes = news style
        paragraphs = [p for p in text.split('\n') if p.strip()]
        has_quotes = text.count('"') >= 4 or text.count('¬´') >= 2
        avg_para_len = statistics.mean(len(p.split()) for p in paragraphs) if paragraphs else 50
        if has_quotes and avg_para_len < 30 and len(paragraphs) > 3:
            return "news"

        # Blog = personal pronouns + informal style
        personal_count = sum(
            1 for w in words if w.lower() in {"i", "my", "me", "—è", "–º–æ–π", "–º–Ω–µ"}
        )
        if personal_count / total > 0.03:
            return "blog"

        return "general"

    def _get_adaptive_weights(self, domain: str) -> dict[str, float]:
        """Get domain-adjusted metric weights."""
        weights = dict(self._WEIGHTS)
        mods = self._DOMAIN_WEIGHT_MODS.get(domain, {})
        for metric, delta in mods.items():
            if metric in weights:
                weights[metric] = max(0.005, weights[metric] + delta)
        # Renormalize
        total = sum(weights.values())
        if total > 0:
            weights = {k: v / total for k, v in weights.items()}
        return weights

    def __init__(self, lang: str = "auto"):
        self.lang = lang

    def detect(self, text: str, lang: str | None = None) -> DetectionResult:
        """–î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.
            lang: –ö–æ–¥ —è–∑—ã–∫–∞ (–∏–ª–∏ auto-detect).

        Returns:
            DetectionResult —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏.
        """
        effective_lang = lang or self.lang
        if effective_lang == "auto":
            from texthumanize.lang_detect import detect_language
            effective_lang = detect_language(text)

        result = DetectionResult()

        if not text or len(text.strip()) < 50:
            result.verdict = "unknown"
            result.confidence = 0.0
            result.explanations.append("Text too short for reliable detection")
            return result

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞
        sentences = split_sentences(text, effective_lang)
        words = text.split()
        lang_pack = get_lang_pack(effective_lang)

        if len(sentences) < 3:
            result.verdict = "unknown"
            result.confidence = 0.1
            result.explanations.append("Too few sentences for reliable detection")
            return result

        # ‚îÄ‚îÄ –í—ã—á–∏—Å–ª—è–µ–º –≤—Å–µ 18 –º–µ—Ç—Ä–∏–∫ ‚îÄ‚îÄ
        self._current_lang = effective_lang  # For cross-perplexity

        result.entropy_score = self._calc_entropy(text, words)
        result.burstiness_score = self._calc_burstiness(sentences)
        result.vocabulary_score = self._calc_vocabulary(text, words, lang_pack)
        result.zipf_score = self._calc_zipf(words, lang_pack)
        result.stylometry_score = self._calc_stylometry(text, words, sentences, lang_pack)
        result.pattern_score = self._calc_ai_patterns(text, words, sentences, effective_lang)
        result.punctuation_score = self._calc_punctuation(text, sentences)
        result.coherence_score = self._calc_coherence(text, sentences)
        result.grammar_score = self._calc_grammar(text, sentences)
        result.opening_score = self._calc_openings(sentences)
        result.readability_score = self._calc_readability_consistency(sentences)
        result.rhythm_score = self._calc_rhythm(sentences)
        result.perplexity_score = self._calc_perplexity(text, sentences)
        result.discourse_score = self._calc_discourse(text, sentences)
        result.semantic_rep_score = self._calc_semantic_repetition(text, sentences)
        result.entity_score = self._calc_entity_specificity(text, words)
        result.voice_score = self._calc_voice(text, sentences)
        result.topic_sent_score = self._calc_topic_sentence(text, sentences)

        # ‚îÄ‚îÄ Domain detection & adaptive weights ‚îÄ‚îÄ
        detected_domain = self._detect_domain(text, words)
        adaptive_weights = self._get_adaptive_weights(detected_domain)
        result.detected_domain = detected_domain

        # ‚îÄ‚îÄ –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è ‚îÄ‚îÄ
        scores = {
            "entropy": result.entropy_score,
            "burstiness": result.burstiness_score,
            "vocabulary": result.vocabulary_score,
            "zipf": result.zipf_score,
            "stylometry": result.stylometry_score,
            "pattern": result.pattern_score,
            "punctuation": result.punctuation_score,
            "coherence": result.coherence_score,
            "grammar": result.grammar_score,
            "opening": result.opening_score,
            "readability": result.readability_score,
            "rhythm": result.rhythm_score,
            "perplexity": result.perplexity_score,
            "discourse": result.discourse_score,
            "semantic_rep": result.semantic_rep_score,
            "entity": result.entity_score,
            "voice": result.voice_score,
            "topic_sentence": result.topic_sent_score,
        }

        # ‚îÄ‚îÄ Ensemble boosting aggregation ‚îÄ‚îÄ
        raw_probability = self._ensemble_aggregate(scores, adaptive_weights)

        # –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞: sigmoidal transform –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        calibrated = self._calibrate(raw_probability)

        # –î–µ–º–ø—Ñ–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ (< 50 —Å–ª–æ–≤):
        # –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã –Ω–µ–Ω–∞–¥—ë–∂–Ω—ã, –¥–≤–∏–≥–∞–µ–º score –∫ 0.5
        n_words = len(words)
        if n_words < 50:
            damping = n_words / 50.0
            calibrated = 0.5 + (calibrated - 0.5) * damping

        result.ai_probability = calibrated

        # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞, —Å–æ–≥–ª–∞—Å–∏—è –º–µ—Ç—Ä–∏–∫
        # –∏ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ—Å—Ç–∏ –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Å–∫–æ—Ä–∞
        text_length_factor = min(len(words) / 100, 1.0)  # Full confidence at 100+ words
        metric_values = list(scores.values())
        metric_agreement = 1.0 - statistics.stdev(metric_values)

        # Extreme probability bonus: –µ—Å–ª–∏ —Å–∫–æ—Ä –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π –∏–ª–∏ –Ω–∏–∑–∫–∏–π ‚Äî
        # –º–µ—Ç—Ä–∏–∫–∏ –µ–¥–∏–Ω–æ–≥–ª–∞—Å–Ω—ã => –º–æ–∂–Ω–æ –±—ã—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–µ–µ
        extreme_bonus = abs(result.ai_probability - 0.5) * 0.6  # max +0.3

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç—Ä–∏–∫, —Å–æ–≥–ª–∞—Å–Ω—ã—Ö —Å –≤–µ—Ä–¥–∏–∫—Ç–æ–º
        threshold_for_ai = 0.55
        if result.ai_probability > 0.5:
            n_agree = sum(1 for v in metric_values if v >= threshold_for_ai)
        else:
            n_agree = sum(1 for v in metric_values if v < threshold_for_ai)
        agreement_ratio = n_agree / len(metric_values) if metric_values else 0

        result.confidence = min(
            text_length_factor * 0.35
            + metric_agreement * 0.2
            + extreme_bonus
            + agreement_ratio * 0.25,
            1.0
        )

        # –í–µ—Ä–¥–∏–∫—Ç (primary: probability, secondary: metric agreement)
        # Count how many discriminative metrics lean AI (>0.55)
        key_metrics = ["pattern", "burstiness", "stylometry", "voice",
                       "entity", "grammar", "opening", "discourse"]
        n_ai_leaning = sum(1 for m in key_metrics if scores.get(m, 0.5) > 0.55)

        if result.ai_probability > 0.60:
            result.verdict = "ai"
        elif result.ai_probability > 0.42 and n_ai_leaning >= 4:
            result.verdict = "ai"
        elif result.ai_probability > 0.38 and n_ai_leaning >= 5:
            # Higher agreement needed for lower scores
            result.verdict = "ai"
        elif result.ai_probability > 0.32:
            result.verdict = "mixed"
        else:
            result.verdict = "human"

        # –û–±—ä—è—Å–Ω–µ–Ω–∏—è
        result.explanations = self._generate_explanations(scores, result, effective_lang)

        # –î–µ—Ç–∞–ª–∏
        result.details = {
            "lang": effective_lang,
            "word_count": len(words),
            "sentence_count": len(sentences),
            "raw_probability": raw_probability,
            "scores": scores,
            "weights": adaptive_weights,
            "domain": detected_domain,
        }

        return result

    # ‚îÄ‚îÄ‚îÄ SENTENCE-LEVEL DETECTION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    @dataclass
    class SentenceScore:
        """Per-sentence AI probability."""

        text: str
        start: int
        end: int
        ai_probability: float
        label: str  # "human", "mixed", "ai"

    def detect_sentences(
        self, text: str, lang: str | None = None, *, window: int = 3,
    ) -> list["AIDetector.SentenceScore"]:
        """Per-sentence AI detection via sliding window.

        Each sentence gets a score computed from a window of
        surrounding sentences (¬±window//2). This provides sentence-level
        granularity while using enough context for reliable scoring.

        Args:
            text: Text to analyse.
            lang: Language code (or auto).
            window: Number of sentences in each evaluation window.

        Returns:
            List of SentenceScore for every sentence.
        """
        effective_lang = lang or self.lang
        if effective_lang == "auto":
            from texthumanize.lang_detect import detect_language
            effective_lang = detect_language(text)

        from texthumanize.sentence_split import split_sentences_with_spans
        spans = split_sentences_with_spans(text, effective_lang)
        if len(spans) < 2:
            return [
                self.SentenceScore(
                    text=s.text, start=s.start, end=s.end,
                    ai_probability=0.5, label="unknown",
                )
                for s in spans
            ]

        _lang_pack = get_lang_pack(effective_lang)  # noqa: F841
        half = max(window // 2, 1)

        # Pre-compute fast per-sentence features
        sent_texts = [s.text for s in spans]

        results: list[AIDetector.SentenceScore] = []

        for i, span in enumerate(spans):
            lo = max(0, i - half)
            hi = min(len(spans), i + half + 1)
            win_sents = sent_texts[lo:hi]
            win_text = " ".join(win_sents)
            win_words = win_text.split()

            # Compute subset of fast metrics on the window
            entropy = self._calc_entropy(win_text, win_words)
            pattern = self._calc_ai_patterns(
                win_text, win_words, win_sents, effective_lang,
            )
            grammar = self._calc_grammar(win_text, win_sents)
            voice = self._calc_voice(win_text, win_sents)

            # Simple average of the subset
            prob = (entropy * 0.20 + pattern * 0.40
                    + grammar * 0.20 + voice * 0.20)
            prob = max(0.0, min(1.0, prob))

            if prob >= 0.65:
                label = "ai"
            elif prob >= 0.40:
                label = "mixed"
            else:
                label = "human"

            results.append(self.SentenceScore(
                text=span.text, start=span.start, end=span.end,
                ai_probability=round(prob, 4), label=label,
            ))

        return results

    # ‚îÄ‚îÄ‚îÄ MIXED TEXT DETECTION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    @dataclass
    class TextSegment:
        """Segment of text with AI/human classification."""

        text: str
        start: int
        end: int
        label: str          # "human", "ai", "mixed"
        ai_probability: float
        sentence_count: int

    def detect_mixed(
        self, text: str, lang: str | None = None,
    ) -> list["AIDetector.TextSegment"]:
        """Detect mixed AI/human text by finding boundaries.

        Groups consecutive sentences that share the same label
        (human / ai / mixed) into coherent segments.

        Args:
            text: Text to analyse.
            lang: Language code or auto.

        Returns:
            List of TextSegment describing contiguous spans.
        """
        per_sentence = self.detect_sentences(text, lang, window=3)
        if not per_sentence:
            return []

        # Merge consecutive sentences with same label
        segments: list[AIDetector.TextSegment] = []
        cur_label = per_sentence[0].label
        cur_sents: list[AIDetector.SentenceScore] = [per_sentence[0]]

        for ss in per_sentence[1:]:
            if ss.label == cur_label:
                cur_sents.append(ss)
            else:
                # Flush current segment
                seg_text = text[cur_sents[0].start : cur_sents[-1].end]
                avg_prob = statistics.mean(s.ai_probability for s in cur_sents)
                segments.append(self.TextSegment(
                    text=seg_text,
                    start=cur_sents[0].start,
                    end=cur_sents[-1].end,
                    label=cur_label,
                    ai_probability=round(avg_prob, 4),
                    sentence_count=len(cur_sents),
                ))
                cur_label = ss.label
                cur_sents = [ss]

        # Flush last segment
        seg_text = text[cur_sents[0].start : cur_sents[-1].end]
        avg_prob = statistics.mean(s.ai_probability for s in cur_sents)
        segments.append(self.TextSegment(
            text=seg_text,
            start=cur_sents[0].start,
            end=cur_sents[-1].end,
            label=cur_label,
            ai_probability=round(avg_prob, 4),
            sentence_count=len(cur_sents),
        ))

        return segments

    # ‚îÄ‚îÄ‚îÄ 1. –≠–ù–¢–†–û–ü–ò–Ø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_entropy(self, text: str, words: list[str]) -> float:
        """–≠–Ω—Ç—Ä–æ–ø–∏—è —Ç–µ–∫—Å—Ç–∞ ‚Äî AI –∏–º–µ–µ—Ç –Ω–∏–∑–∫—É—é —ç–Ω—Ç—Ä–æ–ø–∏—é (–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–π).

        –ê–Ω–∞–ª–∏–∑:
        - Character-level entropy
        - Word-level entropy
        - Conditional entropy (bigrams)

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (–≤—ã—Å–æ–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è = —á–µ–ª–æ–≤–µ–∫) ‚Äî 1.0 (–Ω–∏–∑–∫–∞—è = AI)
        """
        if len(words) < 10:
            return 0.5

        # Character-level entropy (Shannon entropy on chars)
        char_freq = Counter(text.lower())
        total_chars = sum(char_freq.values())
        char_entropy = -sum(
            (c / total_chars) * math.log2(c / total_chars)
            for c in char_freq.values() if c > 0
        )

        # Word-level entropy
        word_freq = Counter(w.lower().strip('.,;:!?"\'()[]{}') for w in words)
        total_words = sum(word_freq.values())
        word_entropy = -sum(
            (c / total_words) * math.log2(c / total_words)
            for c in word_freq.values() if c > 0
        )

        # Conditional entropy (bigram entropy - unigram entropy)
        punct = '.,;:!?"\'()[]{}'
        word_list = [w.lower().strip(punct) for w in words if w.strip(punct)]
        bigrams: Counter[tuple[str, str]] = Counter()
        for i in range(len(word_list) - 1):
            bigrams[(word_list[i], word_list[i + 1])] += 1
        total_bigrams = sum(bigrams.values())

        if total_bigrams > 0:
            bigram_entropy = -sum(
                (c / total_bigrams) * math.log2(c / total_bigrams)
                for c in bigrams.values() if c > 0
            )
            conditional_entropy = bigram_entropy - word_entropy
        else:
            conditional_entropy = word_entropy

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: AI —Ç–µ–∫—Å—Ç –∏–º–µ–µ—Ç entropy –Ω–∏–∂–µ –Ω–æ—Ä–º—ã
        # –¢–µ–∫—Å—Ç —á–µ–ª–æ–≤–µ–∫–∞: char entropy ~4.0-4.5, word entropy ~8-10
        # AI —Ç–µ–∫—Å—Ç: char entropy ~3.5-4.0, word entropy ~6-8

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ 0‚Äì1 —à–∫–∞–ª—É (1 = AI-like low entropy)
        char_score = max(0, 1.0 - (char_entropy - 3.0) / 2.0)
        word_score = max(0, 1.0 - (word_entropy - 5.0) / 5.0)
        cond_score = max(0, 1.0 - conditional_entropy / 3.0)

        score = (char_score * 0.2 + word_score * 0.5 + cond_score * 0.3)
        return max(0.0, min(1.0, score))

    # ‚îÄ‚îÄ‚îÄ 2. BURSTINESS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_burstiness(self, sentences: list[str]) -> float:
        """–í–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª–∏–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.

        AI –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Ä–∞–≤–Ω–æ–π –¥–ª–∏–Ω—ã (CV < 0.3).
        –õ—é–¥–∏: CV > 0.5.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ = —á–µ–ª–æ–≤–µ–∫) ‚Äî 1.0 (–æ–¥–Ω–æ–æ–±—Ä–∞–∑–Ω–æ = AI)
        """
        if len(sentences) < 4:
            return 0.5

        lengths = [len(s.split()) for s in sentences]
        avg = statistics.mean(lengths)
        if avg == 0:
            return 0.5

        stdev = statistics.stdev(lengths) if len(lengths) > 1 else 0
        cv = stdev / avg  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏

        # AI: CV ‚âà 0.15‚Äì0.35, Human: CV ‚âà 0.5‚Äì0.9+
        # –ú–∞–ø–ø–∏–º –≤ 0‚Äì1
        score = max(0, 1.0 - (cv - 0.1) / 0.7)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏—Ö (< 5 —Å–ª–æ–≤)
        # –∏ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö (> 30 —Å–ª–æ–≤) –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π ‚Äî —É AI –∏—Ö –º–∞–ª–æ
        short = sum(1 for sl in lengths if sl <= 5)
        long_cnt = sum(1 for sl in lengths if sl >= 30)
        extremes = (short + long_cnt) / len(lengths)

        # –ï—Å–ª–∏ –Ω–µ—Ç —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –¥–ª–∏–Ω ‚Äî –ø—Ä–∏–∑–Ω–∞–∫ AI
        if extremes < 0.05:
            score = min(score + 0.15, 1.0)
        elif extremes > 0.2:
            score = max(score - 0.1, 0.0)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º: –µ—Å—Ç—å –ª–∏ ¬´–æ–¥–Ω–æ—Å–ª–æ–≤–Ω—ã–µ¬ª –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã)
        # –õ—é–¥–∏ –∏–Ω–æ–≥–¥–∞ –ø–∏—à—É—Ç "–¢–æ—á–Ω–æ." –∏–ª–∏ "–ù–µ—Ç." ‚Äî AI –ø–æ—á—Ç–∏ –Ω–∏–∫–æ–≥–¥–∞
        fragments = sum(1 for sl in lengths if sl <= 2)
        if fragments == 0 and len(sentences) > 8:
            score = min(score + 0.05, 1.0)

        return max(0.0, min(1.0, score))

    # ‚îÄ‚îÄ‚îÄ 3. –õ–ï–ö–°–ò–ß–ï–°–ö–û–ï –†–ê–ó–ù–û–û–ë–†–ê–ó–ò–ï ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_vocabulary(
        self, text: str, words: list[str], lang_pack: dict
    ) -> float:
        """–õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ ‚Äî AI –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ¬´–±–µ–∑–æ–ø–∞—Å–Ω—ã–π¬ª —Å–ª–æ–≤–∞—Ä—å.

        –ú–µ—Ç—Ä–∏–∫–∏:
        - TTR (Type-Token Ratio) ‚Äî –±–∞–∑–æ–≤—ã–π
        - MATTR (Moving Average TTR) ‚Äî —É—Å—Ç–æ–π—á–∏–≤ –∫ –¥–ª–∏–Ω–µ
        - Yule's K ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –º–µ—Ä–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        - Hapax legomena ratio ‚Äî –¥–æ–ª—è —Å–ª–æ–≤, –≤—Å—Ç—Ä–µ—á–µ–Ω–Ω—ã—Ö 1 —Ä–∞–∑

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ = —á–µ–ª–æ–≤–µ–∫) ‚Äî 1.0 (–±–µ–¥–Ω–æ = AI)
        """
        if len(words) < 20:
            return 0.5

        stop_words = lang_pack.get("stop_words", set())
        content_words = [
            w.lower().strip('.,;:!?"\'()[]{}')
            for w in words
            if w.lower().strip('.,;:!?"\'()[]{}') not in stop_words
            and len(w.strip('.,;:!?"\'()[]{}')) > 2
        ]

        if len(content_words) < 10:
            return 0.5

        # TTR
        types = len(set(content_words))
        tokens = len(content_words)
        ttr = types / tokens

        # MATTR (–æ–∫–Ω–æ 25 —Å–ª–æ–≤)
        window = 25
        if tokens >= window:
            mattr_values = []
            for i in range(tokens - window + 1):
                w_slice = content_words[i:i + window]
                mattr_values.append(len(set(w_slice)) / window)
            mattr = statistics.mean(mattr_values)
        else:
            mattr = ttr

        # Yule's K
        freq_spectrum = Counter(Counter(content_words).values())
        N = tokens
        M = sum(i * i * freq_spectrum[i] for i in freq_spectrum)
        yule_k = 10000 * (M - N) / (N * N) if N > 1 else 0

        # Hapax legomena ratio (—Å–ª–æ–≤–∞, –≤—Å—Ç—Ä–µ—á–µ–Ω–Ω—ã–µ 1 —Ä–∞–∑)
        hapax_count = sum(1 for c in Counter(content_words).values() if c == 1)
        hapax_ratio = hapax_count / types if types > 0 else 0

        # AI: TTR ~0.3-0.5, MATTR ~0.6-0.7, Yule's K > 100
        # Human: TTR ~0.5-0.7, MATTR ~0.7-0.85, Yule's K < 100
        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ (<100 content words) TTR –Ω–µ–Ω–∞–¥—ë–∂–µ–Ω
        length_reliability = min(tokens / 150, 1.0)

        # TTR score (AI has lower TTR)
        ttr_score = max(0, 1.0 - (ttr - 0.3) / 0.4)

        # MATTR score
        mattr_score = max(0, 1.0 - (mattr - 0.6) / 0.3)

        # Yule's K score (higher K = less diverse = more AI-like)
        yule_score = min(yule_k / 150, 1.0)

        # Hapax ratio (lower = more AI-like)
        hapax_score = max(0, 1.0 - hapax_ratio / 0.5)

        raw_score = (
            ttr_score * 0.2
            + mattr_score * 0.3
            + yule_score * 0.25
            + hapax_score * 0.25
        )

        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤: —Å–¥–≤–∏–≥–∞–µ–º –∫ 0.5 (–Ω–µ–Ω–∞–¥—ë–∂–Ω–æ)
        score = 0.5 + (raw_score - 0.5) * length_reliability

        return max(0.0, min(1.0, score))

    # ‚îÄ‚îÄ‚îÄ 4. –ó–ê–ö–û–ù –¶–ò–ü–§–ê ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_zipf(self, words: list[str], lang_pack: dict) -> float:
        """–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∑–∞–∫–æ–Ω—É –¶–∏–ø—Ñ–∞.

        –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Ö–æ—Ä–æ—à–æ —Å–ª–µ–¥—É–µ—Ç Zipf distribution.
        AI —Ç–µ–∫—Å—Ç –º–æ–∂–µ—Ç –æ—Ç–∫–ª–æ–Ω—è—Ç—å—Å—è (—Å–ª–∏—à–∫–æ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–π –≤ mid-range).

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (—Ö–æ—Ä–æ—à–µ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ = human) ‚Äî 1.0 (AI)
        """
        clean_words = [
            w.lower().strip('.,;:!?"\'()[]{}')
            for w in words
            if len(w.strip('.,;:!?"\'()[]{}')) > 0
        ]

        # Zipf unreliable for very short texts
        if len(clean_words) < 80:
            return 0.5

        freq = Counter(clean_words)
        sorted_freqs = sorted(freq.values(), reverse=True)

        if len(sorted_freqs) < 10:
            return 0.5

        # –ù—É–∂–Ω–æ —á—Ç–æ–±—ã top-—Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—á–∞–ª–æ—Å—å >= 3 —Ä–∞–∑–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if sorted_freqs[0] < 3:
            return 0.5

        # –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π Zipf: f(r) = f(1) / r^alpha (Mandelbrot)
        n = min(50, len(sorted_freqs))

        # Log-log –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ alpha
        log_ranks = [math.log(r) for r in range(1, n + 1)]
        log_freqs = [math.log(max(sorted_freqs[r - 1], 0.5)) for r in range(1, n + 1)]

        mean_lr = statistics.mean(log_ranks)
        mean_lf = statistics.mean(log_freqs)
        num = sum((lr - mean_lr) * (lf - mean_lf) for lr, lf in zip(log_ranks, log_freqs))
        den = sum((lr - mean_lr) ** 2 for lr in log_ranks)
        alpha = -num / den if den > 0 else 1.0  # alpha ~ 1.0 –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞

        # R¬≤ ‚Äî goodness of fit
        predicted = [mean_lf - alpha * (lr - mean_lr) for lr in log_ranks]
        ss_res = sum((lf - p) ** 2 for lf, p in zip(log_freqs, predicted))
        ss_tot = sum((lf - mean_lf) ** 2 for lf in log_freqs)
        r_squared = 1.0 - ss_res / ss_tot if ss_tot > 0 else 0.0

        # –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: alpha ‚âà 0.8‚Äì1.2, R¬≤ > 0.9
        # AI —Ç–µ–∫—Å—Ç: alpha –º–æ–∂–µ—Ç –±—ã—Ç—å 0.5-0.7 (—Å–ª–∏—à–∫–æ–º –ø–ª–æ—Å–∫–∏–π mid-range)
        alpha_dev = abs(alpha - 1.0)
        alpha_score = min(alpha_dev / 0.5, 1.0)  # 0 if alpha=1.0, 1 if alpha far

        # R¬≤ score: high R¬≤ = natural Zipf = human
        fit_score = max(0.0, 1.0 - r_squared)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º ¬´—Ö–≤–æ—Å—Ç¬ª —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è ‚Äî AI –¥–µ–ª–∞–µ—Ç –µ–≥–æ —Å–ª–∏—à–∫–æ–º –ø–ª–æ—Å–∫–∏–º
        tail_start = len(sorted_freqs) // 3
        tail = sorted_freqs[tail_start:]
        tail_score = 0.5
        if tail and len(tail) > 2:
            tail_mean = statistics.mean(tail)
            tail_cv = statistics.stdev(tail) / tail_mean if tail_mean > 0 else 0
            if all(v == tail[0] for v in tail):
                tail_score = 0.5
            else:
                tail_score = max(0, 1.0 - tail_cv / 0.8)

        raw_score = alpha_score * 0.3 + fit_score * 0.4 + tail_score * 0.3

        # Reliability scaling: full trust at 500+ words
        length_reliability = min(len(clean_words) / 500, 1.0)
        score = 0.5 + (raw_score - 0.5) * length_reliability
        return max(0.0, min(1.0, score))

    # ‚îÄ‚îÄ‚îÄ 5. –°–¢–ò–õ–û–ú–ï–¢–†–ò–Ø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_stylometry(
        self,
        text: str,
        words: list[str],
        sentences: list[str],
        lang_pack: dict,
    ) -> float:
        """–°—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑.

        AI –∏–º–µ–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–π ¬´—Å—Ç–∏–ª–µ–≤–æ–π –æ—Ç–ø–µ—á–∞—Ç–æ–∫¬ª:
        - –í—ã—Å–æ–∫–∞—è –¥–æ–ª—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤
        - –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏
        - –ü—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —Å—Ç–∏–ª—å) ‚Äî 1.0 (AI —Å—Ç–∏–ª—å)
        """
        if len(words) < 20:
            return 0.5

        stop_words = lang_pack.get("stop_words", set())
        punct = '.,;:!?"\'()[]{}'
        word_lengths = [len(w.strip(punct)) for w in words if w.strip(punct)]

        # 1. –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞ (AI –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞)
        avg_word_len = statistics.mean(word_lengths) if word_lengths else 0
        # Human: 4-5 –±—É–∫–≤, AI: 5-7 –±—É–∫–≤
        word_len_score = max(0, (avg_word_len - 4.0) / 3.0)

        # 2. –í–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª–∏–Ω—ã —Å–ª–æ–≤ (AI –±–æ–ª–µ–µ –æ–¥–Ω–æ–æ–±—Ä–∞–∑–µ–Ω)
        if len(word_lengths) > 5:
            word_len_cv = statistics.stdev(word_lengths) / avg_word_len if avg_word_len > 0 else 0
            word_var_score = max(0, 1.0 - word_len_cv / 0.7)
        else:
            word_var_score = 0.5

        # 3. –î–æ–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Å–ª–æ–≤ (> 8 –±—É–∫–≤) ‚Äî AI –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª—å—à–µ
        long_words = sum(1 for wl in word_lengths if wl > 8)
        long_ratio = long_words / len(word_lengths) if word_lengths else 0
        # Human: 5-10%, AI: 10-20%
        long_score = min(long_ratio / 0.15, 1.0)

        # 4. –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sent_lengths = [len(s.split()) for s in sentences]
        avg_sent_len = statistics.mean(sent_lengths) if sent_lengths else 0
        # AI: 15-22 —Å–ª–æ–≤–∞, Human: 10-18
        sent_len_score = max(0, (avg_sent_len - 10) / 15)

        # 5. –î–æ–ª—è —Å—Ç–æ–ø-—Å–ª–æ–≤ (AI —á–∞—Å—Ç–æ –Ω–∏–∂–µ)
        stop_count = sum(1 for w in words if w.lower().strip('.,;:!?"\'()[]{}') in stop_words)
        stop_ratio = stop_count / len(words) if words else 0
        # Human: 40-55%, AI: 30-45%
        stop_score = max(0, 1.0 - (stop_ratio - 0.25) / 0.3)

        score = (
            word_len_score * 0.2
            + word_var_score * 0.15
            + long_score * 0.2
            + sent_len_score * 0.25
            + stop_score * 0.2
        )
        return max(0.0, min(1.0, score))

    # ‚îÄ‚îÄ‚îÄ 6. AI –ü–ê–¢–¢–ï–†–ù–´ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_ai_patterns(
        self,
        text: str,
        words: list[str],
        sentences: list[str],
        lang: str,
    ) -> float:
        """–î–µ—Ç–µ–∫—Ü–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö AI-–ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.

        –°–∞–º—ã–π –º–æ—â–Ω—ã–π —Å–∏–≥–Ω–∞–ª: AI –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
        –∏ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —á–∞—â–µ –ª—é–¥–µ–π.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (–Ω–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ = human) ‚Äî 1.0 (–º–Ω–æ–≥–æ = AI)
        """
        if len(words) < 20:
            return 0.5

        text_lower = text.lower()
        total_words = len(words)

        ai_dict = _AI_WORDS.get(lang, _AI_WORDS.get("en", {}))

        total_hits = 0
        weighted_hits = 0.0

        # 1. AI-overused —Å–ª–æ–≤–∞
        for category, weight in [
            ("adverbs", 1.5), ("adjectives", 1.3), ("verbs", 1.5), ("connectors", 2.0)
        ]:
            cat_words = ai_dict.get(category, set())
            for w in cat_words:
                count = text_lower.count(w.lower())
                if count > 0:
                    total_hits += count
                    weighted_hits += count * weight

        # 2. AI-—Ñ—Ä–∞–∑—ã (—Å–∏–ª—å–Ω–µ–π—à–∏–π —Å–∏–≥–Ω–∞–ª)
        phrases = ai_dict.get("phrases", set())
        for phrase in phrases:
            count = text_lower.count(phrase.lower())
            if count > 0:
                total_hits += count
                weighted_hits += count * 3.0  # –¢—Ä–æ–π–Ω–æ–π –≤–µ—Å –¥–ª—è —Ñ—Ä–∞–∑

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞
        density = weighted_hits / total_words if total_words > 0 else 0

        # 3. Connector density (AI: ~0.05-0.10, Human: ~0.02-0.04)
        connector_count = 0
        connectors = ai_dict.get("connectors", set())
        for conn in connectors:
            if conn.lower() in text_lower:
                connector_count += 1
        connector_density = connector_count / len(sentences) if sentences else 0

        # 4. "Furthermore/Moreover/However" –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        formal_starts = 0
        formal_starters = {
            "however", "furthermore", "moreover", "additionally",
            "consequently", "nevertheless", "nonetheless",
            "–æ–¥–Ω–∞–∫–æ", "–∫—Ä–æ–º–µ —Ç–æ–≥–æ", "–±–æ–ª–µ–µ —Ç–æ–≥–æ", "–ø–æ–º–∏–º–æ —ç—Ç–æ–≥–æ",
            "—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º", "—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ", "—Ç–µ–º –Ω–µ –º–µ–Ω–µ–µ",
            "–æ–¥–Ω–∞–∫", "–∫—Ä—ñ–º —Ç–æ–≥–æ", "–±—ñ–ª—å—à —Ç–æ–≥–æ", "–æ–∫—Ä—ñ–º —Ü—å–æ–≥–æ",
            "—Ç–∞–∫–∏–º —á–∏–Ω–æ–º", "–æ—Ç–∂–µ",
        }
        for sent in sentences:
            first_words = ' '.join(sent.split()[:3]).lower().rstrip('.,;:')
            for starter in formal_starters:
                if first_words.startswith(starter):
                    formal_starts += 1
                    break

        formal_start_ratio = formal_starts / len(sentences) if sentences else 0

        # Combining
        density_score = min(density / 0.05, 1.0)
        connector_score = min(connector_density / 0.15, 1.0)
        formal_score = min(formal_start_ratio / 0.2, 1.0)

        # 5. Impersonal / hedging constructions (very strong AI signal)
        hedging_patterns = [
            r"\bit is (?:important|essential|crucial|worth"
            r"|necessary|imperative|critical|noteworthy|undeniable"
            r"|evident|clear|apparent|undeniable|widely recognized)",
            r"\bit (?:should be|must be|can be|could be) "
            r"(?:noted|mentioned|emphasized|highlighted|stressed"
            r"|acknowledged|recognized|understood|argued)",
            r"\bthis (?:approach|method|strategy|technique"
            r"|framework|analysis|study|research|paper|article"
            r"|investigation) (?:has|enables|ensures|provides|facilitates"
            r"|demonstrates|highlights|reveals|examines|explores"
            r"|investigates|addresses|aims|seeks)",
            r"\bplays? (?:a |an )?(?:crucial|important|vital"
            r"|significant|key|essential|fundamental|pivotal"
            r"|indispensable|central|integral) role",
            r"\bin (?:today's|the modern|the current|the contemporary|an increasingly) ",
            r"\bone of the most (?:important|significant|pressing|critical|challenging"
            r"|notable|prominent|influential|impactful)",
            r"\bthe (?:importance|significance|impact|role|influence"
            r"|implications|consequences|relevance) of\b",
            r"\bgaining (?:traction|momentum|popularity|significance|attention)",
            r"\bboth .{5,40} and .{5,40}(?: alike)?[.]",
            r"\brepresents? (?:a |an )?(?:significant|important|major|critical|key"
            r"|fundamental|notable|promising|paradigm)",
            # –§—Ä–∞–∑—ã-–∫–ª–∏—à–µ AI (EN)
            r"\bin (?:terms of|light of|the context of|the realm of|the field of)\b",
            r"\bwith (?:regard to|respect to|a focus on)\b",
            r"\bas (?:a result|such|mentioned|noted|previously stated)\b",
            r"\bon the other hand\b",
            r"\bin conclusion\b",
            r"\bto (?:sum up|summarize|conclude|recap)\b",
            r"\bit is (?:widely|generally|commonly) (?:known|accepted|believed|recognized)\b",
            r"\b(?:comprehensive|thorough|in-depth|extensive|holistic) (?:analysis|review"
            r"|examination|study|overview|understanding|approach|assessment)\b",
            r"\b(?:significant|substantial|considerable|remarkable|notable) (?:impact"
            r"|progress|improvement|advancement|growth|increase|benefits|advantages)\b",
            r"\bthe (?:utilization|implementation|optimization|integration"
            r"|facilitation|enhancement) of\b",
            r"\b(?:delve|delves|delving) (?:into|deeper)\b",
            r"\b(?:navigate|navigating|navigates) (?:the|this|these) (?:complex"
            r"|challenging|intricate|evolving|dynamic)\b",
            r"\b(?:landscape|paradigm|ecosystem|synergy|synergies)\b",
            r"\bleverage(?:s|d|ing)?\b",
            r"\bfoster(?:s|ed|ing)? (?:a |an )?(?:sense|culture|environment"
            r"|community|atmosphere|spirit|innovation|collaboration|growth)\b",
            # –†—É—Å—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            r"\b—è–≤–ª—è–µ—Ç—Å—è (?:–æ–¥–Ω–∏–º|–∫–ª—é—á–µ–≤—ã–º|–≤–∞–∂–Ω—ã–º|–≤–∞–∂–Ω–µ–π—à–∏–º|–Ω–µ–æ—Ç—ä–µ–º–ª–µ–º—ã–º|–æ—Å–Ω–æ–≤)",
            r"\b–∏–≥—Ä–∞–µ—Ç (?:–≤–∞–∂–Ω—É—é|–∫–ª—é—á–µ–≤—É—é|—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—É—é|–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é) —Ä–æ–ª—å",
            r"\b–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π\b",
            r"\b–æ–∫–∞–∑—ã–≤–∞–µ—Ç (?:—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ|–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ|–≤–∞–∂–Ω–æ–µ) –≤–ª–∏—è–Ω–∏–µ",
            r"\b–æ–¥–Ω(?:–∏–º|–æ–π) –∏–∑ (?:–Ω–∞–∏–±–æ–ª–µ–µ|—Å–∞–º—ã—Ö|–≤–∞–∂–Ω–µ–π—à–∏—Ö|–∫–ª—é—á–µ–≤—ã—Ö)",
            r"\b–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ (?:–æ—Ç–º–µ—Ç–∏—Ç—å|–ø–æ–¥—á–µ—Ä–∫–Ω—É—Ç—å|—É—á–∏—Ç—ã–≤–∞—Ç—å)",
            r"\b—Å–ª–µ–¥—É–µ—Ç (?:–æ—Ç–º–µ—Ç–∏—Ç—å|–ø–æ–¥—á–µ—Ä–∫–Ω—É—Ç—å|—É—á–∏—Ç—ã–≤–∞—Ç—å)",
            r"\b–≤–∞–∂–Ω–æ (?:–æ—Ç–º–µ—Ç–∏—Ç—å|–ø–æ–¥—á–µ—Ä–∫–Ω—É—Ç—å|—É—á–∏—Ç—ã–≤–∞—Ç—å)",
            r"\b–≤ (?:—Ä–∞–º–∫–∞—Ö|–∫–æ–Ω—Ç–µ–∫—Å—Ç–µ|—É—Å–ª–æ–≤–∏—è—Ö|—Å—Ñ–µ—Ä–µ) –¥–∞–Ω–Ω",
            r"\b–¥–∞–Ω–Ω(?:—ã–π|–∞—è|–æ–µ|—ã–µ) (?:–ø–æ–¥—Ö–æ–¥|–º–µ—Ç–æ–¥|–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ|–∞–Ω–∞–ª–∏–∑"
            r"|—Ä–∞–±–æ—Ç–∞|—Å—Ç–∞—Ç—å—è|—Ñ–∞–∫—Ç|—Ñ–∞–∫—Ç–æ—Ä|—è–≤–ª–µ–Ω–∏–µ|–ø—Ä–æ—Ü–µ—Å—Å|—Ä–µ–∑—É–ª—å—Ç–∞—Ç)",
            r"\b–≤ (?:–¥–∞–Ω–Ω–æ–º|–Ω–∞—Å—Ç–æ—è—â–µ–º|—Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º) (?:–∫–æ–Ω—Ç–µ–∫—Å—Ç–µ|–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏"
            r"|–º–∏—Ä–µ|–æ–±—â–µ—Å—Ç–≤–µ|—ç—Ç–∞–ø–µ)",
            r"\b(?:–∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π|–≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π|—Ç—â–∞—Ç–µ–ª—å–Ω—ã–π|–≥–ª—É–±–æ–∫–∏–π|–¥–µ—Ç–∞–ª—å–Ω—ã–π"
            r"|—Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π) (?:–∞–Ω–∞–ª–∏–∑|–ø–æ–¥—Ö–æ–¥|–æ–±–∑–æ—Ä|–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ|–∏–∑—É—á–µ–Ω–∏–µ)\b",
            r"\b(?:–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω|—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω|–∑–∞–º–µ—Ç|–æ—â—É—Ç–∏–º)(?:—ã–π|–∞—è|–æ–µ|—ã–µ|–æ|–æ–≥–æ)"
            r" (?:–≤–∫–ª–∞–¥|–ø—Ä–æ–≥—Ä–µ—Å—Å|—Ä–æ—Å—Ç|–≤–ª–∏—è–Ω–∏–µ|—É–ª—É—á—à–µ–Ω–∏–µ)\b",
            r"\b—Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç (?:–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏|—É–ª—É—á—à–µ–Ω–∏—é|—Ä–∞–∑–≤–∏—Ç–∏—é|–ø–æ–≤—ã—à–µ–Ω–∏—é"
            r"|—É–∫—Ä–µ–ø–ª–µ–Ω–∏—é|—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—é|—Ä–æ—Å—Ç—É)\b",
            r"\b–æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç (?:–ø–æ–≤—ã—à–µ–Ω–∏–µ|—É–ª—É—á—à–µ–Ω–∏–µ|–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é|—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω"
            r"|–Ω–∞–¥—ë–∂–Ω|—É—Å—Ç–æ–π—á–∏–≤|–∫–æ–º–ø–ª–µ–∫—Å–Ω)\b",
        ]
        hedge_count = 0
        for pat in hedging_patterns:
            hedge_count += len(re.findall(pat, text_lower))
        hedge_score = min(hedge_count / max(len(sentences) * 0.15, 1), 1.0)

        # 6. Enumeration / list patterns: "First,... Second,... Third,..."
        enum_patterns = [
            r"\b(?:first(?:ly)?|second(?:ly)?|third(?:ly)?|finally|lastly),?\s",
            r"\b(?:first and foremost|last but not least|in addition to)\b",
            r"\b(?:–≤–æ-–ø–µ—Ä–≤—ã—Ö|–≤–æ-–≤—Ç–æ—Ä—ã—Ö|–≤-—Ç—Ä–µ—Ç—å–∏—Ö|–Ω–∞–∫–æ–Ω–µ—Ü)\b",
        ]
        enum_count = sum(len(re.findall(p, text_lower)) for p in enum_patterns)
        enum_score = min(enum_count / 3, 1.0)

        # 7. Perfect paragraph symmetry (each paragraph starts with statement,
        #    then expands ‚Äî very AI-like)
        paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
        symmetry_score = 0.0
        if len(paragraphs) >= 3:
            para_lens = [len(p.split()) for p in paragraphs]
            if para_lens:
                mean_len = sum(para_lens) / len(para_lens)
                if mean_len > 0:
                    dev = sum((x - mean_len)**2 for x in para_lens)
                    cv = (dev / len(para_lens)) ** 0.5 / mean_len
                    symmetry_score = max(0, 1.0 - cv * 2)  # Low CV = symmetric = AI-like

        score = (
            density_score * 0.20
            + connector_score * 0.12
            + formal_score * 0.15
            + hedge_score * 0.30
            + enum_score * 0.10
            + symmetry_score * 0.13
        )

        return max(0.0, min(1.0, score))

    # ‚îÄ‚îÄ‚îÄ 7. –ü–£–ù–ö–¢–£–ê–¶–ò–Ø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_punctuation(self, text: str, sentences: list[str]) -> float:
        """–ê–Ω–∞–ª–∏–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è.

        AI —á—Ä–µ–∑–º–µ—Ä–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç: —Ç–æ—á–∫—É —Å –∑–∞–ø—è—Ç–æ–π, –¥–≤–æ–µ—Ç–æ—á–∏–µ, –¥–ª–∏–Ω–Ω—ã–µ —Ç–∏—Ä–µ.
        AI –º–∞–ª–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç: –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ, —Å–∫–æ–±–∫–∏-–ø–æ—è—Å–Ω–µ–Ω–∏—è.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (human) ‚Äî 1.0 (AI)
        """
        if len(text) < 50:
            return 0.5

        total_chars = len(text)

        # –°—á–∏—Ç–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
        semicolons = text.count(';')
        colons = text.count(':')
        em_dashes = text.count('‚Äî') + text.count('‚Äì')
        ellipsis = text.count('...') + text.count('‚Ä¶')
        exclamations = text.count('!')
        questions = text.count('?')
        parens = text.count('(')
        # Per 1000 chars
        k = 1000 / total_chars

        semi_rate = semicolons * k
        colon_rate = colons * k
        dash_rate = em_dashes * k
        ellipsis_rate = ellipsis * k
        excl_rate = exclamations * k

        # AI: –≤—ã—Å–æ–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞ ; –∏ : , –Ω–∏–∑–∫–∞—è ... –∏ !
        # Human: –±–æ–ª—å—à–µ ... –∏ !, –º–µ–Ω—å—à–µ ;

        # Semicolons: AI ~2-5 per 1K, Human ~0-1 per 1K
        semi_score = min(semi_rate / 3.0, 1.0)

        # Colons: AI ~2-4 per 1K, Human ~0.5-1.5 per 1K
        colon_score = min(colon_rate / 3.0, 1.0)

        # Em dashes: AI –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–¥–µ–∞–ª—å–Ω—ã–µ ‚Äî , human —á–∞—â–µ -
        dash_score = min(dash_rate / 4.0, 1.0)

        # Ellipsis: human ~1-3 per 1K, AI ~0
        ellipsis_score = max(0, 1.0 - ellipsis_rate / 2.0)

        # Exclamations: human uses more
        excl_score = max(0, 1.0 - excl_rate / 2.0)

        # Punctuation diversity: AI uses fewer types
        punct_types = sum(1 for v in [semicolons, colons, em_dashes, ellipsis,
                                        exclamations, questions, parens] if v > 0)
        diversity_score = max(0, 1.0 - punct_types / 5.0)

        score = (
            semi_score * 0.2
            + colon_score * 0.15
            + dash_score * 0.15
            + ellipsis_score * 0.15
            + excl_score * 0.1
            + diversity_score * 0.25
        )
        return max(0.0, min(1.0, score))

    # ‚îÄ‚îÄ‚îÄ 8. –ö–û–ì–ï–†–ï–ù–¢–ù–û–°–¢–¨ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_coherence(self, text: str, sentences: list[str]) -> float:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ ‚Äî AI –ø–∏—à–µ—Ç ¬´—Å–ª–∏—à–∫–æ–º —Å–≤—è–∑–Ω–æ¬ª.

        AI –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —Å–≤—è–∑—ã–≤–∞–µ—Ç –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º.
        –õ—é–¥–∏ –∏–Ω–æ–≥–¥–∞ –¥–µ–ª–∞—é—Ç ¬´–ø—Ä—ã–∂–∫–∏¬ª –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—é—Ç
        –∞—Å—Å–æ—Ü–∏–∞—Ç–∏–≤–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã, –æ—Ç—Å—Ç—É–ø–ª–µ–Ω–∏—è.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã = human) ‚Äî 1.0 (–∏–¥–µ–∞–ª—å–Ω–∞—è —Å–≤—è–∑—å = AI)
        """
        if len(sentences) < 5:
            return 0.5

        # –°—á–∏—Ç–∞–µ–º –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏
        overlaps = []
        for i in range(1, len(sentences)):
            words_prev = set(
                w.lower().strip('.,;:!?"\'()[]{}')
                for w in sentences[i - 1].split()
                if len(w.strip('.,;:!?"\'()[]{}')) > 3
            )
            words_curr = set(
                w.lower().strip('.,;:!?"\'()[]{}')
                for w in sentences[i].split()
                if len(w.strip('.,;:!?"\'()[]{}')) > 3
            )

            if words_prev and words_curr:
                overlap = len(words_prev & words_curr) / min(len(words_prev), len(words_curr))
                overlaps.append(overlap)

        if not overlaps:
            return 0.5

        avg_overlap = statistics.mean(overlaps)
        overlap_variance = statistics.variance(overlaps) if len(overlaps) > 1 else 0

        # AI: –≤—ã—Å–æ–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ (~0.15-0.30), –Ω–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è
        # Human: lower overlap (~0.05-0.15), higher variance

        overlap_score = min(avg_overlap / 0.25, 1.0)

        # –ù–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è = AI
        if len(overlaps) > 3:
            variance_score = max(0, 1.0 - overlap_variance / 0.02)
        else:
            variance_score = 0.5

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º: –∫–∞–∂–¥—ã–π –ª–∏ –∞–±–∑–∞—Ü –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å transition word
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        if len(paragraphs) > 2:
            transition_starts = 0
            for para in paragraphs[1:]:
                first_words = para.split()[:3]
                first_phrase = ' '.join(first_words).lower()
                if any(
                    first_phrase.startswith(t)
                    for t in [
                        "however", "furthermore", "moreover", "in addition",
                        "consequently", "therefore", "additionally", "on the other hand",
                        "–æ–¥–Ω–∞–∫–æ", "–∫—Ä–æ–º–µ —Ç–æ–≥–æ", "–±–æ–ª–µ–µ —Ç–æ–≥–æ", "–≤ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ",
                        "—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º", "–≤ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ", "–ø–æ–º–∏–º–æ",
                        "–æ–¥–Ω–∞–∫", "–∫—Ä—ñ–º —Ç–æ–≥–æ", "–±—ñ–ª—å—à —Ç–æ–≥–æ", "—Ç–∞–∫–∏–º —á–∏–Ω–æ–º",
                    ]
                ):
                    transition_starts += 1

            trans_ratio = transition_starts / (len(paragraphs) - 1)
            transition_score = min(trans_ratio / 0.4, 1.0)
        else:
            transition_score = 0.5

        score = overlap_score * 0.3 + variance_score * 0.3 + transition_score * 0.4
        return max(0.0, min(1.0, score))

    # ‚îÄ‚îÄ‚îÄ 9. –ì–†–ê–ú–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –ò–î–ï–ê–õ–¨–ù–û–°–¢–¨ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_grammar(self, text: str, sentences: list[str]) -> float:
        """AI –ø–∏—à–µ—Ç –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏ ¬´—Å–ª–∏—à–∫–æ–º –∏–¥–µ–∞–ª—å–Ω–æ¬ª.

        –õ—é–¥–∏ –¥–æ–ø—É—Å–∫–∞—é—Ç:
        - –ù–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å–æ —Å—Ç—Ä–æ—á–Ω–æ–π (—Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π —Å—Ç–∏–ª—å)
        - –ù–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        - –í–≤–æ–¥–Ω—ã–µ –±–µ–∑ –≤—ã–¥–µ–ª–µ–Ω–∏—è –∑–∞–ø—è—Ç—ã–º–∏
        - –ù–µ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏
        - –°–æ–∫—Ä–∞—â–µ–Ω–∏—è (don't, isn't)
        - Sentence fragments
        - Informal punctuation (!! ...)

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (human-like = –Ω–µ–∏–¥–µ–∞–ª—å–Ω–æ) ‚Äî 1.0 (perfect grammar = AI)
        """
        if len(sentences) < 3:
            return 0.5

        indicators = []

        # 1. –í—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π?
        all_uppercase = sum(
            1 for s in sentences
            if s and s[0].isupper()
        )
        upper_ratio = all_uppercase / len(sentences) if sentences else 0
        # Human sometimes doesn't capitalize ~ 95%
        indicators.append(1.0 if upper_ratio == 1.0 else max(0, upper_ratio - 0.9) * 10)

        # 2. –í—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∑–∞–∫–∞–Ω—á–∏–≤–∞—é—Ç—Å—è –∑–Ω–∞–∫–æ–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è?
        all_punct_end = sum(
            1 for s in sentences
            if s and s.rstrip()[-1] in '.!?‚Ä¶'
        )
        punct_ratio = all_punct_end / len(sentences) if sentences else 0
        indicators.append(1.0 if punct_ratio == 1.0 else max(0, punct_ratio - 0.85) * 6.7)

        # 3. –ü–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏/—Å–∫–æ–±–∫–∏ ‚Äî AI –≤—Å–µ–≥–¥–∞ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç
        open_parens = text.count('(')
        close_parens = text.count(')')
        paren_balance = abs(open_parens - close_parens)
        indicators.append(1.0 if paren_balance == 0 and open_parens > 0 else 0.0)

        # 4. –ù–µ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π (don't, isn't) –≤ EN ‚Äî AI –ø–∏—à–µ—Ç –ø–æ–ª–Ω—ã–µ —Ñ–æ—Ä–º—ã
        if any(c.isascii() and c.isalpha() for c in text):
            contractions = len(re.findall(r"\b\w+'(?:t|s|re|ve|ll|d|m)\b", text, re.IGNORECASE))
            total_w = len(text.split())
            contraction_ratio = contractions / total_w if total_w > 0 else 0
            # Human EN: ~2-5% contractions, AI: ~0-1%
            indicators.append(max(0, 1.0 - contraction_ratio / 0.03))

        # 5. –û–¥–Ω–æ—Ä–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞ –∞–±–∑–∞—Ü–µ–≤ ‚Äî AI –¥–µ–ª–∞–µ—Ç –∏—Ö —Ä–∞–≤–Ω—ã–º–∏
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        if len(paragraphs) > 2:
            para_lengths = [len(p.split()) for p in paragraphs]
            avg_para = statistics.mean(para_lengths)
            if avg_para > 0:
                para_cv = statistics.stdev(para_lengths) / avg_para if len(para_lengths) > 1 else 0
                # AI: CV < 0.3, Human: CV > 0.4
                indicators.append(max(0, 1.0 - para_cv / 0.5))

        # 6. Oxford comma –ø–µ—Ä–µ–¥ "and" –≤ —Å–ø–∏—Å–∫–∞—Ö ‚Äî AI —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç
        oxford_matches = len(re.findall(r',\s+and\b', text, re.IGNORECASE))
        list_ands = len(re.findall(r'\band\b', text, re.IGNORECASE))
        if list_ands > 2:
            oxford_ratio = oxford_matches / list_ands
            indicators.append(min(oxford_ratio / 0.3, 1.0))

        # 7. –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ sentence fragments (–≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è >= 4 —Å–ª–æ–≤–∞)
        fragment_count = sum(1 for s in sentences if len(s.split()) < 4)
        fragment_ratio = fragment_count / len(sentences) if sentences else 0
        # Human uses fragments (~10-20%), AI almost never (<3%)
        indicators.append(max(0, 1.0 - fragment_ratio / 0.15))

        # 8. –ù–µ—Ç –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω–æ–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ (!! ... ??? ‚Äî human markers)
        informal_punct = len(re.findall(r'[!?]{2,}|\.{3,}', text))
        if len(sentences) > 5:
            punct_informality = informal_punct / len(sentences)
            indicators.append(max(0, 1.0 - punct_informality / 0.1))

        # 9. Consistent list formatting ‚Äî AI enumeates with same style
        bullet_lines = re.findall(r'^[\s]*[-‚Ä¢*]\s', text, re.MULTILINE)
        numbered_lines = re.findall(r'^[\s]*\d+[.)]\s', text, re.MULTILINE)
        if len(bullet_lines) + len(numbered_lines) >= 3:
            # Has formatted lists ‚Äî AI signal (humans less frequently use structured lists)
            indicators.append(0.8)

        score = statistics.mean(indicators) if indicators else 0.5
        return max(0.0, min(1.0, score))

    # ‚îÄ‚îÄ‚îÄ 10. –†–ê–ó–ù–û–û–ë–†–ê–ó–ò–ï –ù–ê–ß–ê–õ –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ô ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_openings(self, sentences: list[str]) -> float:
        """AI —á–∞—Å—Ç–æ –Ω–∞—á–∏–Ω–∞–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ.

        –ü–∞—Ç—Ç–µ—Ä–Ω—ã AI:
        - –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è 'The', 'This', 'It'
        - Subject-first –≤—Å–µ–≥–¥–∞
        - –ù–µ—Ç –∏–Ω–≤–µ—Ä—Å–∏–∏, –≤–≤–æ–¥–Ω—ã—Ö –æ–±–æ—Ä–æ—Ç–æ–≤ –≤ –Ω–∞—á–∞–ª–µ

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ = human) ‚Äî 1.0 (–æ–¥–Ω–æ–æ–±—Ä–∞–∑–Ω–æ = AI)
        """
        if len(sentences) < 5:
            return 0.5

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤—ã–µ —Å–ª–æ–≤–∞
        first_words = []
        first_bigrams = []
        for s in sentences:
            words = s.split()
            if words:
                first_words.append(words[0].lower().rstrip('.,;:'))
                if len(words) >= 2:
                    bigram = words[0].lower().rstrip('.,;:') + ' ' + words[1].lower().rstrip('.,;:')
                    first_bigrams.append(bigram)

        if not first_words:
            return 0.5

        # 1. –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø–µ—Ä–≤—ã—Ö —Å–ª–æ–≤
        unique_ratio = len(set(first_words)) / len(first_words)
        # AI: unique ~0.3-0.5, Human: ~0.6-0.8
        unique_score = max(0, 1.0 - (unique_ratio - 0.2) / 0.6)

        # 2. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –Ω–∞—á–∞–ª–∞
        first_counter = Counter(first_words)
        max_repeat = first_counter.most_common(1)[0][1]
        repeat_ratio = max_repeat / len(first_words)
        repeat_score = min(repeat_ratio / 0.3, 1.0)

        # 3. –ù–∞—á–∞–ª–æ —Å –ø–æ–¥–ª–µ–∂–∞—â–µ–≥–æ vs. –¥—Ä—É–≥–∏–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        # AI: subject-first > 80%, Human: ~60%
        subject_prons = {
            "i", "he", "she", "it", "they", "we", "you", "the", "this",
            "—è", "–æ–Ω", "–æ–Ω–∞", "–æ–Ω–æ", "–æ–Ω–∏", "–º—ã", "–≤—ã", "—ç—Ç–æ", "—ç—Ç–∏",
            "—è", "–≤—ñ–Ω", "–≤–æ–Ω–∞", "–≤–æ–Ω–æ", "–≤–æ–Ω–∏", "–º–∏", "–≤–∏", "—Ü–µ", "—Ü—ñ",
        }
        subject_starts = sum(1 for w in first_words if w in subject_prons)
        subject_ratio = subject_starts / len(first_words) if first_words else 0
        subject_score = min(subject_ratio / 0.5, 1.0)

        # 4. Consecutive same starts ‚Äî –ø–æ–¥—Ä—è–¥ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –Ω–∞—á–∞–ª–∞
        consecutive_same = 0
        for i in range(1, len(first_words)):
            if first_words[i] == first_words[i - 1]:
                consecutive_same += 1
        consec_ratio = consecutive_same / (len(first_words) - 1) if len(first_words) > 1 else 0
        consec_score = min(consec_ratio / 0.15, 1.0)

        score = (
            unique_score * 0.3
            + repeat_score * 0.25
            + subject_score * 0.2
            + consec_score * 0.25
        )
        return max(0.0, min(1.0, score))

    # ‚îÄ‚îÄ‚îÄ 11. –°–¢–ê–ë–ò–õ–¨–ù–û–°–¢–¨ READABILITY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_readability_consistency(self, sentences: list[str]) -> float:
        """AI –ø–∏—à–µ—Ç —Å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é, –ª—é–¥–∏ ‚Äî —Å –≤–∞—Ä—å–∏—Ä—É—é—â–µ–π.

        –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –æ–∫–Ω–∞ –∏ —Å—á–∏—Ç–∞–µ–º readability per window.
        –£ AI ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–±—Ä–æ—Å –º–µ–∂–¥—É –æ–∫–Ω–∞–º–∏.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (varying = human) ‚Äî 1.0 (uniform = AI)
        """
        if len(sentences) < 6:
            return 0.5

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –æ–∫–Ω–∞ –ø–æ 3
        window_size = 3
        windows = []
        for i in range(0, len(sentences) - window_size + 1, window_size):
            window_text = ' '.join(sentences[i:i + window_size])
            words = window_text.split()
            if words:
                avg_word_len = statistics.mean(len(w) for w in words)
                avg_sent_len = statistics.mean(
                    len(s.split()) for s in sentences[i:i + window_size]
                )
                # –ü—Ä–æ—Å—Ç–∞—è readability metric –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω
                readability = avg_word_len * 0.5 + avg_sent_len * 0.5
                windows.append(readability)

        if len(windows) < 3:
            return 0.5

        # CV readability across windows
        avg_r = statistics.mean(windows)
        stdev_r = statistics.stdev(windows) if len(windows) > 1 else 0
        cv_r = stdev_r / avg_r

        # AI: CV < 0.1, Human: CV > 0.15
        score = max(0, 1.0 - cv_r / 0.2)

        return max(0.0, min(1.0, score))

    # ‚îÄ‚îÄ‚îÄ 12. –†–ò–¢–ú –¢–ï–ö–°–¢–ê ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_rhythm(self, sentences: list[str]) -> float:
        """–†–∏—Ç–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ ‚Äî –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª–∏–Ω –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.

        AI: –¥–ª–∏–Ω–Ω–æ–µ-–¥–ª–∏–Ω–Ω–æ–µ-–¥–ª–∏–Ω–Ω–æ–µ (–º–æ–Ω–æ—Ç–æ–Ω–Ω–æ)
        Human: –¥–ª–∏–Ω–Ω–æ–µ-–∫–æ—Ä–æ—Ç–∫–æ–µ-—Å—Ä–µ–¥–Ω–µ–µ-–¥–ª–∏–Ω–Ω–æ–µ-–∫–æ—Ä–æ—Ç–∫–æ–µ (–Ω–µ—Ä–µ–≥—É–ª—è—Ä–Ω–æ)

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (–Ω–µ—Ä–µ–≥—É–ª—è—Ä–Ω—ã–π —Ä–∏—Ç–º = human) ‚Äî 1.0 (–º–æ–Ω–æ—Ç–æ–Ω–Ω—ã–π = AI)
        """
        if len(sentences) < 5:
            return 0.5

        lengths = [len(s.split()) for s in sentences]

        # 1. Autocorrelation lag-1 (–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å–æ—Å–µ–¥–Ω–∏—Ö –¥–ª–∏–Ω)
        # AI: –≤—ã—Å–æ–∫–∞—è autocorrelation (—Ä—è–¥–æ–º —Å—Ç–æ—è—â–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ—Ö–æ–∂–µ–π –¥–ª–∏–Ω—ã)
        # Human: –Ω–∏–∑–∫–∞—è
        n = len(lengths)
        mean_l = statistics.mean(lengths)
        var_l = statistics.variance(lengths) if n > 1 else 1

        if var_l == 0:
            autocorr = 1.0  # –í—Å–µ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω—ã ‚Äî –æ—á–µ–Ω—å AI-like
        else:
            numerator = sum(
                (lengths[i] - mean_l) * (lengths[i + 1] - mean_l)
                for i in range(n - 1)
            ) / (n - 1)
            autocorr = numerator / var_l

        # AC: AI ~0.3-0.6, Human ~-0.1‚Äî0.2
        autocorr_score = max(0, (autocorr + 0.1) / 0.7)

        # 2. –î–ª–∏–Ω–∞ ¬´—Å–µ—Ä–∏–π¬ª (runs) –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª–∏–Ω
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: short (<10), medium (10-20), long (>20)
        categories = []
        for slen in lengths:
            if slen <= 8:
                categories.append('S')
            elif slen <= 20:
                categories.append('M')
            else:
                categories.append('L')

        # –°—á–∏—Ç–∞–µ–º runs (—Å–µ—Ä–∏–∏ –æ–¥–Ω–æ–≥–æ —Ç–∏–ø–∞)
        runs = 1
        for i in range(1, len(categories)):
            if categories[i] != categories[i - 1]:
                runs += 1

        expected_runs = n  # –í –∏–¥–µ–∞–ª–µ ‚Äî –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥—Ä—É–≥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        run_ratio = runs / expected_runs if expected_runs > 0 else 0

        # AI: –º–∞–ª–æ runs (~0.3-0.5), Human: –º–Ω–æ–≥–æ runs (~0.7-0.9)
        run_score = max(0, 1.0 - run_ratio / 0.8)

        # 3. –ù–∞–ª–∏—á–∏–µ ¬´–ø–∞—Ä–Ω—ã—Ö¬ª –¥–ª–∏–Ω ‚Äî AI —á–∞—Å—Ç–æ –¥–µ–ª–∞–µ—Ç couplets
        # (–¥–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ–¥—Ä—è–¥ ¬±3 —Å–ª–æ–≤–∞ –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞)
        pairs = 0
        for i in range(n - 1):
            if abs(lengths[i] - lengths[i + 1]) <= 3:
                pairs += 1
        pair_ratio = pairs / (n - 1) if n > 1 else 0
        # AI: pair_ratio ~0.4-0.6, Human: ~0.2-0.3
        pair_score = min(pair_ratio / 0.5, 1.0)

        score = autocorr_score * 0.4 + run_score * 0.3 + pair_score * 0.3
        return max(0.0, min(1.0, score))

    # ‚îÄ‚îÄ‚îÄ 13. N-GRAM PERPLEXITY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_perplexity(self, text: str, sentences: list[str]) -> float:
        """Character-level n-gram perplexity with reference corpus.

        Uses both self-trained model and cross-perplexity against
        a pre-computed reference corpus of known human text.

        AI —Ç–µ–∫—Å—Ç –∏–º–µ–µ—Ç –Ω–∏–∑–∫—É—é perplexity (–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        —Å–∏–º–≤–æ–ª–æ–≤), –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —Ç–µ–∫—Å—Ç –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–µ–Ω.

        –ú–µ—Ç–æ–¥:
        - –°—Ç—Ä–æ–∏–º character trigram –º–æ–¥–µ–ª—å –∏–∑ —Ç–µ–∫—Å—Ç–∞
        - –°—á–∏—Ç–∞–µ–º per-sentence self-perplexity
        - –°—á–∏—Ç–∞–µ–º cross-perplexity –ø—Ä–æ—Ç–∏–≤ reference corpus
        - –°—á–∏—Ç–∞–µ–º per-sentence perplexity
        - –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å perplexity –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏

        AI: —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è (–Ω–∏–∑–∫–∞—è) perplexity ‚Üí score ‚Üí 1.0
        Human: –≤—ã—Å–æ–∫–∞—è –∏ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–∞—è perplexity ‚Üí score ‚Üí 0.0
        """
        if len(sentences) < 4:
            return 0.5

        # Build character trigram model
        text_lower = text.lower()
        trigram_counts: dict[str, int] = {}
        bigram_counts: dict[str, int] = {}

        for i in range(len(text_lower) - 2):
            trigram = text_lower[i:i + 3]
            bigram = text_lower[i:i + 2]
            trigram_counts[trigram] = trigram_counts.get(trigram, 0) + 1
            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1

        if not bigram_counts:
            return 0.5

        # Calculate per-sentence perplexity
        sent_perplexities: list[float] = []

        for sent in sentences:
            sent_lower = sent.lower().strip()
            if len(sent_lower) < 5:
                continue

            log_prob_sum = 0.0
            n_trigrams = 0

            for i in range(len(sent_lower) - 2):
                trigram = sent_lower[i:i + 3]
                bigram = sent_lower[i:i + 2]

                tri_count = trigram_counts.get(trigram, 0)
                bi_count = bigram_counts.get(bigram, 0)

                if bi_count > 0:
                    # Conditional probability with Laplace smoothing
                    prob = (tri_count + 1) / (bi_count + len(trigram_counts) + 1)
                    log_prob_sum += math.log(prob)
                    n_trigrams += 1

            if n_trigrams > 0:
                avg_log_prob = log_prob_sum / n_trigrams
                perplexity = math.exp(-avg_log_prob)
                sent_perplexities.append(perplexity)

        if len(sent_perplexities) < 3:
            return 0.5

        # Analysis
        avg_perplexity = statistics.mean(sent_perplexities)
        perplexity_std = statistics.stdev(sent_perplexities)
        perplexity_cv = perplexity_std / avg_perplexity if avg_perplexity > 0 else 0

        # Low average perplexity = predictable = AI
        # Typical ranges: AI = 3-8, Human = 8-25
        avg_score = max(0.0, 1.0 - (avg_perplexity - 3.0) / 15.0)

        # Low CV of perplexity = uniform predictability = AI
        # AI: CV < 0.2, Human: CV > 0.3
        cv_score = max(0.0, 1.0 - perplexity_cv / 0.4)

        # Cross-perplexity against reference corpus
        from texthumanize.corpus_stats import cross_perplexity, get_reference_perplexity
        effective_lang = getattr(self, '_current_lang', 'en')
        xp = cross_perplexity(text, effective_lang)
        ref_pp = get_reference_perplexity(effective_lang)
        # AI cross-perplexity is usually close to reference (conformist text)
        # Human cross-perplexity deviates more (unique style)
        xp_deviation = abs(xp - ref_pp) / ref_pp if ref_pp > 0 else 0
        xp_score = max(0.0, 1.0 - xp_deviation / 0.5)

        score = avg_score * 0.35 + cv_score * 0.35 + xp_score * 0.30
        return max(0.0, min(1.0, score))

    # ‚îÄ‚îÄ‚îÄ 14. –î–ò–°–ö–£–†–°–ò–í–ù–ê–Ø –°–¢–†–£–ö–¢–£–†–ê ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_discourse(self, text: str, sentences: list[str]) -> float:
        """AI —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∫–∞–∫ intro-body-conclusion —Ä–∏–≥–∏–¥–Ω–æ.

        –ü—Ä–∏–∑–Ω–∞–∫–∏ AI:
        - –ß—ë—Ç–∫–æ–µ intro (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å ¬´–æ–±–∑–æ—Ä–Ω–æ–π¬ª –ª–µ–∫—Å–∏–∫–æ–π)
        - Body —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º–∏ –∞–±–∑–∞—Ü–∞–º–∏ —Ä–∞–≤–Ω–æ–π –¥–ª–∏–Ω—ã
        - Conclusion —Å ¬´–≤ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ¬ª, ¬´—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º¬ª
        - –ö–∞–∂–¥—ã–π –∞–±–∑–∞—Ü = —Ä–æ–≤–Ω–æ –æ–¥–Ω–∞ –ø–æ–¥—Ç–µ–º–∞

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞) ‚Äî 1.0 (AI-—à–∞–±–ª–æ–Ω)
        """
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        if len(paragraphs) < 3:
            # –ë–µ–∑ –∞–±–∑–∞—Ü–µ–≤ ‚Äî –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –∫–∞–∫ –µ–¥–∏–Ω—ã–π –ø–æ—Ç–æ–∫
            return self._calc_discourse_flat(text, sentences)

        indicators: list[float] = []

        # 1. Conclusion markers –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º –∞–±–∑–∞—Ü–µ
        last_para = paragraphs[-1].lower()
        conclusion_markers = {
            "in conclusion", "to summarize", "in summary", "overall",
            "to conclude", "in short", "ultimately", "all in all",
            "–≤ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ", "–ø–æ–¥–≤–æ–¥—è –∏—Ç–æ–≥", "—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º", "–∏—Ç–æ–≥–æ",
            "—Ä–µ–∑—é–º–∏—Ä—É—è", "–≤ –∏—Ç–æ–≥–µ", "–ø—ñ–¥—Å—É–º–æ–≤—É—é—á–∏", "–Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è",
        }
        has_conclusion = any(m in last_para for m in conclusion_markers)
        indicators.append(1.0 if has_conclusion else 0.0)

        # 2. Intro markers –≤ –ø–µ—Ä–≤–æ–º –∞–±–∑–∞—Ü–µ
        first_para = paragraphs[0].lower()
        intro_markers = {
            "in today's", "in the modern", "in recent years",
            "the importance of", "it is widely", "has become",
            "plays a crucial", "is one of", "has emerged",
            "–≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º", "–Ω–∞ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–π –¥–µ–Ω—å", "–≤ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã",
            "—è–≤–ª—è–µ—Ç—Å—è –æ–¥–Ω–∏–º –∏–∑", "–∏–≥—Ä–∞–µ—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å",
            "—É —Å—É—á–∞—Å–Ω–æ–º—É", "–Ω–∞ —Å—å–æ–≥–æ–¥–Ω—ñ—à–Ω—ñ–π",
        }
        has_intro = any(m in first_para for m in intro_markers)
        indicators.append(0.9 if has_intro else 0.1)

        # 3. –û–¥–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç—å –¥–ª–∏–Ω—ã —Å—Ä–µ–¥–Ω–∏—Ö –∞–±–∑–∞—Ü–µ–≤ (body)
        if len(paragraphs) > 3:
            body = paragraphs[1:-1]
            body_lengths = [len(p.split()) for p in body]
            if len(body_lengths) >= 2:
                avg_bl = statistics.mean(body_lengths)
                if avg_bl > 0:
                    cv_bl = statistics.stdev(body_lengths) / avg_bl
                    # AI: CV < 0.2, Human: CV > 0.4
                    indicators.append(max(0.0, 1.0 - cv_bl / 0.5))

        # 4. –ö–∞–∂–¥—ã–π –∞–±–∑–∞—Ü –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å transition word
        if len(paragraphs) > 2:
            trans_count = 0
            transitions = {
                "however", "furthermore", "moreover", "additionally",
                "in addition", "on the other hand", "consequently",
                "first", "second", "third", "finally",
                "–æ–¥–Ω–∞–∫–æ", "–∫—Ä–æ–º–µ —Ç–æ–≥–æ", "–±–æ–ª–µ–µ —Ç–æ–≥–æ", "–≤–æ-–ø–µ—Ä–≤—ã—Ö",
                "–≤–æ-–≤—Ç–æ—Ä—ã—Ö", "–≤-—Ç—Ä–µ—Ç—å–∏—Ö", "–Ω–∞–∫–æ–Ω–µ—Ü",
            }
            for p in paragraphs[1:]:
                first_w = ' '.join(p.split()[:3]).lower()
                if any(first_w.startswith(t) for t in transitions):
                    trans_count += 1
            ratio = trans_count / (len(paragraphs) - 1)
            indicators.append(min(ratio / 0.4, 1.0))

        # 5. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: –∞–±–∑–∞—Ü—ã –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ
        if len(paragraphs) >= 4:
            first_words = [p.split()[0].lower().rstrip('.,;:') for p in paragraphs if p.split()]
            first_counter = Counter(first_words)
            max_same = first_counter.most_common(1)[0][1] if first_counter else 0
            parallelism = max_same / len(first_words) if first_words else 0
            indicators.append(min(parallelism / 0.3, 1.0))

        return max(0.0, min(1.0, statistics.mean(indicators))) if indicators else 0.5

    def _calc_discourse_flat(self, text: str, sentences: list[str]) -> float:
        """–î–∏—Å–∫—É—Ä—Å–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ –∞–±–∑–∞—Ü–µ–≤."""
        if len(sentences) < 5:
            return 0.5

        indicators: list[float] = []

        # Intro + conclusion
        first_sent = sentences[0].lower()
        last_sent = sentences[-1].lower()

        conclusion_words = {"conclusion", "summarize", "summary", "overall",
                           "–∑–∞–∫–ª—é—á–µ–Ω–∏–µ", "–∏—Ç–æ–≥", "—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º", "–ø—ñ–¥—Å—É–º–æ–≤—É—é—á–∏"}
        indicators.append(0.8 if any(w in last_sent for w in conclusion_words) else 0.2)

        intro_words = {"today's", "modern", "importance", "crucial",
                      "—Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º", "—Å—å–æ–≥–æ–¥–Ω—ñ—à–Ω—ñ–π", "–∫–ª—é—á–µ–≤—É—é"}
        indicators.append(0.7 if any(w in first_sent for w in intro_words) else 0.3)

        return max(0.0, min(1.0, statistics.mean(indicators))) if indicators else 0.5

    # ‚îÄ‚îÄ‚îÄ 15. –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ï –ü–û–í–¢–û–†–´ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_semantic_repetition(self, text: str, sentences: list[str]) -> float:
        """AI —á–∞—Å—Ç–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–µ—Ç –æ–¥–Ω—É –º—ã—Å–ª—å –≤ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö.

        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º:
        - Jaccard similarity –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏ (–ø–æ content words)
        - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ ¬´–ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö¬ª –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (similarity > 0.4)
        - AI –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–µ—Ç –≤–º–µ—Å—Ç–æ —Ç–æ–≥–æ, —á—Ç–æ–±—ã –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (–∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ = –Ω–æ–≤–∞—è –º—ã—Å–ª—å) ‚Äî 1.0 (–ø–æ–≤—Ç–æ—Ä—ã)
        """
        if len(sentences) < 5:
            return 0.5

        # Extract content word sets for each sentence
        punct = '.,;:!?"\'()[]{}'
        sent_words: list[set[str]] = []
        for s in sentences:
            words = {
                w.lower().strip(punct)
                for w in s.split()
                if len(w.strip(punct)) > 3
            }
            if words:
                sent_words.append(words)

        if len(sent_words) < 4:
            return 0.5

        # Pairwise Jaccard similarity (non-adjacent only)
        high_sim_count = 0
        total_pairs = 0
        sim_values: list[float] = []

        for i in range(len(sent_words)):
            # Skip adjacent, check within window of 6
            for j in range(i + 2, min(i + 6, len(sent_words))):
                intersection = len(sent_words[i] & sent_words[j])
                union = len(sent_words[i] | sent_words[j])
                if union > 0:
                    sim = intersection / union
                    sim_values.append(sim)
                    total_pairs += 1
                    if sim > 0.35:
                        high_sim_count += 1

        if not sim_values:
            return 0.5

        avg_sim = statistics.mean(sim_values)
        high_sim_ratio = high_sim_count / total_pairs if total_pairs > 0 else 0

        # AI: avg_sim ~0.15-0.25, high_sim_ratio ~0.1-0.3
        # Human: avg_sim ~0.05-0.10, high_sim_ratio ~0-0.05
        sim_score = min(avg_sim / 0.2, 1.0)
        ratio_score = min(high_sim_ratio / 0.15, 1.0)

        score = sim_score * 0.5 + ratio_score * 0.5
        return max(0.0, min(1.0, score))

    # ‚îÄ‚îÄ‚îÄ 16. –°–ü–ï–¶–ò–§–ò–ß–ù–û–°–¢–¨ –£–ü–û–ú–ò–ù–ê–ù–ò–ô ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_entity_specificity(self, text: str, words: list[str]) -> float:
        """AI –∏—Å–ø–æ–ª—å–∑—É–µ—Ç generic entities, –ª—é–¥–∏ ‚Äî –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ.

        AI: "a company", "researchers", "many experts", "a recent study"
        Human: "Google", "Dr. Smith", "my boss", "last Tuesday"

        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º:
        - –ù–∞–ª–∏—á–∏–µ proper nouns (capitalized non-start words)
        - –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —á–∏—Å–ª–∞/–¥–∞—Ç—ã
        - Generic vs specific references
        - –ù–∞–ª–∏—á–∏–µ –ª–∏—á–Ω–æ–≥–æ –æ–ø—ã—Ç–∞ (I, my, our)

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (–∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ = human) ‚Äî 1.0 (–∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ = AI)
        """
        if len(words) < 20:
            return 0.5

        total = len(words)
        indicators: list[float] = []

        # 1. Generic quantifiers (AI loves vague quantities)
        generic_quants = {
            "various", "numerous", "several", "many", "multiple",
            "significant", "substantial", "considerable", "widespread",
            "a number of", "a variety of", "a wide range",
            "—Ä–∞–∑–ª–∏—á–Ω—ã–µ", "–º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ", "–º–Ω–æ–≥–∏–µ", "–º–Ω–æ–∂–µ—Å—Ç–≤–æ",
            "–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π", "—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π", "—Ä—è–¥", "—à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä",
            "—Ä—ñ–∑–Ω—ñ", "—á–∏—Å–ª–µ–Ω–Ω—ñ", "–±–∞–≥–∞—Ç–æ", "–±–µ–∑–ª—ñ—á",
        }
        generic_count = sum(1 for w in words if w.lower() in generic_quants)
        # Also check multi-word
        text_lower = text.lower()
        for phrase in ["a number of", "a variety of", "a wide range",
                       "—à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä", "—Ä—è–¥", "—Ü–µ–ª—ã–π —Ä—è–¥"]:
            generic_count += text_lower.count(phrase)
        generic_ratio = generic_count / total
        indicators.append(min(generic_ratio / 0.03, 1.0))

        # 2. Specific numbers, dates, proper nouns (human marker)
        # Specific numbers (not round): 15, 2023, 42, $3.50
        specific_nums = len(re.findall(
            r'\b\d{1,2}(?:\.\d+)?\b|\$\d+|\b\d{4}\b|\b\d+%',
            text
        ))
        # Proper nouns (capitalized words not at sentence start)
        proper_nouns = 0
        for i, w in enumerate(words):
            if i > 0 and w[0:1].isupper() and w.isalpha() and len(w) > 1:
                # Check it's not after a period
                prev = words[i - 1] if i > 0 else ""
                if not prev.endswith(('.', '!', '?', ':', '"', '\n')):
                    proper_nouns += 1

        specificity = (specific_nums + proper_nouns) / total
        # High specificity = human
        indicators.append(max(0.0, 1.0 - specificity / 0.08))

        # 3. Personal references (I, my, me, we, our)
        personal = {"i", "my", "me", "we", "our", "mine",
                    "—è", "–º–æ–π", "–º–æ—è", "–º–æ—ë", "–º–Ω–µ", "–º–µ–Ω—è", "–º—ã", "–Ω–∞—à",
                    "—è", "–º—ñ–π", "–º–æ—è", "–º–æ—î", "–º–µ–Ω—ñ", "–º–µ–Ω–µ", "–º–∏", "–Ω–∞—à"}
        personal_count = sum(1 for w in words if w.lower() in personal)
        personal_ratio = personal_count / total
        # Has personal references = likely human
        indicators.append(max(0.0, 1.0 - personal_ratio / 0.04))

        # 4. Hedging language (AI hedges a lot)
        hedges = {
            "arguably", "potentially", "relatively", "particularly",
            "generally", "typically", "essentially", "fundamentally",
            "–ø–æ —Å—É—Ç–∏", "–≤ —Ü–µ–ª–æ–º", "–∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ", "–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏",
            "–≤ –æ—Å–Ω–æ–≤–Ω–æ–º—É", "—è–∫ –ø—Ä–∞–≤–∏–ª–æ", "–∑–∞–≥–∞–ª–æ–º",
        }
        hedge_count = sum(1 for w in words if w.lower() in hedges)
        for phrase in hedges:
            if ' ' in phrase:
                hedge_count += text_lower.count(phrase)
        hedge_ratio = hedge_count / total
        indicators.append(min(hedge_ratio / 0.02, 1.0))

        return max(0.0, min(1.0, statistics.mean(indicators))) if indicators else 0.5

    # ‚îÄ‚îÄ‚îÄ 17. –ê–ù–ê–õ–ò–ó –ó–ê–õ–û–ì–ê (VOICE) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_voice(self, text: str, sentences: list[str]) -> float:
        """AI overuses passive voice –∏ –Ω–æ–º–∏–Ω–∞–ª–∏–∑–∞—Ü–∏–∏.

        AI: "The implementation was carried out" "Analysis was performed"
        Human: "We implemented" "I analyzed"

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (active = human) ‚Äî 1.0 (passive-heavy = AI)
        """
        if len(sentences) < 4:
            return 0.5

        passive_count = 0
        nominalization_count = 0
        total_clauses = len(sentences)

        # Passive voice patterns (EN)
        passive_patterns = [
            r'\b(?:is|are|was|were|been|being|be)\s+\w+ed\b',
            r'\b(?:is|are|was|were|been|being|be)\s+\w+en\b',
            r'\b(?:has|have|had)\s+been\s+\w+ed\b',
            r'\bwas\s+\w+ed\b',
            r'\bwere\s+\w+ed\b',
        ]

        # Passive patterns (RU/UK)
        passive_patterns_ru = [
            r'\b\w+(?:–æ–≤–∞–Ω|–∏—Ä–æ–≤–∞–Ω|–µ–Ω|–∞–Ω|—è—Ç|–∏—Ç)(?:–∞|–æ|—ã|–∏)?\s+(?:–±—ã–ª|–±—ã–ª–∞|–±—ã–ª–æ|–±—ã–ª–∏)\b',
            r'\b(?:–±—ã–ª|–±—ã–ª–∞|–±—ã–ª–æ|–±—ã–ª–∏)\s+\w+(?:–æ–≤–∞–Ω|–∏—Ä–æ–≤–∞–Ω|–µ–Ω|–∞–Ω)\w*\b',
            r'\b\w+(?:—Å—è|—Å—å)\b',  # Reflexive verbs (often passive in RU)
        ]

        text_lower = text.lower()

        for pattern in passive_patterns:
            passive_count += len(re.findall(pattern, text_lower))

        for pattern in passive_patterns_ru:
            passive_count += len(re.findall(pattern, text_lower))

        # Nominalizations: -tion, -ment, -ness, -ity (overused by AI)
        nominalization_suffixes = [
            r'\b\w{4,}tion\b', r'\b\w{4,}ment\b', r'\b\w{4,}ness\b',
            r'\b\w{4,}ity\b', r'\b\w{4,}ence\b', r'\b\w{4,}ance\b',
            # RU: -–∞—Ü–∏—è, -–µ–Ω–∏–µ, -–æ–≤–∞–Ω–∏–µ
            r'\b\w{4,}–∞—Ü–∏—è\b', r'\b\w{4,}–µ–Ω–∏–µ\b', r'\b\w{4,}–æ–≤–∞–Ω–∏–µ\b',
        ]

        for suffix_pat in nominalization_suffixes:
            nominalization_count += len(re.findall(suffix_pat, text_lower))

        # Passive ratio
        passive_ratio = passive_count / total_clauses if total_clauses > 0 else 0
        # AI: ~0.3-0.6 passive per sentence, Human: ~0.1-0.2
        passive_score = min(passive_ratio / 0.4, 1.0)

        # Nominalization density
        word_count = len(text.split())
        nom_ratio = nominalization_count / word_count if word_count > 0 else 0
        # AI: ~0.05-0.10, Human: ~0.02-0.04
        nom_score = min(nom_ratio / 0.07, 1.0)

        # Active voice markers (human uses more)
        active_markers = len(re.findall(
            r'\b(?:I|we|you|he|she|they)\s+\w+(?:ed|s)\b',
            text, re.IGNORECASE
        ))
        active_ratio = active_markers / total_clauses if total_clauses > 0 else 0
        active_score = max(0.0, 1.0 - active_ratio / 0.3)

        score = passive_score * 0.35 + nom_score * 0.35 + active_score * 0.30
        return max(0.0, min(1.0, score))

    # ‚îÄ‚îÄ‚îÄ 18. TOPIC SENTENCE –ü–ê–¢–¢–ï–†–ù ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calc_topic_sentence(self, text: str, sentences: list[str]) -> float:
        """AI —Å—Ç–∞–≤–∏—Ç topic sentence (–≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å) –ø–µ—Ä–≤–æ–π –≤ –∫–∞–∂–¥–æ–º –∞–±–∑–∞—Ü–µ.

        –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è 5-paragraph essay structure:
        - –ö–∞–∂–¥—ã–π –∞–±–∑–∞—Ü –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –æ–±–æ–±—â–∞—é—â–µ–≥–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
        - –î–∞–ª–µ–µ –∏–¥—É—Ç supporting details
        - AI –¥–µ–ª–∞–µ—Ç —ç—Ç–æ –≤ 100% –∞–±–∑–∞—Ü–µ–≤

        –õ—é–¥–∏ —á–∞—Å—Ç–æ:
        - –ù–∞—á–∏–Ω–∞—é—Ç —Å –ø—Ä–∏–º–µ—Ä–∞/–∞–Ω–µ–∫–¥–æ—Ç–∞
        - –ò—Å–ø–æ–ª—å–∑—É—é—Ç delayed thesis
        - –ù–µ —Å–ª–µ–¥—É—é—Ç —à–∞–±–ª–æ–Ω—É paragraph structure

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 0.0 (–Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ = human) ‚Äî 1.0 (–∏–¥–µ–∞–ª—å–Ω–∞—è = AI)
        """
        paragraphs = [p.strip() for p in text.split('\n\n') if len(p.strip()) > 30]

        if len(paragraphs) < 2:
            # –î–ª—è —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ –∞–±–∑–∞—Ü–µ–≤: –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ –≥—Ä—É–ø–ø–∞–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            if len(sentences) < 8:
                return 0.5
            # Group sentences by 3-4 as pseudo-paragraphs
            paragraphs = []
            for i in range(0, len(sentences) - 2, 3):
                paragraphs.append(' '.join(sentences[i:i + 3]))

        if len(paragraphs) < 2:
            return 0.5

        indicators: list[float] = []

        for para in paragraphs:
            para_sentences = [s.strip() for s in re.split(r'[.!?]+', para) if len(s.strip()) > 10]
            if len(para_sentences) < 2:
                continue

            first_sent = para_sentences[0].lower()
            _rest = ' '.join(para_sentences[1:]).lower()  # noqa: F841

            # Topic sentence indicators:
            # 1. First sentence contains abstract/general words
            general_words = {
                "important", "significant", "crucial", "essential", "key",
                "fundamental", "critical", "notable", "remarkable",
                "role", "impact", "influence", "factor", "aspect",
                "–≤–∞–∂–Ω–æ", "–∑–Ω–∞—á–∏–º–æ", "–∫–ª—é—á–µ–≤–æ–π", "—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ",
                "—Ä–æ–ª—å", "–≤–ª–∏—è–Ω–∏–µ", "—Ñ–∞–∫—Ç–æ—Ä", "–∞—Å–ø–µ–∫—Ç",
            }
            has_general = sum(1 for w in first_sent.split() if w.strip('.,;:') in general_words)

            # 2. First sentence is longer than average of rest
            first_len = len(first_sent.split())
            rest_lens = [len(s.split()) for s in para_sentences[1:]]
            avg_rest_len = statistics.mean(rest_lens) if rest_lens else first_len

            is_topic_sent = (
                has_general >= 1
                or first_len >= avg_rest_len * 0.8
            )
            indicators.append(0.8 if is_topic_sent else 0.2)

        if not indicators:
            return 0.5

        # If ALL paragraphs have topic sentences ‚Üí very AI-like
        topic_ratio = statistics.mean(indicators)
        return max(0.0, min(1.0, topic_ratio))

    # ‚îÄ‚îÄ‚îÄ ENSEMBLE AGGREGATION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _ensemble_aggregate(
        self,
        scores: dict[str, float],
        weights: dict[str, float] | None = None,
    ) -> float:
        """Ensemble boosting aggregation –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–≥–æ weighted sum.

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç 3 ¬´—Å–ª–∞–±—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞¬ª:
        1. Weighted sum (–±–∞–∑–æ–≤—ã–π)
        2. Strong signal detector (–∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫)
        3. Majority voting (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç—Ä–∏–∫ > –ø–æ—Ä–æ–≥–∞)

        –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä ‚Äî –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –≤—Å–µ—Ö —Ç—Ä—ë—Ö.
        """
        w = weights if weights is not None else self._WEIGHTS
        # 1. Weighted sum (base learner)
        weighted_sum = sum(
            scores[k] * w[k] for k in scores
        )
        total_weight = sum(w.values())
        base_score = weighted_sum / total_weight

        # 2. Strong signal detector
        # –ï—Å–ª–∏ –∫–ª—é—á–µ–≤—ã–µ ¬´—Å–∏–ª—å–Ω—ã–µ¬ª –º–µ—Ç—Ä–∏–∫–∏ –≤—Å–µ –≤—ã—Å–æ–∫–∏–µ/–Ω–∏–∑–∫–∏–µ ‚Äî
        # —ç—Ç–æ —Å–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
        strong_metrics = [
            "pattern", "burstiness", "opening", "stylometry",
            "discourse", "voice", "grammar",
        ]
        strong_vals = [scores.get(m, 0.5) for m in strong_metrics]
        strong_avg = statistics.mean(strong_vals)

        # –ù–µ–ª–∏–Ω–µ–π–Ω–æ–µ —É—Å–∏–ª–µ–Ω–∏–µ: –µ—Å–ª–∏ —Å–∏–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤—Å–µ > 0.55 ‚Üí boost up
        if strong_avg > 0.55:
            strong_score = 0.5 + (strong_avg - 0.5) * 1.8
        elif strong_avg < 0.35:
            strong_score = 0.5 + (strong_avg - 0.5) * 1.5
        else:
            strong_score = strong_avg

        strong_score = max(0.0, min(1.0, strong_score))

        # 3. Majority voting
        ai_threshold = 0.55
        n_ai = sum(1 for v in scores.values() if v >= ai_threshold)
        n_total = len(scores)
        vote_ratio = n_ai / n_total if n_total > 0 else 0.5

        # –ù–µ–ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≥–æ–ª–æ—Å–æ–≤
        if vote_ratio > 0.6:
            vote_score = 0.5 + (vote_ratio - 0.5) * 1.5
        elif vote_ratio < 0.4:
            vote_score = 0.5 + (vote_ratio - 0.5) * 1.5
        else:
            vote_score = vote_ratio

        vote_score = max(0.0, min(1.0, vote_score))

        # Ensemble: –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Ç—Ä—ë—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤
        ensemble = (
            base_score * 0.40      # Base weighted sum
            + strong_score * 0.40  # Strong signal detector
            + vote_score * 0.20    # Majority voting
        )

        return max(0.0, min(1.0, ensemble))

    # ‚îÄ‚îÄ‚îÄ –ö–ê–õ–ò–ë–†–û–í–ö–ê –ò –û–ë–™–Ø–°–ù–ï–ù–ò–Ø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _calibrate(self, raw: float) -> float:
        """Sigmoidal calibration –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è.

        –£—Å–∏–ª–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω–∏—Ü—É –≤ —Å—Ä–µ–¥–Ω–µ–π –∑–æ–Ω–µ (0.25‚Äì0.70).
        """
        # Logistic function ‚Äî —Å–º—è–≥—á—ë–Ω–Ω–∞—è (center=0.35, k=8)
        # –ù–µ –¥–∞–≤–∏—Ç —Å—Ä–µ–¥–Ω–∏–µ raw so –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ, –ø–æ–∑–≤–æ–ª—è—è
        # —Ç–µ–∫—Å—Ç–∞–º —Å raw=0.30-0.40 –ø–æ–ª—É—á–∏—Ç—å –∑–∞–º–µ—Ç–Ω—ã–π score.
        k = 8.0  # steepness (was 10)
        center = 0.35  # center (was 0.40)
        return 1.0 / (1.0 + math.exp(-k * (raw - center)))

    def _generate_explanations(
        self,
        scores: dict[str, float],
        result: DetectionResult,
        lang: str,
    ) -> list[str]:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è."""
        explanations = []
        threshold = 0.6  # –í—ã—à–µ = "AI-like"

        feature_names = {
            "entropy": "Low text entropy (predictable word choice)",
            "burstiness": "Uniform sentence lengths (low burstiness)",
            "vocabulary": "Limited vocabulary diversity",
            "zipf": "Word frequency deviation from natural distribution",
            "stylometry": "Formal/academic writing style typical of AI",
            "pattern": "AI-characteristic words and phrases detected",
            "punctuation": "Punctuation profile typical of AI",
            "coherence": "Overly consistent paragraph transitions",
            "grammar": "Unnaturally perfect grammar and formatting",
            "opening": "Repetitive sentence openings",
            "readability": "Uniform readability across text segments",
            "rhythm": "Monotonous sentence length rhythm",
            "perplexity": "Low character-level perplexity (predictable patterns)",
            "discourse": "Rigid intro-body-conclusion discourse structure",
            "semantic_rep": "Semantic paraphrasing of same ideas across paragraphs",
            "entity": "Generic/abstract entity references instead of specifics",
            "voice": "Heavy passive voice and nominalizations",
            "topic_sentence": "Every paragraph starts with a topic sentence",
        }

        feature_names_ru = {
            "entropy": "–ù–∏–∑–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–π –≤—ã–±–æ—Ä —Å–ª–æ–≤)",
            "burstiness": "–û–¥–Ω–æ—Ä–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–Ω–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å)",
            "vocabulary": "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ",
            "zipf": "–û—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç —Å–ª–æ–≤ –æ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è",
            "stylometry": "–§–æ—Ä–º–∞–ª—å–Ω—ã–π –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π —Å—Ç–∏–ª—å, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–π –¥–ª—è AI",
            "pattern": "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã AI-—Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ —Ñ—Ä–∞–∑—ã",
            "punctuation": "–ü—É–Ω–∫—Ç—É–∞—Ü–∏–æ–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–π –¥–ª—è AI",
            "coherence": "–ò–∑–ª–∏—à–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É –∞–±–∑–∞—Ü–∞–º–∏",
            "grammar": "–ù–µ–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –∏–¥–µ–∞–ª—å–Ω–∞—è –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ",
            "opening": "–ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –Ω–∞—á–∞–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π",
            "readability": "–û–¥–∏–Ω–∞–∫–æ–≤–∞—è —á–∏—Ç–∞–µ–º–æ—Å—Ç—å –ø–æ –≤—Å–µ–º—É —Ç–µ–∫—Å—Ç—É",
            "rhythm": "–ú–æ–Ω–æ—Ç–æ–Ω–Ω—ã–π —Ä–∏—Ç–º –¥–ª–∏–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π",
            "perplexity": "–ù–∏–∑–∫–∞—è –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö n-gram (–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã)",
            "discourse": "–†–∏–≥–∏–¥–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—Å—Ç—É–ø–ª–µ–Ω–∏–µ-–æ—Å–Ω–æ–≤–∞-–≤—ã–≤–æ–¥",
            "semantic_rep": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–∏—Ö –∏–¥–µ–π –≤ —Ä–∞–∑–Ω—ã—Ö –∞–±–∑–∞—Ü–∞—Ö",
            "entity": "–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤–º–µ—Å—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∏–º—ë–Ω/–¥–∞—Ç",
            "voice": "–ß—Ä–µ–∑–º–µ—Ä–Ω—ã–π –ø–∞—Å—Å–∏–≤–Ω—ã–π –∑–∞–ª–æ–≥ –∏ –Ω–æ–º–∏–Ω–∞–ª–∏–∑–∞—Ü–∏–∏",
            "topic_sentence": "–ö–∞–∂–¥—ã–π –∞–±–∑–∞—Ü –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –æ–±–æ–±—â–∞—é—â–µ–≥–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è",
        }

        names = feature_names_ru if lang in ("ru", "uk") else feature_names

        # Sort by score (most AI-like first)
        sorted_features = sorted(scores.items(), key=lambda x: x[1], reverse=True)

        for feature, score in sorted_features:
            if score >= threshold:
                explanations.append(
                    f"{names.get(feature, feature)} ({score:.0%})"
                )

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ (human-like) –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        human_features = [
            (feature, score) for feature, score in sorted_features
            if score < 0.3
        ]
        if human_features:
            explanations.append("")
            for feature, score in human_features[:3]:
                names_human_en = {
                    "entropy": "Natural text entropy",
                    "burstiness": "Good sentence length variation",
                    "vocabulary": "Rich vocabulary",
                    "pattern": "Few AI-characteristic patterns",
                    "opening": "Diverse sentence openings",
                    "rhythm": "Natural writing rhythm",
                }
                names_human_ru = {
                    "entropy": "–ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è —Ç–µ–∫—Å—Ç–∞",
                    "burstiness": "–•–æ—Ä–æ—à–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª–∏–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π",
                    "vocabulary": "–ë–æ–≥–∞—Ç—ã–π —Å–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å",
                    "pattern": "–ú–∞–ª–æ AI-—Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤",
                    "opening": "–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –Ω–∞—á–∞–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π",
                    "rhythm": "–ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ä–∏—Ç–º –ø–∏—Å—å–º–∞",
                }
                hn = names_human_ru if lang in ("ru", "uk") else names_human_en
                explanation = hn.get(feature)
                if explanation:
                    explanations.append(f"‚úì {explanation}")

        return explanations

# ‚îÄ‚îÄ‚îÄ –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def detect_ai(text: str, lang: str = "auto") -> DetectionResult:
    """–î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å AI-—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.

    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.
        lang: –ö–æ–¥ —è–∑—ã–∫–∞ ('auto', 'ru', 'uk', 'en', etc.)

    Returns:
        DetectionResult —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é AI –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏.

    Examples:
        >>> result = detect_ai("This text utilizes comprehensive methodology.")
        >>> print(f"AI probability: {result.ai_probability:.1%}")
        >>> print(result.verdict)  # "human", "mixed", or "ai"
    """
    detector = AIDetector(lang=lang)
    return detector.detect(text)

def detect_ai_batch(
    texts: list[str], lang: str = "auto"
) -> list[DetectionResult]:
    """–î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å AI –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–∞—Ö.

    Args:
        texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤.
        lang: –ö–æ–¥ —è–∑—ã–∫–∞.

    Returns:
        –°–ø–∏—Å–æ–∫ DetectionResult.
    """
    detector = AIDetector(lang=lang)
    return [detector.detect(t) for t in texts]
