"""CLI-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å TextHumanize."""

from __future__ import annotations

import argparse
import json
import logging
import sys
from typing import Any

from texthumanize import __version__
from texthumanize.core import (
    adjust_tone,
    analyze,
    analyze_coherence,
    analyze_tone,
    detect_ai,
    detect_watermarks,
    explain,
    full_readability,
    humanize,
    paraphrase,
    spin,
    spin_variants,
)

logger = logging.getLogger(__name__)


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ CLI."""
    parser = argparse.ArgumentParser(
        prog="texthumanize",
        description="TextHumanize ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∞—è –≥—É–º–∞–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞. "
        "–î–µ–ª–∞–µ—Ç AI-—Ç–µ–∫—Å—Ç—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ –¥–ª—è —á—Ç–µ–Ω–∏—è.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã:
  texthumanize input.txt
  texthumanize input.txt -l ru -p chat -i 80
  texthumanize input.txt -o output.txt --report report.json
  texthumanize input.txt --keep "RankBot AI" "Promopilot"
  texthumanize --analyze input.txt
  texthumanize detect input.txt
  texthumanize detect input.txt --verbose
  echo "–¢–µ–∫—Å—Ç" | texthumanize detect -
  echo "–¢–µ–∫—Å—Ç" | texthumanize -
        """,
    )

    parser.add_argument(
        "input",
        help="–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (–∏–ª–∏ '-' –¥–ª—è stdin), –∏–ª–∏ 'detect' –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ AI",
    )
    parser.add_argument(
        "-o", "--output",
        help="–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é stdout)",
    )
    parser.add_argument(
        "-l", "--lang",
        default="auto",
        choices=["auto", "ru", "uk", "en", "de", "fr", "es", "pl", "pt", "it"],
        help="–Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: auto)",
    )
    parser.add_argument(
        "-p", "--profile",
        default="web",
        choices=["chat", "web", "seo", "docs", "formal",
                 "academic", "marketing", "social", "email"],
        help="–ü—Ä–æ—Ñ–∏–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: web)",
    )
    parser.add_argument(
        "-i", "--intensity",
        type=int,
        default=60,
        help="–ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ 0-100 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 60)",
    )
    parser.add_argument(
        "--keep",
        nargs="*",
        default=[],
        help="–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞/—Ç–µ—Ä–º–∏–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–ª—å–∑—è –º–µ–Ω—è—Ç—å",
    )
    parser.add_argument(
        "--brand",
        nargs="*",
        default=[],
        help="–ë—Ä–µ–Ω–¥–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è –∑–∞—â–∏—Ç—ã",
    )
    parser.add_argument(
        "--max-change",
        type=float,
        default=0.4,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏–π 0-1 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.4)",
    )
    parser.add_argument(
        "--report",
        help="–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á—ë—Ç–∞ (JSON)",
    )
    parser.add_argument(
        "--analyze",
        action="store_true",
        help="–¢–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑ –±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏",
    )
    parser.add_argument(
        "--explain",
        action="store_true",
        help="–ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á—ë—Ç –æ–± –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö",
    )
    parser.add_argument(
        "--detect-ai",
        action="store_true",
        help="–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏—é",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ (–¥–ª—è detect-ai / detect)",
    )
    parser.add_argument(
        "--paraphrase",
        action="store_true",
        help="–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç",
    )
    parser.add_argument(
        "--tone",
        metavar="TARGET",
        help="–°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å (neutral, formal, casual, academic, marketing)",
    )
    parser.add_argument(
        "--tone-analyze",
        action="store_true",
        help="–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏",
    )
    parser.add_argument(
        "--watermarks",
        action="store_true",
        help="–û–±–Ω–∞—Ä—É–∂–∏—Ç—å –∏ —É–¥–∞–ª–∏—Ç—å –≤–æ–¥—è–Ω—ã–µ –∑–Ω–∞–∫–∏",
    )
    parser.add_argument(
        "--spin",
        action="store_true",
        help="–°–ø–∏–Ω–Ω–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞",
    )
    parser.add_argument(
        "--variants",
        type=int,
        metavar="N",
        help="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è N –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å–ø–∏–Ω–Ω–∏–Ω–≥–∞",
    )
    parser.add_argument(
        "--coherence",
        action="store_true",
        help="–ê–Ω–∞–ª–∏–∑ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏",
    )
    parser.add_argument(
        "--readability",
        action="store_true",
        help="–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏",
    )
    parser.add_argument(
        "--api",
        action="store_true",
        help="–ó–∞–ø—É—Å—Ç–∏—Ç—å API-—Å–µ—Ä–≤–µ—Ä",
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8080,
        help="–ü–æ—Ä—Ç API-—Å–µ—Ä–≤–µ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 8080)",
    )
    parser.add_argument(
        "--seed",
        type=int,
        help="–°–∏–¥ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
    )
    parser.add_argument(
        "-v", "--version",
        action="version",
        version=f"texthumanize {__version__}",
    )

    args, remaining = parser.parse_known_args()

    # ‚îÄ‚îÄ Handle detect subcommand ‚îÄ‚îÄ
    if args.input == 'detect':
        _handle_detect_command(args, remaining)
        return

    # API-—Å–µ—Ä–≤–µ—Ä (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç input)
    if getattr(args, 'api', False):
        from texthumanize.api import run_server
        run_server(port=args.port)
        return

    # –ß—Ç–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    if args.input == "-":
        text = sys.stdin.read()
    else:
        try:
            with open(args.input, "r", encoding="utf-8") as f:
                text = f.read()
        except FileNotFoundError:
            print(f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª '{args.input}' –Ω–µ –Ω–∞–π–¥–µ–Ω", file=sys.stderr)
            sys.exit(1)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}", file=sys.stderr)
            sys.exit(1)

    # AI Detection
    result: Any
    if getattr(args, 'detect_ai', False):
        result = detect_ai(text, lang=args.lang)
        print(json.dumps(result, ensure_ascii=False, indent=2))
        return

    # Paraphrase
    if getattr(args, 'paraphrase', False):
        result = paraphrase(text, lang=args.lang, intensity=args.intensity / 100.0)
        _output_text(result, args)
        return

    # Tone analysis
    if getattr(args, 'tone_analyze', False):
        result = analyze_tone(text, lang=args.lang)
        print(json.dumps(result, ensure_ascii=False, indent=2))
        return

    # Tone adjustment
    if getattr(args, 'tone', None):
        result = adjust_tone(text, target=args.tone, lang=args.lang)
        _output_text(result, args)
        return

    # Watermarks
    if getattr(args, 'watermarks', False):
        result = detect_watermarks(text, lang=args.lang)
        if result['has_watermarks']:
            print(json.dumps(result, ensure_ascii=False, indent=2), file=sys.stderr)
            _output_text(result['cleaned_text'], args)
        else:
            print('–í–æ–¥—è–Ω—ã–µ –∑–Ω–∞–∫–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã.', file=sys.stderr)
            _output_text(text, args)
        return

    # Spin
    if getattr(args, 'spin', False):
        result = spin(text, lang=args.lang, intensity=args.intensity / 100.0)
        _output_text(result, args)
        return

    # Spin variants
    if getattr(args, 'variants', None):
        results = spin_variants(text, count=args.variants, lang=args.lang)
        for i, v in enumerate(results, 1):
            print(f"--- –í–∞—Ä–∏–∞–Ω—Ç {i} ---")
            print(v)
            print()
        return

    # Coherence
    if getattr(args, 'coherence', False):
        result = analyze_coherence(text, lang=args.lang)
        print(json.dumps(result, ensure_ascii=False, indent=2))
        return

    # Readability
    if getattr(args, 'readability', False):
        result = full_readability(text, lang=args.lang)
        print(json.dumps(result, ensure_ascii=False, indent=2))
        return

    # –†–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞
    if args.analyze:
        report = analyze(text, lang=args.lang)
        output = {
            "lang": report.lang,
            "total_chars": report.total_chars,
            "total_words": report.total_words,
            "total_sentences": report.total_sentences,
            "avg_sentence_length": round(report.avg_sentence_length, 2),
            "sentence_length_variance": round(report.sentence_length_variance, 2),
            "bureaucratic_ratio": round(report.bureaucratic_ratio, 4),
            "connector_ratio": round(report.connector_ratio, 4),
            "repetition_score": round(report.repetition_score, 4),
            "typography_score": round(report.typography_score, 4),
            "artificiality_score": round(report.artificiality_score, 2),
            "details": {
                "found_bureaucratic": report.details.get("found_bureaucratic", []),
                "found_connectors": report.details.get("found_connectors", []),
                "typography_issues": report.details.get("typography_issues", []),
            },
        }
        print(json.dumps(output, ensure_ascii=False, indent=2))
        return

    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    result = humanize(
        text,
        lang=args.lang,
        profile=args.profile,
        intensity=args.intensity,
        preserve={
            "brand_terms": args.brand,
        },
        constraints={
            "max_change_ratio": args.max_change,
            "keep_keywords": args.keep,
        },
        seed=args.seed,
    )

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    if args.output:
        try:
            with open(args.output, "w", encoding="utf-8") as f:
                f.write(result.text)
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {args.output}", file=sys.stderr)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞: {e}", file=sys.stderr)
            sys.exit(1)
    else:
        print(result.text)

    # –û—Ç—á—ë—Ç –æ–± –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö
    if args.explain:
        report_text = explain(result)
        print("\n" + report_text, file=sys.stderr)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞
    if args.report:
        report_data = {
            "lang": result.lang,
            "profile": result.profile,
            "intensity": result.intensity,
            "change_ratio": round(result.change_ratio, 4),
            "changes_count": len(result.changes),
            "changes": result.changes[:50],
            "metrics_before": result.metrics_before,
            "metrics_after": result.metrics_after,
        }
        try:
            with open(args.report, "w", encoding="utf-8") as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            print(f"–û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {args.report}", file=sys.stderr)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –æ—Ç—á—ë—Ç–∞: {e}", file=sys.stderr)


def _handle_detect_command(args, remaining: list[str]) -> None:
    """Handle 'texthumanize detect [file] [--verbose] [--json]' command."""
    detect_input = "-"
    use_json = False
    verbose = getattr(args, 'verbose', False)

    for a in remaining:
        if a == "--json":
            use_json = True
        elif a == "--verbose":
            verbose = True
        elif not a.startswith("-"):
            detect_input = a

    if detect_input == "-":
        text = sys.stdin.read()
    else:
        try:
            with open(detect_input, "r", encoding="utf-8") as f:
                text = f.read()
        except FileNotFoundError:
            print(f"Error: file '{detect_input}' not found", file=sys.stderr)
            sys.exit(1)
        except Exception as e:
            print(f"Error reading file: {e}", file=sys.stderr)
            sys.exit(1)

    lang = args.lang if hasattr(args, 'lang') else "auto"
    result = detect_ai(text, lang=lang)

    if use_json:
        print(json.dumps(result, ensure_ascii=False, indent=2))
        return

    # Human-readable output
    verdict_icons = {"ai": "ü§ñ", "human": "üë§", "mixed": "üîÄ", "unknown": "‚ùì"}
    icon = verdict_icons.get(result["verdict"], "")

    print(f"\n  {icon} Verdict: {result['verdict'].upper()}")
    print(f"  AI Probability: {result['score']:.1%}")
    print(f"  Confidence: {result['confidence']:.1%}")

    if verbose:
        print("\n  Metrics (0.0=human, 1.0=AI):")
        for metric, val in result["metrics"].items():
            bar = "‚ñà" * int(val * 20) + "‚ñë" * (20 - int(val * 20))
            print(f"    {metric:25s} {bar} {val:.2f}")

        if result.get("explanations"):
            print("\n  Key findings:")
            for exp in result["explanations"]:
                if exp:
                    print(f"    ‚Ä¢ {exp}")

    print()


def _output_text(text: str, args) -> None:
    """Output text to file or stdout."""
    if args.output:
        try:
            with open(args.output, "w", encoding="utf-8") as f:
                f.write(text)
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {args.output}", file=sys.stderr)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞: {e}", file=sys.stderr)
            sys.exit(1)
    else:
        print(text)


if __name__ == "__main__":
    main()
