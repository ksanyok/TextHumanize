"""CLI-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å TextHumanize."""

from __future__ import annotations

import argparse
import json
import logging
import sys
from typing import Any

from texthumanize import __version__
from texthumanize.core import (
    adjust_tone,
    analyze,
    analyze_coherence,
    analyze_tone,
    detect_ai,
    detect_watermarks,
    explain,
    full_readability,
    humanize,
    paraphrase,
    spin,
    spin_variants,
)

logger = logging.getLogger(__name__)


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ CLI."""
    parser = argparse.ArgumentParser(
        prog="texthumanize",
        description="TextHumanize ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∞—è –≥—É–º–∞–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞. "
        "–î–µ–ª–∞–µ—Ç AI-—Ç–µ–∫—Å—Ç—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ –¥–ª—è —á—Ç–µ–Ω–∏—è.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã:
  texthumanize input.txt
  texthumanize input.txt -l ru -p chat -i 80
  texthumanize input.txt -o output.txt --report report.json
  texthumanize input.txt --keep "RankBot AI" "Promopilot"
  texthumanize --analyze input.txt
  texthumanize detect input.txt
  texthumanize detect input.txt --verbose
  echo "–¢–µ–∫—Å—Ç" | texthumanize detect -
  echo "–¢–µ–∫—Å—Ç" | texthumanize -
        """,
    )

    parser.add_argument(
        "input",
        help="–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (–∏–ª–∏ '-' –¥–ª—è stdin), –∏–ª–∏ 'detect' –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ AI",
    )
    parser.add_argument(
        "-o", "--output",
        help="–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é stdout)",
    )
    parser.add_argument(
        "-l", "--lang",
        default="auto",
        choices=["auto", "ru", "uk", "en", "de", "fr", "es", "pl", "pt", "it"],
        help="–Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: auto)",
    )
    parser.add_argument(
        "-p", "--profile",
        default="web",
        choices=["chat", "web", "seo", "docs", "formal",
                 "academic", "marketing", "social", "email"],
        help="–ü—Ä–æ—Ñ–∏–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: web)",
    )
    parser.add_argument(
        "-i", "--intensity",
        type=int,
        default=60,
        help="–ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ 0-100 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 60)",
    )
    parser.add_argument(
        "--keep",
        nargs="*",
        default=[],
        help="–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞/—Ç–µ—Ä–º–∏–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–ª—å–∑—è –º–µ–Ω—è—Ç—å",
    )
    parser.add_argument(
        "--brand",
        nargs="*",
        default=[],
        help="–ë—Ä–µ–Ω–¥–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è –∑–∞—â–∏—Ç—ã",
    )
    parser.add_argument(
        "--max-change",
        type=float,
        default=0.4,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏–π 0-1 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.4)",
    )
    parser.add_argument(
        "--report",
        help="–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á—ë—Ç–∞ (JSON)",
    )
    parser.add_argument(
        "--analyze",
        action="store_true",
        help="–¢–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑ –±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏",
    )
    parser.add_argument(
        "--explain",
        action="store_true",
        help="–ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á—ë—Ç –æ–± –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö",
    )
    parser.add_argument(
        "--detect-ai",
        action="store_true",
        help="–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏—é",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ (–¥–ª—è detect-ai / detect)",
    )
    parser.add_argument(
        "--paraphrase",
        action="store_true",
        help="–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç",
    )
    parser.add_argument(
        "--tone",
        metavar="TARGET",
        help="–°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å (neutral, formal, casual, academic, marketing)",
    )
    parser.add_argument(
        "--tone-analyze",
        action="store_true",
        help="–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏",
    )
    parser.add_argument(
        "--watermarks",
        action="store_true",
        help="–û–±–Ω–∞—Ä—É–∂–∏—Ç—å –∏ —É–¥–∞–ª–∏—Ç—å –≤–æ–¥—è–Ω—ã–µ –∑–Ω–∞–∫–∏",
    )
    parser.add_argument(
        "--spin",
        action="store_true",
        help="–°–ø–∏–Ω–Ω–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞",
    )
    parser.add_argument(
        "--variants",
        type=int,
        metavar="N",
        help="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è N –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å–ø–∏–Ω–Ω–∏–Ω–≥–∞",
    )
    parser.add_argument(
        "--coherence",
        action="store_true",
        help="–ê–Ω–∞–ª–∏–∑ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏",
    )
    parser.add_argument(
        "--readability",
        action="store_true",
        help="–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏",
    )
    parser.add_argument(
        "--api",
        action="store_true",
        help="–ó–∞–ø—É—Å—Ç–∏—Ç—å API-—Å–µ—Ä–≤–µ—Ä",
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8080,
        help="–ü–æ—Ä—Ç API-—Å–µ—Ä–≤–µ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 8080)",
    )
    parser.add_argument(
        "--seed",
        type=int,
        help="–°–∏–¥ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
    )
    parser.add_argument(
        "-v", "--version",
        action="version",
        version=f"texthumanize {__version__}",
    )

    args, remaining = parser.parse_known_args()

    # ‚îÄ‚îÄ Handle detect subcommand ‚îÄ‚îÄ
    if args.input == 'detect':
        _handle_detect_command(args, remaining)
        return

    # ‚îÄ‚îÄ Handle benchmark subcommand ‚îÄ‚îÄ
    if args.input == 'benchmark':
        _handle_benchmark_command(args, remaining)
        return

    # API-—Å–µ—Ä–≤–µ—Ä (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç input)
    if getattr(args, 'api', False):
        from texthumanize.api import run_server
        run_server(port=args.port)
        return

    # –ß—Ç–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    if args.input == "-":
        text = sys.stdin.read()
    else:
        try:
            with open(args.input, "r", encoding="utf-8") as f:
                text = f.read()
        except FileNotFoundError:
            print(f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª '{args.input}' –Ω–µ –Ω–∞–π–¥–µ–Ω", file=sys.stderr)
            sys.exit(1)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}", file=sys.stderr)
            sys.exit(1)

    # AI Detection
    result: Any
    if getattr(args, 'detect_ai', False):
        result = detect_ai(text, lang=args.lang)
        print(json.dumps(result, ensure_ascii=False, indent=2))
        return

    # Paraphrase
    if getattr(args, 'paraphrase', False):
        result = paraphrase(text, lang=args.lang, intensity=args.intensity / 100.0)
        _output_text(result, args)
        return

    # Tone analysis
    if getattr(args, 'tone_analyze', False):
        result = analyze_tone(text, lang=args.lang)
        print(json.dumps(result, ensure_ascii=False, indent=2))
        return

    # Tone adjustment
    if getattr(args, 'tone', None):
        result = adjust_tone(text, target=args.tone, lang=args.lang)
        _output_text(result, args)
        return

    # Watermarks
    if getattr(args, 'watermarks', False):
        result = detect_watermarks(text, lang=args.lang)
        if result['has_watermarks']:
            print(json.dumps(result, ensure_ascii=False, indent=2), file=sys.stderr)
            _output_text(result['cleaned_text'], args)
        else:
            print('–í–æ–¥—è–Ω—ã–µ –∑–Ω–∞–∫–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã.', file=sys.stderr)
            _output_text(text, args)
        return

    # Spin
    if getattr(args, 'spin', False):
        result = spin(text, lang=args.lang, intensity=args.intensity / 100.0)
        _output_text(result, args)
        return

    # Spin variants
    if getattr(args, 'variants', None):
        results = spin_variants(text, count=args.variants, lang=args.lang)
        for i, v in enumerate(results, 1):
            print(f"--- –í–∞—Ä–∏–∞–Ω—Ç {i} ---")
            print(v)
            print()
        return

    # Coherence
    if getattr(args, 'coherence', False):
        result = analyze_coherence(text, lang=args.lang)
        print(json.dumps(result, ensure_ascii=False, indent=2))
        return

    # Readability
    if getattr(args, 'readability', False):
        result = full_readability(text, lang=args.lang)
        print(json.dumps(result, ensure_ascii=False, indent=2))
        return

    # –†–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞
    if args.analyze:
        report = analyze(text, lang=args.lang)
        output = {
            "lang": report.lang,
            "total_chars": report.total_chars,
            "total_words": report.total_words,
            "total_sentences": report.total_sentences,
            "avg_sentence_length": round(report.avg_sentence_length, 2),
            "sentence_length_variance": round(report.sentence_length_variance, 2),
            "bureaucratic_ratio": round(report.bureaucratic_ratio, 4),
            "connector_ratio": round(report.connector_ratio, 4),
            "repetition_score": round(report.repetition_score, 4),
            "typography_score": round(report.typography_score, 4),
            "artificiality_score": round(report.artificiality_score, 2),
            "details": {
                "found_bureaucratic": report.details.get("found_bureaucratic", []),
                "found_connectors": report.details.get("found_connectors", []),
                "typography_issues": report.details.get("typography_issues", []),
            },
        }
        print(json.dumps(output, ensure_ascii=False, indent=2))
        return

    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    result = humanize(
        text,
        lang=args.lang,
        profile=args.profile,
        intensity=args.intensity,
        preserve={
            "brand_terms": args.brand,
        },
        constraints={
            "max_change_ratio": args.max_change,
            "keep_keywords": args.keep,
        },
        seed=args.seed,
    )

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    if args.output:
        try:
            with open(args.output, "w", encoding="utf-8") as f:
                f.write(result.text)
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {args.output}", file=sys.stderr)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞: {e}", file=sys.stderr)
            sys.exit(1)
    else:
        print(result.text)

    # –û—Ç—á—ë—Ç –æ–± –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö
    if args.explain:
        report_text = explain(result)
        print("\n" + report_text, file=sys.stderr)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞
    if args.report:
        report_data = {
            "lang": result.lang,
            "profile": result.profile,
            "intensity": result.intensity,
            "change_ratio": round(result.change_ratio, 4),
            "changes_count": len(result.changes),
            "changes": result.changes[:50],
            "metrics_before": result.metrics_before,
            "metrics_after": result.metrics_after,
        }
        try:
            with open(args.report, "w", encoding="utf-8") as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            print(f"–û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {args.report}", file=sys.stderr)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –æ—Ç—á—ë—Ç–∞: {e}", file=sys.stderr)


def _handle_detect_command(args, remaining: list[str]) -> None:
    """Handle 'texthumanize detect [file] [--verbose] [--json]' command."""
    detect_input = "-"
    use_json = False
    verbose = getattr(args, 'verbose', False)

    for a in remaining:
        if a == "--json":
            use_json = True
        elif a == "--verbose":
            verbose = True
        elif not a.startswith("-"):
            detect_input = a

    if detect_input == "-":
        text = sys.stdin.read()
    else:
        try:
            with open(detect_input, "r", encoding="utf-8") as f:
                text = f.read()
        except FileNotFoundError:
            print(f"Error: file '{detect_input}' not found", file=sys.stderr)
            sys.exit(1)
        except Exception as e:
            print(f"Error reading file: {e}", file=sys.stderr)
            sys.exit(1)

    lang = args.lang if hasattr(args, 'lang') else "auto"
    result = detect_ai(text, lang=lang)

    if use_json:
        print(json.dumps(result, ensure_ascii=False, indent=2))
        return

    # Human-readable output
    verdict_icons = {"ai": "ü§ñ", "human": "üë§", "mixed": "üîÄ", "unknown": "‚ùì"}
    icon = verdict_icons.get(result["verdict"], "")

    print(f"\n  {icon} Verdict: {result['verdict'].upper()}")
    print(f"  AI Probability: {result['score']:.1%}")
    print(f"  Confidence: {result['confidence']:.1%}")

    if verbose:
        print("\n  Metrics (0.0=human, 1.0=AI):")
        for metric, val in result["metrics"].items():
            bar = "‚ñà" * int(val * 20) + "‚ñë" * (20 - int(val * 20))
            print(f"    {metric:25s} {bar} {val:.2f}")

        if result.get("explanations"):
            print("\n  Key findings:")
            for exp in result["explanations"]:
                if exp:
                    print(f"    ‚Ä¢ {exp}")

    print()


def _handle_benchmark_command(args, remaining: list[str]) -> None:
    """Handle 'texthumanize benchmark' ‚Äî run comprehensive quality/speed benchmarks."""
    import time as _time

    use_json = "--json" in remaining
    lang = args.lang if hasattr(args, "lang") and args.lang != "auto" else "en"

    # --- Sample texts for benchmarking ---
    _en_short = (
        "Furthermore, it is important to note that the "
        "implementation of this approach facilitates optimization."
    )
    _en_medium = (
        "Furthermore, it is important to note that the "
        "implementation of cloud computing facilitates the "
        "optimization of business processes. Additionally, the "
        "utilization of microservices constitutes a significant "
        "advancement. Nevertheless, considerable challenges "
        "remain in the area of security. It is worth mentioning "
        "that these challenges necessitate comprehensive "
        "solutions. Moreover, the integration of artificial "
        "intelligence provides unprecedented opportunities "
        "for automation."
    )
    _ru_short = (
        "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –¥–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —è–≤–ª—è–µ—Ç—Å—è "
        "–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º –¥–ª—è –æ—Å—É—â–µ—Å—Ç–≤–ª–µ–Ω–∏—è "
        "–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á."
    )
    _ru_medium = (
        "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –¥–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —è–≤–ª—è–µ—Ç—Å—è "
        "–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º –¥–ª—è –æ—Å—É—â–µ—Å—Ç–≤–ª–µ–Ω–∏—è "
        "–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —Å–ª–µ–¥—É–µ—Ç –ø–æ–¥—á–µ—Ä–∫–Ω—É—Ç—å "
        "–≤–∞–∂–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–π. "
        "–í —Ä–∞–º–∫–∞—Ö –¥–∞–Ω–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –±—ã–ª–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ, —á—Ç–æ "
        "–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç "
        "–ø–æ–≤—ã—à–µ–Ω–∏—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, —Å—É—â–µ—Å—Ç–≤—É—é—Ç "
        "–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ "
        "—É—á–∏—Ç—ã–≤–∞—Ç—å."
    )
    samples = {
        "en": [
            ("short", _en_short),
            ("medium", _en_medium),
            ("long", (_en_medium + " ") * 3),
        ],
        "ru": [
            ("short", _ru_short),
            ("medium", _ru_medium),
            ("long", (_ru_medium + " ") * 3),
        ],
    }

    test_samples = samples.get(lang, samples["en"])

    if not use_json:
        print("=" * 60)
        print(f"  TextHumanize Benchmark ‚Äî v{__version__}")
        print(f"  Language: {lang}")
        print("=" * 60)

    total_chars = 0
    total_time_humanize = 0.0
    total_time_detect = 0.0
    quality_scores: list[float] = []
    change_ratios: list[float] = []
    ai_improvements: list[tuple[float, float]] = []
    results_data: list[dict] = []

    for label, sample_text in test_samples:
        chars = len(sample_text)
        total_chars += chars

        # Humanize benchmark
        t0 = _time.perf_counter()
        result = humanize(sample_text, lang=lang, profile="web", intensity=60, seed=42)
        t_humanize = _time.perf_counter() - t0
        total_time_humanize += t_humanize

        # AI detection benchmark (before & after)
        t0 = _time.perf_counter()
        ai_before = detect_ai(sample_text, lang=lang)
        t_detect = _time.perf_counter() - t0
        total_time_detect += t_detect

        ai_after = detect_ai(result.text, lang=lang)

        quality_scores.append(getattr(result, "quality_score", 0.0))
        change_ratios.append(getattr(result, "change_ratio", 0.0))
        ai_improvements.append((ai_before["score"], ai_after["score"]))

        row = {
            "label": label,
            "chars": chars,
            "humanize_ms": round(t_humanize * 1000, 1),
            "detect_ms": round(t_detect * 1000, 1),
            "throughput": round(chars / t_humanize) if t_humanize > 0 else 0,
            "change_ratio": round(getattr(result, "change_ratio", 0), 3),
            "quality_score": round(getattr(result, "quality_score", 0), 3),
            "ai_before": round(ai_before["score"], 3),
            "ai_after": round(ai_after["score"], 3),
            "verdict_before": ai_before["verdict"],
            "verdict_after": ai_after["verdict"],
        }
        results_data.append(row)

        if not use_json:
            print(f"\n  [{label}] {chars} chars")
            print(f"    Humanize: {row['humanize_ms']}ms ({row['throughput']:,} chars/sec)")
            print(f"    Detect:   {row['detect_ms']}ms")
            print(f"    Change:   {row['change_ratio']:.1%}")
            print(f"    Quality:  {row['quality_score']:.2f}")
            print(
                f"    AI score: {row['ai_before']:.0%}"
                f" ‚Üí {row['ai_after']:.0%}"
                f" ({row['verdict_before']}"
                f" ‚Üí {row['verdict_after']})"
            )

    # Determinism check
    r1 = humanize(test_samples[0][1], lang=lang, seed=12345)
    r2 = humanize(test_samples[0][1], lang=lang, seed=12345)
    deterministic = r1.text == r2.text

    # Summary
    avg_throughput = round(total_chars / total_time_humanize) if total_time_humanize > 0 else 0
    avg_quality = round(sum(quality_scores) / len(quality_scores), 3) if quality_scores else 0
    avg_change = round(sum(change_ratios) / len(change_ratios), 3) if change_ratios else 0
    avg_ai_drop = round(
        sum(b - a for b, a in ai_improvements) / len(ai_improvements), 3
    ) if ai_improvements else 0

    summary = {
        "version": __version__,
        "lang": lang,
        "total_chars": total_chars,
        "total_humanize_ms": round(total_time_humanize * 1000, 1),
        "total_detect_ms": round(total_time_detect * 1000, 1),
        "avg_throughput_chars_sec": avg_throughput,
        "avg_quality_score": avg_quality,
        "avg_change_ratio": avg_change,
        "avg_ai_score_drop": avg_ai_drop,
        "deterministic": deterministic,
        "samples": results_data,
    }

    if use_json:
        print(json.dumps(summary, ensure_ascii=False, indent=2))
    else:
        print("\n" + "=" * 60)
        print("  SUMMARY")
        print("=" * 60)
        print(f"  Total chars processed: {total_chars:,}")
        print(f"  Avg throughput:        {avg_throughput:,} chars/sec")
        print(f"  Avg quality score:     {avg_quality:.2f}")
        print(f"  Avg change ratio:      {avg_change:.1%}")
        print(f"  Avg AI score drop:     {avg_ai_drop:+.1%}")
        print(f"  Deterministic:         {'‚úÖ' if deterministic else '‚ùå'}")
        print("=" * 60)


def _output_text(text: str, args) -> None:
    """Output text to file or stdout."""
    if args.output:
        try:
            with open(args.output, "w", encoding="utf-8") as f:
                f.write(text)
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {args.output}", file=sys.stderr)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞: {e}", file=sys.stderr)
            sys.exit(1)
    else:
        print(text)


if __name__ == "__main__":
    main()
