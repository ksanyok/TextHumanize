"""Golden-—Ç–µ—Å—Ç—ã: —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—Ö–æ–¥—ã ‚Üí –æ–∂–∏–¥–∞–µ–º—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤—ã—Ö–æ–¥–∞."""

import pytest
from texthumanize import humanize, analyze


# –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã, —Ç–∏–ø–∏—á–Ω—ã–µ –¥–ª—è AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
AI_TEXT_RU = """
–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —è–≤–ª—è–µ—Ç—Å—è –æ–¥–Ω–æ–π –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç–∏. –î–∞–Ω–Ω–∞—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤. –û–¥–Ω–∞–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –ò–ò –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–ª–æ–∂–Ω—É—é –∑–∞–¥–∞—á—É.

–í –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è –∞–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ –¥–∞–Ω–Ω–æ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º –ò–ò –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –≤–∞–∂–Ω–æ –ø–æ–¥—á–µ—Ä–∫–Ω—É—Ç—å, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ò–ò –¥–æ–ª–∂–Ω–æ –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å—Å—è –Ω–∞–¥–ª–µ–∂–∞—â–∏–º –æ–±—Ä–∞–∑–æ–º.

–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –¥–∞–Ω–Ω–∞—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è —è–≤–ª—è–µ—Ç—Å—è —á—Ä–µ–∑–≤—ã—á–∞–π–Ω–æ –≤–∞–∂–Ω–æ–π. –ë–µ–∑—É—Å–ª–æ–≤–Ω–æ, –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º –æ–Ω–∞ –±—É–¥–µ—Ç –∏–≥—Ä–∞—Ç—å –µ—â—ë –±–æ–ª–µ–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é —Ä–æ–ª—å. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–¥–µ–ª—è—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –µ—ë —Ä–∞–∑–≤–∏—Ç–∏—é.
""".strip()

AI_TEXT_UK = """
–®—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç —î –æ–¥–Ω—ñ—î—é –∑ –Ω–∞–π–±—ñ–ª—å—à –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π —Å—É—á–∞—Å–Ω–æ—Å—Ç—ñ. –î–∞–Ω–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—è –∑–¥—ñ–π—Å–Ω—é—î –æ–±—Ä–æ–±–∫—É –≤–µ–ª–∏–∫–∏—Ö –æ–±—Å—è–≥—ñ–≤ –¥–∞–Ω–∏—Ö —Ç–∞ –Ω–∞–¥–∞—î –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó —Ä—ñ–∑–Ω–∏—Ö –ø—Ä–æ—Ü–µ—Å—ñ–≤. –û–¥–Ω–∞–∫ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ –∑–∞–∑–Ω–∞—á–∏—Ç–∏, —â–æ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è –®–Ü —è–≤–ª—è—î —Å–æ–±–æ—é —Å–∫–ª–∞–¥–Ω–µ –∑–∞–≤–¥–∞–Ω–Ω—è.

–ù–∞ –¥–∞–Ω–∏–π —á–∞—Å –∑–¥—ñ–π—Å–Ω—é—î—Ç—å—Å—è –∞–∫—Ç–∏–≤–Ω–∏–π —Ä–æ–∑–≤–∏—Ç–æ–∫ –¥–∞–Ω–æ–≥–æ –Ω–∞–ø—Ä—è–º–∫—É. –ë—ñ–ª—å—à —Ç–æ–≥–æ, —Ñ—É–Ω–∫—Ü—ñ–æ–Ω—É–≤–∞–Ω–Ω—è —Å–∏—Å—Ç–µ–º –®–Ü –∑–∞–±–µ–∑–ø–µ—á—É—î –∑–Ω–∞—á–Ω–µ –ø—ñ–¥–≤–∏—â–µ–Ω–Ω—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ. –¢–∏–º –Ω–µ –º–µ–Ω—à, –≤–∞–∂–ª–∏–≤–æ –ø—ñ–¥–∫—Ä–µ—Å–ª–∏—Ç–∏, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –®–Ü –ø–æ–≤–∏–Ω–Ω–æ –∑–¥—ñ–π—Å–Ω—é–≤–∞—Ç–∏—Å—è –Ω–∞–ª–µ–∂–Ω–∏–º —á–∏–Ω–æ–º.

–¢–∞–∫–∏–º —á–∏–Ω–æ–º, –¥–∞–Ω–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—è —î –Ω–∞–¥–∑–≤–∏—á–∞–π–Ω–æ –≤–∞–∂–ª–∏–≤–æ—é. –ë–µ–∑—É–º–æ–≤–Ω–æ, –Ω–∞–¥–∞–ª—ñ –≤–æ–Ω–∞ –±—É–¥–µ –≤—ñ–¥—ñ–≥—Ä–∞–≤–∞—Ç–∏ —â–µ –±—ñ–ª—å—à –∑–Ω–∞—á–Ω—É —Ä–æ–ª—å. –û—Ç–∂–µ, –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ –ø—Ä–∏–¥—ñ–ª—è—Ç–∏ —É–≤–∞–≥—É —ó—ó —Ä–æ–∑–≤–∏—Ç–∫—É.
""".strip()

AI_TEXT_EN = """
Artificial intelligence is one of the most promising technologies of the modern era. This technology utilizes the processing of large volumes of data and facilitates the automation of various processes. However, it is important to note that the implementation of AI represents a considerable challenge.

At the present time, the development of this field is being actively pursued. Furthermore, the functioning of AI systems ensures a significant increase in efficiency. Nevertheless, it should be noted that the utilization of AI must be carried out in an appropriate manner.

Thus, this technology is extremely important. Undoubtedly, it will play an even more significant role in the future. Consequently, it is necessary to pay attention to its development.
""".strip()


class TestGoldenRussian:
    """Golden-—Ç–µ—Å—Ç—ã –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞."""

    def test_typography_normalized(self):
        """–¢–∏–ø–æ–≥—Ä–∞—Ñ–∏–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç—Å—è."""
        result = humanize(AI_TEXT_RU, lang="ru", profile="web", seed=42)
        # –î–ª–∏–Ω–Ω—ã–µ —Ç–∏—Ä–µ –Ω–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ web-–ø—Ä–æ—Ñ–∏–ª–µ
        # (–µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ)
        assert isinstance(result.text, str)
        assert len(result.text) > 100

    def test_bureaucratic_reduced(self):
        """–ö–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º—ã —É–º–µ–Ω—å—à–µ–Ω—ã."""
        result = humanize(AI_TEXT_RU, lang="ru", profile="chat", intensity=80, seed=42)
        text_lower = result.text.lower()
        # –•–æ—Ç—è –±—ã –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–∞–º–µ–Ω–µ–Ω—ã
        # –ù–µ –≤—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∑–∞–º–µ–Ω—è—Ç—Å—è (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç random), –Ω–æ —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –æ—Ç–ª–∏—á–∞—Ç—å—Å—è
        assert result.text != AI_TEXT_RU

    def test_artificiality_decreases(self):
        """–û—Ü–µ–Ω–∫–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ —Å–Ω–∏–∂–∞–µ—Ç—Å—è."""
        report_before = analyze(AI_TEXT_RU, lang="ru")
        result = humanize(AI_TEXT_RU, lang="ru", profile="web", intensity=70, seed=42)
        report_after = analyze(result.text, lang="ru")

        # –û—Ü–µ–Ω–∫–∞ –¥–æ–ª–∂–Ω–∞ —Å–Ω–∏–∑–∏—Ç—å—Å—è –∏–ª–∏ —Ö–æ—Ç—è –±—ã –Ω–µ –≤—ã—Ä–∞—Å—Ç–∏ —Å–∏–ª—å–Ω–æ
        assert report_after.artificiality_score <= report_before.artificiality_score + 10

    def test_preserves_meaning(self):
        """–°–º—ã—Å–ª —Ç–µ–∫—Å—Ç–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è (—Ç–µ–∫—Å—Ç –≤—Å—ë –µ—â—ë –ø—Ä–æ –ò–ò)."""
        result = humanize(AI_TEXT_RU, lang="ru", profile="web", seed=42)
        # –ö–ª—é—á–µ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è –¥–æ–ª–∂–Ω—ã –æ—Å—Ç–∞—Ç—å—Å—è
        text_lower = result.text.lower()
        assert "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç" in text_lower or "–∏–∏" in text_lower or "—ñ—ó" in text_lower

    def test_not_too_many_changes(self):
        """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö."""
        result = humanize(AI_TEXT_RU, lang="ru", profile="web", intensity=60, seed=42)
        assert result.change_ratio <= 0.75  # –ù–µ –±–æ–ª–µ–µ 75% –∏–∑–º–µ–Ω–µ–Ω–∏–π

    def test_sentences_still_valid(self):
        """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤—Å—ë –µ—â—ë –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ (–∑–∞–∫–∞–Ω—á–∏–≤–∞—é—Ç—Å—è –Ω–∞ .!?)."""
        result = humanize(AI_TEXT_RU, lang="ru", profile="web", seed=42)
        # –£ —Ç–µ–∫—Å—Ç–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ç–æ—á–∫–∏
        assert '.' in result.text


class TestGoldenUkrainian:
    """Golden-—Ç–µ—Å—Ç—ã –¥–ª—è —É–∫—Ä–∞–∏–Ω—Å–∫–æ–≥–æ —è–∑—ã–∫–∞."""

    def test_basic_processing(self):
        """–£–∫—Ä–∞–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è."""
        result = humanize(AI_TEXT_UK, lang="uk", profile="web", seed=42)
        assert isinstance(result.text, str)
        assert result.text != AI_TEXT_UK
        assert len(result.text) > 100

    def test_language_detected(self):
        """–£–∫—Ä–∞–∏–Ω—Å–∫–∏–π —è–∑—ã–∫ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è."""
        result = humanize(AI_TEXT_UK, lang="auto", seed=42)
        assert result.lang == "uk"

    def test_artificiality_decreases(self):
        """–û—Ü–µ–Ω–∫–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ —Å–Ω–∏–∂–∞–µ—Ç—Å—è."""
        report_before = analyze(AI_TEXT_UK, lang="uk")
        result = humanize(AI_TEXT_UK, lang="uk", profile="web", intensity=70, seed=42)
        report_after = analyze(result.text, lang="uk")
        assert report_after.artificiality_score <= report_before.artificiality_score + 10


class TestGoldenEnglish:
    """Golden-—Ç–µ—Å—Ç—ã –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞."""

    def test_basic_processing(self):
        """–ê–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è."""
        result = humanize(AI_TEXT_EN, lang="en", profile="web", seed=42)
        assert isinstance(result.text, str)
        assert result.text != AI_TEXT_EN
        assert len(result.text) > 100

    def test_language_detected(self):
        """–ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è."""
        result = humanize(AI_TEXT_EN, lang="auto", seed=42)
        assert result.lang == "en"


class TestPropertyBased:
    """Property-—Ç–µ—Å—Ç—ã: –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å–µ–≥–¥–∞ –¥–æ–ª–∂–Ω—ã –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è."""

    @pytest.mark.parametrize("lang", ["ru", "uk", "en"])
    def test_urls_preserved(self, lang):
        """URL —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ."""
        url = "https://example.com/path?key=value&foo=bar"
        text = f"–¢–µ–∫—Å—Ç –Ω–∞—á–∞–ª–æ {url} —Ç–µ–∫—Å—Ç –∫–æ–Ω–µ—Ü."
        result = humanize(text, lang=lang, seed=42)
        assert url in result.text

    @pytest.mark.parametrize("lang", ["ru", "uk", "en"])
    def test_emails_preserved(self, lang):
        """Email —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ."""
        email = "test@example.com"
        text = f"–ü–∏—à–∏—Ç–µ –Ω–∞ {email} –¥–ª—è —Å–≤—è–∑–∏."
        result = humanize(text, lang=lang, seed=42)
        assert email in result.text

    @pytest.mark.parametrize("lang", ["ru", "uk", "en"])
    def test_code_blocks_preserved(self, lang):
        """–ë–ª–æ–∫–∏ –∫–æ–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è."""
        text = "–¢–µ–∫—Å—Ç:\n```\ncode here\n```\n–ö–æ–Ω–µ—Ü."
        result = humanize(text, lang=lang, seed=42)
        assert "```" in result.text
        assert "code here" in result.text

    def test_emoji_safe(self):
        """–≠–º–æ–¥–∑–∏ –Ω–µ –ª–æ–º–∞—é—Ç –ø–∞—Ä—Å–µ—Ä."""
        text = "–¢–µ–∫—Å—Ç —Å–æ —Å–º–∞–π–ª–æ–º üòÄ –∏ –µ—â—ë üéâ –∏ –∫–æ–Ω–µ—Ü."
        result = humanize(text, lang="ru", seed=42)
        assert "üòÄ" in result.text
        assert "üéâ" in result.text

    def test_numbers_preserved(self):
        """–ß–∏—Å–ª–∞ –Ω–µ —Ç–µ—Ä—è—é—Ç—Å—è."""
        text = "–í 2024 –≥–æ–¥—É –±—ã–ª–æ 150 —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∏ 3.5 –º–ª–Ω –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤."
        result = humanize(text, lang="ru", seed=42)
        assert "2024" in result.text
        assert "150" in result.text

    def test_multiline_text(self):
        """–ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π —Ç–µ–∫—Å—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è."""
        text = (
            "–ü–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü —Ç–µ–∫—Å—Ç–∞.\n\n"
            "–í—Ç–æ—Ä–æ–π –∞–±–∑–∞—Ü —Ç–µ–∫—Å—Ç–∞.\n\n"
            "–¢—Ä–µ—Ç–∏–π –∞–±–∑–∞—Ü."
        )
        result = humanize(text, lang="ru", seed=42)
        assert isinstance(result.text, str)
        assert len(result.text) > 10

    def test_mixed_content(self):
        """–°–º–µ—à–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (markdown + —Ç–µ–∫—Å—Ç)."""
        text = (
            "# –ó–∞–≥–æ–ª–æ–≤–æ–∫\n\n"
            "–î–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç **—è–≤–ª—è–µ—Ç—Å—è** –ø—Ä–∏–º–µ—Ä–æ–º.\n\n"
            "- –ü–µ—Ä–≤—ã–π –ø—É–Ω–∫—Ç\n"
            "- –í—Ç–æ—Ä–æ–π –ø—É–Ω–∫—Ç\n\n"
            "–°—Å—ã–ª–∫–∞: [–ø—Ä–∏–º–µ—Ä](https://example.com)\n"
        )
        result = humanize(text, lang="ru", seed=42)
        assert isinstance(result.text, str)
        # Markdown-—Å—Å—ã–ª–∫–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å—Å—è
        assert "https://example.com" in result.text
