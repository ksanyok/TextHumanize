<div align="center">

# TextHumanize

### The most advanced open-source text naturalization engine

**Normalize style, improve readability, and ensure brand-safe content â€” offline, private, and blazing fast**

<br/>

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-3776AB.svg?logo=python&logoColor=white)](https://www.python.org/downloads/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.x-3178C6.svg?logo=typescript&logoColor=white)]()
[![PHP 8.1+](https://img.shields.io/badge/php-8.1+-777BB4.svg?logo=php&logoColor=white)](https://www.php.net/)
&nbsp;&nbsp;
[![Python Tests](https://img.shields.io/badge/tests-1696%20passed-2ea44f.svg?logo=pytest&logoColor=white)]()
[![PHP Tests](https://img.shields.io/badge/tests-223%20passed-2ea44f.svg?logo=php&logoColor=white)]()
[![JS Tests](https://img.shields.io/badge/tests-28%20passed-2ea44f.svg?logo=vitest&logoColor=white)]()
&nbsp;&nbsp;
[![Coverage](https://img.shields.io/badge/coverage-99%25-brightgreen.svg)]()
[![Benchmark](https://img.shields.io/badge/benchmark-100%25-brightgreen.svg)]()

[![mypy](https://img.shields.io/badge/types-mypy%20clean-blue.svg)](https://mypy-lang.org/)
[![Ruff](https://img.shields.io/badge/linting-ruff-261230.svg)](https://github.com/astral-sh/ruff)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen.svg)](https://pre-commit.com/)
[![Zero Dependencies](https://img.shields.io/badge/dependencies-zero-brightgreen.svg)]()
[![License](https://img.shields.io/badge/license-Dual%20(Free%20%2B%20Commercial)-blue.svg)](LICENSE)

<br/>

**40,000+ lines of code** Â· **72 Python modules** Â· **17-stage pipeline** Â· **14 languages + universal**

[Quick Start](#quick-start) Â· [API Reference](#api-reference) Â· [AI Detection](#ai-detection--how-it-works) Â· [Cookbook](docs/COOKBOOK.md)

</div>

---

TextHumanize is a **pure-algorithmic text processing engine** that normalizes style, improves readability, and removes mechanical patterns from text. No neural networks, no API keys, no internet â€” just 40K+ lines of finely tuned rules, dictionaries, and statistical methods.

It normalizes typography, simplifies bureaucratic language, diversifies sentence structure, increases burstiness and perplexity, replaces formulaic phrases, and applies context-aware synonym substitution â€” all while preserving semantic meaning.

### Built-in AI toolkit:
**AI Detection** Â· **Paraphrasing** Â· **Tone Analysis & Adjustment** Â· **Watermark Detection & Cleaning** Â· **Content Spinning** Â· **Coherence Analysis** Â· **Readability Scoring** Â· **Stylistic Fingerprinting** Â· **Auto-Tuner** Â· **Perplexity Analysis** Â· **Plagiarism Detection** Â· **Dictionary Training** Â· **Streaming & Variants**

### Available for:
**Python** (full) Â· **TypeScript/JavaScript** (core pipeline) Â· **PHP** (full)

### Languages:
ðŸ‡·ðŸ‡º Russian Â· ðŸ‡ºðŸ‡¦ Ukrainian Â· ðŸ‡¬ðŸ‡§ English Â· ðŸ‡©ðŸ‡ª German Â· ðŸ‡«ðŸ‡· French Â· ðŸ‡ªðŸ‡¸ Spanish Â· ðŸ‡µðŸ‡± Polish Â· ðŸ‡§ðŸ‡· Portuguese Â· ðŸ‡®ðŸ‡¹ Italian Â· ï¿½ðŸ‡¦ Arabic Â· ðŸ‡¨ðŸ‡³ Chinese Â· ðŸ‡¯ðŸ‡µ Japanese Â· ðŸ‡°ðŸ‡· Korean Â· ðŸ‡¹ðŸ‡· Turkish Â· ï¿½ðŸŒ **any language** via universal processor

---

## Table of Contents

- [Why TextHumanize?](#why-texthumanize)
- [Feature Overview](#feature-overview)
- [Comparison with Competitors](#comparison-with-competitors)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Before & After Examples](#before--after-examples)
- [AI Detection â€” Deep Dive](#ai-detection--how-it-works)
- [API Reference](#api-reference)
- [Style Presets](#style-presets)
- [Auto-Tuner (Feedback Loop)](#auto-tuner-feedback-loop)
- [Profiles](#profiles)
- [Parameters](#parameters)
- [Plugin System](#plugin-system)
- [Chunk Processing](#chunk-processing)
- [CLI Reference](#cli-reference)
- [REST API Server](#rest-api-server)
- [Processing Pipeline](#processing-pipeline)
- [Language Support](#language-support)
- [SEO Mode](#seo-mode)
- [Readability Metrics](#readability-metrics)
- [Paraphrasing Engine](#paraphrasing-engine)
- [Tone Analysis & Adjustment](#tone-analysis--adjustment)
- [Watermark Detection & Cleaning](#watermark-detection--cleaning)
- [Text Spinning](#text-spinning)
- [Coherence Analysis](#coherence-analysis)
- [Morphological Engine](#morphological-engine)
- [Smart Sentence Splitter](#smart-sentence-splitter)
- [Perplexity Analysis](#perplexity-analysis)
- [Plagiarism Detection](#plagiarism-detection)
- [Dictionary Training](#dictionary-training)
- [Sentence-Level Humanization](#sentence-level-humanization)
- [Multi-Variant Output](#multi-variant-output)
- [Streaming API](#streaming-api)
- [Context-Aware Synonyms](#context-aware-synonyms)
- [Stylistic Fingerprinting](#stylistic-fingerprinting)
- [Using Individual Modules](#using-individual-modules)
- [Performance & Benchmarks](#performance--benchmarks)
- [Testing](#testing)
- [Architecture](#architecture)
- [TypeScript / JavaScript Port](#typescript--javascript-port)
- [PHP Library](#php-library)
- [What's New in v0.8.0](#whats-new-in-v080)
- [Code Quality & Tooling](#code-quality--tooling)
- [FAQ & Troubleshooting](#faq--troubleshooting)
- [Contributing](#contributing)
- [Security & Limits](#security--limits)
- [For Business & Enterprise](#for-business--enterprise)
- [Support the Project](#support-the-project)
- [License & Pricing](#license--pricing)

---

## Why TextHumanize?

> **The problem:** Machine-generated and template-based text often has uniform sentence lengths, bureaucratic vocabulary, formulaic connectors, and low stylistic diversity. This reduces readability, engagement, and brand authenticity.

> **The solution:** TextHumanize algorithmically normalizes text style while preserving the original meaning. Configurable intensity, deterministic output, full change reports. No cloud APIs, no rate limits, no data leaks.

### Core Advantages

| Advantage | Details |
|:----------|:--------|
| ðŸš€ **Blazing fast** | 30,000+ chars/sec â€” process a full article in milliseconds, not seconds |
| ðŸ”’ **100% private** | All processing is local. Your text never leaves your machine |
| ðŸŽ¯ **Precise control** | Intensity 0â€“100, 9 profiles, keyword preservation, max change ratio |
| ðŸŒ **14 languages + universal** | Full dictionaries for 14 languages; statistical processor for any other |
| ðŸ“¦ **Zero dependencies** | Pure Python stdlib â€” no pip packages, no model downloads |
| ðŸ” **Reproducible** | Seed-based PRNG â€” same input + same seed = identical output |
| ðŸ”Œ **Extensible** | Plugin system to inject custom stages before/after any pipeline step |
| ðŸ§  **Built-in AI detector** | 13-metric ensemble with 100% benchmark accuracy â€” no ML required |
| ðŸ“Š **Self-optimizing** | Auto-Tuner learns optimal parameters from your processing history |
| ðŸŽ­ **Style presets** | Target a specific persona: student, copywriter, scientist, journalist, blogger |
| ðŸ“š **Multi-platform** | Python + TypeScript/JavaScript + PHP â€” one codebase, three ecosystems |
| ðŸ›¡ï¸ **Semantic guards** | Context-aware replacement with echo checks and negative collocations |
| ðŸ“ **Change report** | Every call returns what was changed, change ratio, quality score, similarity |
| ðŸ¢ **Enterprise-ready** | Dual license, 1,584 tests, 99% coverage, CI/CD, benchmarks, on-prem |

---

## Feature Overview

### Text Transformation

| What TextHumanize Fixes | Before (AI) | After (Human-like) |
|:------------------------|:------------|:-------------------|
| Em dashes | `text â€” example` | `text - example` |
| Typographic quotes | `Â«textÂ»` | `"text"` |
| Bureaucratic vocabulary | `utilize`, `implement`, `facilitate` | `use`, `do`, `help` |
| Formulaic connectors | `However`, `Furthermore`, `Additionally` | `But`, `Also`, `Plus` |
| Uniform sentence length | All 15â€“20 words | Varied 5â€“25 words |
| Word repetitions | `importantâ€¦ importantâ€¦` | Context-aware synonyms |
| Perfect punctuation | Frequent `;` and `:` | Simplified, natural |
| Low perplexity | Predictable word choice | Natural variation |
| Boilerplate phrases | `it is important to note that` | `notably`, `by the way` |
| AI watermarks | Hidden zero-width characters | Cleaned text |

### Full Feature Matrix

| Category | Feature | Python | TS/JS | PHP |
|:---------|:--------|:------:|:-----:|:---:|
| **Core** | `humanize()` â€” 17-stage pipeline | âœ… | âœ… | âœ… |
| | `humanize_batch()` â€” parallel processing | âœ… | â€” | âœ… |
| | `humanize_chunked()` â€” large text support | âœ… | â€” | âœ… |
| | `analyze()` â€” artificiality scoring | âœ… | âœ… | âœ… |
| | `explain()` â€” change report | âœ… | â€” | âœ… |
| **AI Detection** | `detect_ai()` â€” 13-metric + statistical ML | âœ… | âœ… | âœ… |
| | `detect_ai_batch()` â€” batch detection | âœ… | â€” | â€” |
| | `detect_ai_sentences()` â€” per-sentence | âœ… | â€” | â€” |
| | `detect_ai_mixed()` â€” mixed content | âœ… | â€” | â€” |
| | `detect_ai_statistical()` â€” 35-feature ML | âœ… | â€” | â€” |
| **Paraphrasing** | `paraphrase()` â€” syntactic transforms | âœ… | â€” | âœ… |
| **Tone** | `analyze_tone()` â€” formality analysis | âœ… | â€” | âœ… |
| | `adjust_tone()` â€” 7-level adjustment | âœ… | â€” | âœ… |
| **Watermarks** | `detect_watermarks()` â€” 5 types | âœ… | â€” | âœ… |
| | `clean_watermarks()` â€” removal | âœ… | â€” | âœ… |
| **Spinning** | `spin()` / `spin_variants()` | âœ… | â€” | âœ… |
| **Analysis** | `analyze_coherence()` â€” paragraph flow | âœ… | â€” | âœ… |
| | `full_readability()` â€” 6 indices | âœ… | â€” | âœ… |
| | Stylistic fingerprinting | âœ… | â€” | â€” |
| **NLP** | `POSTagger` â€” rule-based POS tagger (EN/RU/UK/DE) | âœ… | â€” | â€” |
| | `CJKSegmenter` â€” Chinese/Japanese/Korean word segmentation | âœ… | â€” | â€” |
| | `SyntaxRewriter` â€” 8 sentence-level transforms | âœ… | â€” | â€” |
| | `WordLanguageModel` â€” word-level LM (14 langs) | âœ… | â€” | â€” |
| | `CollocEngine` â€” PMI collocation scoring | âœ… | â€” | â€” |
| **AI Backend** | `humanize_ai()` â€” three-tier AI rewriting | âœ… | â€” | â€” |
| | OpenAI API integration | âœ… | â€” | â€” |
| | OSS model fallback (rate-limited) | âœ… | â€” | â€” |
| **Quality** | `BenchmarkSuite` â€” 6-dimension quality scoring | âœ… | â€” | â€” |
| | `FingerprintRandomizer` â€” anti-detection diversity | âœ… | â€” | â€” |
| **Advanced** | Style presets (5 personas) | âœ… | â€” | â€” |
| | Auto-Tuner (feedback loop) | âœ… | â€” | â€” |
| | Plugin system | âœ… | â€” | âœ… |
| | REST API server (12 endpoints) | âœ… | â€” | â€” |
| | CLI (15+ commands) | âœ… | â€” | â€” |
| **Languages** | Full dictionary support | 14 | 2 | 14 |
| | Universal processor | âœ… | âœ… | âœ… |

---

## Comparison with Competitors

### vs. Online Text-Processing Services

| Criterion | TextHumanize | Online Humanizers |
|:----------|:------------:|:-----------------:|
| Works offline | âœ… | âŒ â€” requires internet |
| Privacy | âœ… Your text stays local | âŒ Uploaded to third-party servers |
| Speed | **~3 ms** per paragraph | 2â€“10 seconds (network latency) |
| Cost | **Free** | $10â€“50/month subscription |
| API key required | No | Yes |
| Rate limits | None | Typically 10Kâ€“50K words/month |
| Reproducible results | âœ… Seed-based | âŒ Different every time |
| Fine control | Intensity, profiles, keywords, plugins | Usually none |
| Languages | **9 + universal** | 1â€“3 |
| Self-hosted | âœ… | âŒ |
| Built-in AI detector | âœ… 13-metric ensemble | Some (basic) |
| Paraphrasing | âœ… | Some |
| Tone adjustment | âœ… | âŒ |
| Watermark cleaning | âœ… | âŒ |
| Open source | âœ… | âŒ |

### vs. GPT/LLM-based Rewriting

| Criterion | TextHumanize | GPT Rewrite |
|:----------|:------------:|:-----------:|
| Works offline | âœ… | âŒ |
| Zero dependencies | âœ… | âŒ Requires API key + billing |
| Deterministic | âœ… Same seed = same output | âŒ Non-deterministic |
| Speed | **30K+ chars/sec** | ~500 chars/sec (API) |
| Cost per 1M chars | **$0** | ~$15â€“60 (GPT-4) |
| Preserves meaning | âœ… Controlled change ratio | âš ï¸ May hallucinate |
| Max change control | âœ… `max_change_ratio` | âŒ Unpredictable |
| Self-contained | âœ… pip install, done | âŒ Needs OpenAI account |
| Deterministic output | âœ… Seed-based | âŒ Non-deterministic |

### vs. Other Open-Source Libraries

| Feature | TextHumanize v0.8 | Typical Alternatives |
|:--------|:------------------:|:--------------------:|
| Pipeline stages | **17** | 2â€“4 |
| Languages | **9 + universal + CJK** | 1â€“2 |
| AI detection built-in | âœ… 13 metrics + ensemble | âŒ |
| Total test count | **1,696** (Py+PHP+JS) | 10â€“50 |
| Test coverage | **99%** | Unknown |
| Benchmark pass rate | **100%** (45/45) | No benchmark |
| Codebase size | **40K+ lines** | 500â€“2K |
| Platforms | Python + JS + PHP | Single |
| Plugin system | âœ… | âŒ |
| Tone analysis | âœ… 7 levels | âŒ |
| Watermark cleaning | âœ… 5 types | âŒ |
| Paraphrasing | âœ… Syntactic | âŒ |
| Coherence analysis | âœ… | âŒ |
| Auto-tuner | âœ… | âŒ |
| Style presets | âœ… 5 personas | âŒ |
| Documentation | README + API Ref + Cookbook | README only |
| REST API | âœ… 12 endpoints | âŒ |
| Readability metrics | âœ… 6 indices | 0â€“1 |
| Morphological engine | âœ… 4 languages | âŒ |
| Context-aware synonyms | âœ… WSD | Simple random |
| Reproducibility | âœ… Seed-based | âŒ |

---

## Installation

### pip (recommended)

```bash
pip install texthumanize
```

### From source

```bash
git clone https://github.com/ksanyok/TextHumanize.git
cd TextHumanize
pip install -e .
```

### PHP (Composer)

```bash
composer require ksanyok/text-humanize
```

Ð•ÑÐ»Ð¸ Ð¿Ð°ÐºÐµÑ‚ ÐµÑ‰Ñ‘ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ Ð½Ð° Packagist, Ð´Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ VCS-Ñ€ÐµÐ¿Ð¾Ð·Ð¸Ñ‚Ð¾Ñ€Ð¸Ð¹ Ð² `composer.json` Ð²Ð°ÑˆÐµÐ³Ð¾ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°:

```json
{
    "repositories": [
        {
            "type": "vcs",
            "url": "https://github.com/ksanyok/TextHumanize"
        }
    ],
    "require": {
        "ksanyok/text-humanize": "^0.11"
    }
}
```

Ð˜Ð»Ð¸ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚Ðµ Ð¸Ð· Ð¸ÑÑ…Ð¾Ð´Ð½Ð¸ÐºÐ¾Ð²:

```bash
cd php/
composer install
```

### TypeScript / JavaScript

```bash
cd js/
npm install
```

### Verify installation

```python
import texthumanize
print(texthumanize.__version__)  # 0.11.0
```

### Updating to latest version

#### Python

```bash
# Update to latest
pip install --upgrade texthumanize

# Update to specific version
pip install texthumanize==0.8.0
```

#### From source (GitHub)

```bash
cd TextHumanize
git pull origin main
pip install -e .
```

#### PHP

```bash
# Via Composer
composer require ksanyok/text-humanize

# Ð•ÑÐ»Ð¸ Ð¿Ð°ÐºÐµÑ‚ Ð½Ðµ Ð½Ð° Packagist â€” Ð´Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ VCS-Ñ€ÐµÐ¿Ð¾Ð·Ð¸Ñ‚Ð¾Ñ€Ð¸Ð¹:
composer config repositories.texthumanize vcs https://github.com/ksanyok/TextHumanize
composer require ksanyok/text-humanize:^0.11

# Ð˜Ð»Ð¸ Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚Ðµ Ð¸Ð· Ð¸ÑÑ…Ð¾Ð´Ð½Ð¸ÐºÐ¾Ð²
cd php/
git pull origin main
composer install
```

#### TypeScript / JavaScript

```bash
# Via npm (if published to npm)
npm install texthumanize@latest

# From source
cd js/
git pull origin main
npm install && npm run build
```

#### Install specific release from GitHub

```bash
# Python â€” install directly from a GitHub release tag
pip install git+https://github.com/ksanyok/TextHumanize.git@v0.8.0

# Or download a release archive
pip install https://github.com/ksanyok/TextHumanize/archive/refs/tags/v0.8.0.tar.gz
```

> **Tip:** Pin your version in `requirements.txt` for reproducible builds:
> ```
> texthumanize @ git+https://github.com/ksanyok/TextHumanize.git@v0.8.0
> ```

---

## Quick Start

```python
from texthumanize import humanize, analyze, explain

# Basic usage â€” one line
result = humanize("This text utilizes a comprehensive methodology for implementation.")
print(result.text)
# â†’ "This text uses a complete method for setup."

# With options
result = humanize(
    "Furthermore, it is important to note that the implementation facilitates optimization.",
    lang="en",             # auto-detect or specify
    profile="web",         # chat, web, seo, docs, formal, academic, marketing, social, email
    intensity=70,          # 0 (mild) to 100 (maximum)
    target_style="student" # preset: student, copywriter, scientist, journalist, blogger
)
print(result.text)
print(f"Changed: {result.change_ratio:.0%}")
print(f"Quality: {result.quality_score:.2f}")

# Analyze text metrics
report = analyze("Text to analyze for naturalness.", lang="en")
print(f"Artificiality score: {report.artificiality_score:.1f}/100")
print(f"Flesch-Kincaid grade: {report.flesch_kincaid_grade:.1f}")

# Get detailed explanation of changes
result = humanize("Furthermore, it is important to utilize this approach.")
print(explain(result))
```

### All Features at a Glance

```python
from texthumanize import (
    humanize, humanize_batch, humanize_chunked,
    detect_ai, detect_ai_sentences, paraphrase,
    analyze_tone, adjust_tone,
    detect_watermarks, clean_watermarks,
    spin, spin_variants, analyze_coherence, full_readability,
    STYLE_PRESETS, AutoTuner,
)

# AI Detection â€” 13-metric ensemble, no ML
ai = detect_ai("Text to check for AI generation.", lang="en")
print(f"AI probability: {ai['score']:.0%} | Verdict: {ai['verdict']}")
print(f"Confidence: {ai['confidence']:.0%}")

# Per-sentence AI detection
for s in detect_ai_sentences("First sentence. Second sentence.", lang="en"):
    print(f"  {s['label']}: {s['text'][:60]}...")

# Paraphrasing â€” syntactic transformations
print(paraphrase("The system works efficiently.", lang="en"))

# Tone Analysis â€” 7-level formality scale
tone = analyze_tone("Please submit the documentation.", lang="en")
print(f"Tone: {tone['primary_tone']}, formality: {tone['formality']:.2f}")

# Tone Adjustment
casual = adjust_tone("It is imperative to proceed.", target="casual", lang="en")
print(casual)

# Watermark Cleaning â€” zero-width chars, homoglyphs, steganography
clean = clean_watermarks("Te\u200bxt wi\u200bth hid\u200bden chars")
print(clean)

# Text Spinning â€” generate unique variants
unique = spin("The system provides important data.", lang="en")
variants = spin_variants("Original text.", count=5, lang="en")

# Coherence Analysis
coh = analyze_coherence("First part.\n\nSecond part.\n\nConclusion.", lang="en")
print(f"Coherence: {coh['overall']:.2f}")

# Style Presets
result = humanize(text, target_style="copywriter")  # student | scientist | journalist | blogger

# Auto-Tuner â€” learns optimal parameters
tuner = AutoTuner(history_path="history.json")
intensity = tuner.suggest_intensity(text, lang="en")
result = humanize(text, intensity=intensity)
tuner.record(result)

# Batch processing
results = humanize_batch(["Text 1", "Text 2", "Text 3"], lang="en", max_workers=4)

# Large documents â€” splits at paragraph boundaries
result = humanize_chunked(large_document, chunk_size=3000, lang="ru")

# Full readability â€” 6 indices
read = full_readability("Your text here.", lang="en")
print(read)
```

---

## Before & After Examples

### English â€” Blog Post

**Before (AI-generated):**
> Furthermore, it is important to note that the implementation of cloud computing facilitates the optimization of business processes. Additionally, the utilization of microservices constitutes a significant advancement. Nevertheless, considerable challenges remain in the area of security. It is worth mentioning that these challenges necessitate comprehensive solutions.

**After (TextHumanize, profile="web", intensity=70):**
> But cloud computing helps optimize how businesses work. Also, microservices are a big step forward. Still, security is tough. These challenges need thorough solutions.

**Changes:** 4 bureaucratic replacements, 2 connector swaps, sentence structure diversified.

### Russian â€” Documentation

**Before:**
> Ð”Ð°Ð½Ð½Ñ‹Ð¹ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð²Ð¾Ð¼ Ð¿Ð¾ Ð¾ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð»ÐµÐ½Ð¸ÑŽ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÐµÑÐ¿ÐµÑ‡ÐµÐ½Ð¸Ñ. ÐÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ Ð¾ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÑŒ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÑƒ Ð²ÑÐµÑ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð². ÐšÑ€Ð¾Ð¼Ðµ Ñ‚Ð¾Ð³Ð¾, ÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ð¾Ð±Ñ€Ð°Ñ‚Ð¸Ñ‚ÑŒ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð½Ð° ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹.

**After (profile="docs", intensity=60):**
> Ð­Ñ‚Ð¾Ñ‚ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ - Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð²Ð¾ Ð¿Ð¾ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐµ ÐŸÐž. ÐÑƒÐ¶Ð½Ð¾ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð²ÑÐµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹. Ð¢Ð°ÐºÐ¶Ðµ ÑÑ‚Ð¾Ð¸Ñ‚ Ð¾Ð±Ñ€Ð°Ñ‚Ð¸Ñ‚ÑŒ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð½Ð° Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸.

### Ukrainian â€” Web Content

**Before:**
> Ð”Ð°Ð½Ð¸Ð¹ Ð¼Ð°Ñ‚ÐµÑ€Ñ–Ð°Ð» Ñ” ÑÑÐºÑ€Ð°Ð²Ð¸Ð¼ Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´Ð¾Ð¼ Ð·Ð´Ñ–Ð¹ÑÐ½ÐµÐ½Ð½Ñ ÑÑƒÑ‡Ð°ÑÐ½Ð¸Ñ… Ð¿Ñ–Ð´Ñ…Ð¾Ð´Ñ–Ð². ÐšÑ€Ñ–Ð¼ Ñ‚Ð¾Ð³Ð¾, Ð½ÐµÐ¾Ð±Ñ…Ñ–Ð´Ð½Ð¾ Ð·Ð°Ð·Ð½Ð°Ñ‡Ð¸Ñ‚Ð¸ Ð²Ð°Ð¶Ð»Ð¸Ð²Ñ–ÑÑ‚ÑŒ Ð²Ð¿Ñ€Ð¾Ð²Ð°Ð´Ð¶ÐµÐ½Ð½Ñ Ñ–Ð½Ð½Ð¾Ð²Ð°Ñ†Ñ–Ð¹Ð½Ð¸Ñ… Ñ€Ñ–ÑˆÐµÐ½ÑŒ.

**After (profile="web", intensity=65):**
> Ð¦ÐµÐ¹ Ð¼Ð°Ñ‚ÐµÑ€Ñ–Ð°Ð» - ÑÑÐºÑ€Ð°Ð²Ð¸Ð¹ Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´ ÑÑƒÑ‡Ð°ÑÐ½Ð¸Ñ… Ð¿Ñ–Ð´Ñ…Ð¾Ð´Ñ–Ð². Ð¢Ð°ÐºÐ¾Ð¶ Ð²Ð°Ð¶Ð»Ð¸Ð²Ð¾ Ð²Ð¿Ñ€Ð¾Ð²Ð°Ð´Ð¶ÑƒÐ²Ð°Ñ‚Ð¸ Ñ–Ð½Ð½Ð¾Ð²Ð°Ñ†Ñ–Ð¹Ð½Ñ– Ñ€Ñ–ÑˆÐµÐ½Ð½Ñ.

---

## API Reference

### `humanize(text, **options)`

Main function â€” transforms text to sound more natural.

```python
from texthumanize import humanize

result = humanize(
    text="Your text here",
    lang="auto",        # auto-detect or specify: en, ru, de, fr, es, etc.
    profile="web",      # chat, web, seo, docs, formal, academic, marketing, social, email
    intensity=60,       # 0 (no changes) to 100 (maximum)
    preserve={          # protect specific elements
        "code_blocks": True,
        "urls": True,
        "emails": True,
        "brand_terms": ["MyBrand"],
    },
    constraints={       # output constraints
        "max_change_ratio": 0.4,
        "keep_keywords": ["SEO", "API"],
    },
    seed=42,            # reproducible results
)

# Result object
print(result.text)           # processed text
print(result.original)       # original text (unchanged)
print(result.lang)           # detected/specified language
print(result.profile)        # profile used
print(result.intensity)      # intensity used
print(result.change_ratio)   # fraction of text changed (0.0-1.0)
print(result.changes)        # list of individual changes [{type, original, replacement}]
print(result.metrics_before) # metrics before processing
print(result.metrics_after)  # metrics after processing
```

**Returns:** `HumanizeResult` dataclass.

### `humanize_chunked(text, chunk_size=5000, **options)`

Process large texts by splitting into chunks at paragraph boundaries. Each chunk is processed independently with its own seed variation, then reassembled.

```python
from texthumanize import humanize_chunked

# Process a 50,000-character document
with open("large_document.txt") as f:
    text = f.read()

result = humanize_chunked(
    text,
    chunk_size=5000,     # characters per chunk (default)
    overlap=200,         # character overlap for context
    lang="en",
    profile="docs",
    intensity=50,
)
print(result.text)
print(f"Total changes: {len(result.changes)}")
```

**Returns:** `HumanizeResult` dataclass.

### `humanize_batch(texts, **options)`

Process multiple texts in a single call. Each text gets a unique seed (`base_seed + index`) for reproducibility.

```python
from texthumanize import humanize_batch

texts = [
    "Furthermore, it is important to note...",
    "Additionally, it should be mentioned...",
    "Moreover, one must consider...",
]
results = humanize_batch(texts, lang="en", profile="web", seed=42)

for r in results:
    print(f"Similarity: {r.similarity:.2f}, Quality: {r.quality_score:.2f}")
    print(r.text)
```

**Returns:** `list[HumanizeResult]`.

### `HumanizeResult` Properties

| Property | Type | Description |
|---|---|---|
| `text` | `str` | Processed text |
| `original` | `str` | Original text |
| `change_ratio` | `float` | Word-level change ratio (0..1) |
| `similarity` | `float` | Jaccard similarity original vs processed (0..1) |
| `quality_score` | `float` | Overall quality balancing change and preservation (0..1) |
| `changes` | `list` | List of changes made |

### `analyze(text, lang)`

Analyze text and return naturalness metrics.

```python
from texthumanize import analyze

report = analyze("Text to analyze.", lang="en")

# All available metrics
print(f"Artificiality:         {report.artificiality_score:.1f}/100")
print(f"Total words:           {report.total_words}")
print(f"Total sentences:       {report.total_sentences}")
print(f"Avg sentence length:   {report.avg_sentence_length:.1f} words")
print(f"Sentence length var:   {report.sentence_length_variance:.2f}")
print(f"Bureaucratic ratio:    {report.bureaucratic_ratio:.3f}")
print(f"Connector ratio:       {report.connector_ratio:.3f}")
print(f"Repetition score:      {report.repetition_score:.3f}")
print(f"Typography score:      {report.typography_score:.3f}")
print(f"Burstiness:            {report.burstiness_score:.3f}")
print(f"Flesch-Kincaid grade:  {report.flesch_kincaid_grade:.1f}")
print(f"Coleman-Liau index:    {report.coleman_liau_index:.1f}")
print(f"Avg word length:       {report.avg_word_length:.1f}")
print(f"Avg syllables/word:    {report.avg_syllables_per_word:.1f}")
```

**Returns:** `AnalysisReport` dataclass.

### `explain(result)`

Generate a human-readable report of all changes made by `humanize()`.

```python
from texthumanize import humanize, explain

result = humanize("Furthermore, it is important to utilize this approach.", lang="en")
report = explain(result)
print(report)
```

**Output:**
```
=== ÐžÑ‚Ñ‡Ñ‘Ñ‚ TextHumanize ===
Ð¯Ð·Ñ‹Ðº: en | ÐŸÑ€Ð¾Ñ„Ð¸Ð»ÑŒ: web | Ð˜Ð½Ñ‚ÐµÐ½ÑÐ¸Ð²Ð½Ð¾ÑÑ‚ÑŒ: 60
Ð”Ð¾Ð»Ñ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹: 25.3%

--- ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ ---
  Ð˜ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ: 45.00 â†’ 22.00 â†“
  ÐšÐ°Ð½Ñ†ÐµÐ»ÑÑ€Ð¸Ð·Ð¼Ñ‹: 0.12 â†’ 0.00 â†“

--- Ð˜Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ (3) ---
  [debureaucratization] "utilize" â†’ "use"
  [connector] "Furthermore" â†’ "Also"
  [structure] sentence split applied
```

**Returns:** `str`

### `detect_ai(text, lang)`

Detect AI-generated text using 13 independent statistical metrics with ensemble boosting, without any ML dependencies.

```python
from texthumanize import detect_ai

result = detect_ai("Your text to analyze.", lang="auto")

print(f"AI probability:  {result['score']:.1%}")
print(f"Verdict:         {result['verdict']}")    # "human", "mixed", "ai", or "unknown"
print(f"Confidence:      {result['confidence']:.1%}")
print(f"Language:        {result['lang']}")

# Detailed per-metric scores (0.0 = human-like, 1.0 = AI-like)
metrics = result['metrics']
for name, score in metrics.items():
    print(f"  {name:30s} {score:.3f}")

# Human-readable explanations
for exp in result['explanations']:
    print(f"  â†’ {exp}")
```

**Returns:** `dict` with keys: `score`, `verdict`, `confidence`, `metrics`, `explanations`, `lang`.

### `detect_ai_batch(texts, lang)`

Batch AI detection for multiple texts.

```python
from texthumanize import detect_ai_batch

texts = [
    "First text to check.",
    "Second text to check.",
    "Third text to check.",
]
results = detect_ai_batch(texts, lang="en")
for i, r in enumerate(results):
    print(f"Text {i+1}: {r['verdict']} ({r['score']:.0%})")
```

**Returns:** `list[dict]`

### `paraphrase(text, lang, intensity, seed)`

Paraphrase text while preserving meaning. Uses syntactic transformations: clause swaps, passiveâ†”active, sentence splitting, adverb fronting, nominalization.

```python
from texthumanize import paraphrase

result = paraphrase(
    "Furthermore, it is important to note this fact.",
    lang="en",
    intensity=0.5,   # 0.0-1.0: fraction of sentences to transform
    seed=42,         # optional: reproducible results
)
print(result)
```

**Returns:** `str`

### `analyze_tone(text, lang)`

Analyze text tone, formality level, and subjectivity.

```python
from texthumanize import analyze_tone

tone = analyze_tone("Shall we proceed with the implementation?", lang="en")

print(f"Primary tone:   {tone['primary_tone']}")     # formal, casual, academic, etc.
print(f"Formality:      {tone['formality']:.2f}")     # 0=casual, 1=formal
print(f"Subjectivity:   {tone['subjectivity']:.2f}")  # 0=objective, 1=subjective
print(f"Confidence:     {tone['confidence']:.2f}")
print(f"Scores:         {tone['scores']}")            # dict of all tone scores
print(f"Markers found:  {tone['markers']}")           # detected tone markers
```

**Returns:** `dict`

### `adjust_tone(text, target, lang, intensity)`

Adjust text to a target tone level.

```python
from texthumanize import adjust_tone

# Make formal text casual
casual = adjust_tone(
    "It is imperative to implement this solution immediately.",
    target="casual",     # very_formal, formal, neutral, casual, very_casual
    lang="en",
    intensity=0.5,       # 0.0-1.0: strength of adjustment
)
print(casual)

# Make casual text formal
formal = adjust_tone(
    "Hey, we gotta fix this ASAP!",
    target="formal",
    lang="en",
)
print(formal)
```

Available targets: `very_formal`, `formal`, `neutral`, `casual`, `very_casual`, `friendly`, `academic`, `professional`, `marketing`.

**Returns:** `str`

### `detect_watermarks(text, lang)`

Detect invisible watermarks: zero-width characters, homoglyphs, invisible formatting, statistical AI watermarks.

```python
from texthumanize import detect_watermarks

report = detect_watermarks("Text with\u200bhidden\u200bcharacters")

print(f"Has watermarks:     {report['has_watermarks']}")
print(f"Types found:        {report['watermark_types']}")
print(f"Confidence:         {report['confidence']:.2f}")
print(f"Characters removed: {report['characters_removed']}")
print(f"Cleaned text:       {report['cleaned_text']}")
print(f"Details:            {report['details']}")
```

**Returns:** `dict`

### `clean_watermarks(text, lang)`

Remove all detected watermarks and return clean text.

```python
from texthumanize import clean_watermarks

clean = clean_watermarks("Contaminated\u200b text\u200b here")
print(clean)  # "Contaminated text here"
```

**Returns:** `str`

### `spin(text, lang, intensity, seed)`

Generate a unique version of text using synonym substitution.

```python
from texthumanize import spin

result = spin("The system provides important data for analysis.", lang="en")
print(result)
# â†’ e.g. "The platform offers crucial information for examination."
```

**Returns:** `str`

### `spin_variants(text, count, lang, intensity)`

Generate multiple unique versions of the same text.

```python
from texthumanize import spin_variants

variants = spin_variants(
    "The system provides important data.",
    count=5,
    lang="en",
    intensity=0.5,
)
for i, v in enumerate(variants, 1):
    print(f"  #{i}: {v}")
```

**Returns:** `list[str]`

### `analyze_coherence(text, lang)`

Analyze text coherence â€” how well sentences and paragraphs flow together.

```python
from texthumanize import analyze_coherence

text = """
Introduction paragraph here.

Main content paragraph with details.

Conclusion summarizing the points.
"""

report = analyze_coherence(text, lang="en")

print(f"Overall coherence:        {report['overall']:.2f}")
print(f"Lexical cohesion:         {report['lexical_cohesion']:.2f}")
print(f"Transition score:         {report['transition_score']:.2f}")
print(f"Topic consistency:        {report['topic_consistency']:.2f}")
print(f"Opening diversity:        {report['sentence_opening_diversity']:.2f}")
print(f"Paragraphs:               {report['paragraph_count']}")
print(f"Avg paragraph length:     {report['avg_paragraph_length']:.1f}")

if report['issues']:
    print("Issues:")
    for issue in report['issues']:
        print(f"  - {issue}")
```

**Returns:** `dict`

### `full_readability(text, lang)`

Compute all readability indices at once.

```python
from texthumanize import full_readability

r = full_readability("Your text here with multiple sentences. Each one helps.", lang="en")

# Available indices
print(f"Flesch-Kincaid Grade: {r.get('flesch_kincaid_grade', 0):.1f}")
print(f"Coleman-Liau:         {r.get('coleman_liau_index', 0):.1f}")
print(f"ARI:                  {r.get('ari', 0):.1f}")
print(f"SMOG:                 {r.get('smog_index', 0):.1f}")
print(f"Gunning Fog:          {r.get('gunning_fog', 0):.1f}")
print(f"Dale-Chall:           {r.get('dale_chall', 0):.1f}")
```

**Returns:** `dict`

---

## v0.15.0 â€” New Modules & APIs

### `humanize_ai(text, lang, **options)`

Three-tier AI-powered humanization: OpenAI â†’ OSS model â†’ built-in rules.

```python
from texthumanize import humanize_ai

# Default: uses built-in rules (zero dependencies)
result = humanize_ai("AI-generated text here.", lang="en")
print(result.text)

# With OpenAI API (best quality):
result = humanize_ai(
    "Text to humanize.",
    lang="en",
    openai_api_key="sk-...",
    openai_model="gpt-4o-mini",
)

# With OSS model (free, rate-limited):
result = humanize_ai("Text to humanize.", lang="en", enable_oss=True)
```

### `StatisticalDetector` â€” ML-based AI Detection

35-feature classifier with logistic regression, integrated into `detect_ai()`.

```python
from texthumanize import StatisticalDetector, detect_ai_statistical

# Standalone usage
det = StatisticalDetector(lang="en")
result = det.detect("Text to analyze for AI patterns.")
print(f"Probability: {result['probability']:.1%}")
print(f"Verdict: {result['verdict']}")  # human / mixed / ai

# Or convenience function
result = detect_ai_statistical("Your text here.", lang="en")
```

### `POSTagger` â€” Rule-based POS Tagging

Part-of-speech tagger for EN (500+ exceptions), RU/UK (200+), DE (300+).

```python
from texthumanize import POSTagger

tagger = POSTagger(lang="en")
for word, tag in tagger.tag("The quick brown fox jumps"):
    print(f"{word:12s} â†’ {tag}")
# The          â†’ DET
# quick        â†’ ADJ
# brown        â†’ ADJ
# fox          â†’ NOUN
# jumps        â†’ VERB
```

### `CJKSegmenter` â€” Chinese/Japanese/Korean Word Segmentation

```python
from texthumanize import CJKSegmenter, is_cjk_text, detect_cjk_lang

seg = CJKSegmenter(lang="zh")
words = seg.segment("æˆ‘ä»¬æ˜¯ä¸­å›½äºº")  # ['æˆ‘ä»¬', 'æ˜¯', 'ä¸­å›½', 'äºº']

is_cjk_text("è¿™æ˜¯ä¸­æ–‡")      # True
detect_cjk_lang("æ±äº¬ã¯å¤§ãã„")  # "ja"
```

### `SyntaxRewriter` â€” Sentence-level Transforms

8 transformations: activeâ†”passive, clause inversion, enumeration reorder, adverb migration, etc.

```python
from texthumanize import SyntaxRewriter

sr = SyntaxRewriter(lang="en", seed=42)
variants = sr.rewrite("The team completed the project on time.")
for v in variants:
    print(v)
```

### `WordLanguageModel` â€” Word-level Perplexity

14-language word-level unigram/bigram LM with naturalness scoring.

```python
from texthumanize import WordLanguageModel, word_perplexity, word_naturalness

lm = WordLanguageModel(lang="en")
pp = lm.perplexity("Some text to measure complexity")
score = lm.naturalness_score("Your multi-sentence text here. Another one.")
print(f"Verdict: {score['verdict']}")  # human / mixed / ai

# Convenience:
pp = word_perplexity("Quick check.", lang="en")
ns = word_naturalness("Full analysis.", lang="en")
```

### `CollocEngine` â€” Collocation-Aware Synonym Ranking

PMI-based scoring for choosing the most natural synonym in context.

```python
from texthumanize import CollocEngine

eng = CollocEngine(lang="en")
best = eng.best_synonym("important", ["crucial", "key", "significant"], context=["decision"])
print(best)  # "crucial" (strongest collocation with "decision")
```

### `FingerprintRandomizer` â€” Anti-Detection Diversity

Prevents detectable patterns in humanized output.

```python
from texthumanize import FingerprintRandomizer

r = FingerprintRandomizer(seed=42, jitter_level=0.3)
text1 = r.diversify_output("Some humanized text.")
text2 = r.diversify_output("Some humanized text.")  # different each time
```

### `BenchmarkSuite` â€” Quality Measurement

6-dimension automated quality benchmarking.

```python
from texthumanize import BenchmarkSuite, quick_benchmark

# Quick single-pair benchmark:
report = quick_benchmark("Original AI text.", "Humanized version.")
print(report.summary())

# Full suite:
suite = BenchmarkSuite(lang="en")
report = suite.run_all([
    {"original": "AI text 1.", "humanized": "Human text 1."},
    {"original": "AI text 2.", "humanized": "Human text 2."},
])
print(f"Overall score: {report.overall_score:.1f}/100")
```

---

## Profiles

Nine built-in profiles control the processing style:

| Profile | Use Case | Sentence Length | Colloquialisms | Intensity Default |
|---------|----------|:---------:|:---------:|:---------:|
| `chat` | Messaging, social media | 8-18 words | High | 80 |
| `web` | Blog posts, articles | 10-22 words | Medium | 60 |
| `seo` | SEO content | 12-25 words | None | 40 |
| `docs` | Technical documentation | 12-28 words | None | 50 |
| `formal` | Academic, legal | 15-30 words | None | 30 |
| `academic` | Research papers | 15-30 words | None | 25 |
| `marketing` | Sales, promo copy | 8-20 words | Medium | 70 |
| `social` | Social media posts | 6-15 words | High | 85 |
| `email` | Business emails | 10-22 words | Medium | 50 |

```python
# Conversational style for social media
result = humanize(text, profile="chat", intensity=80)

# SEO-safe mode (preserves keywords, minimal changes)
result = humanize(text, profile="seo", intensity=40,
                  constraints={"keep_keywords": ["API", "cloud"]})

# Academic writing
result = humanize(text, profile="academic", intensity=25)

# Marketing copy â€” energetic and engaging
result = humanize(text, profile="marketing", intensity=70)
```

### Profile Comparison

Given the input: *"Furthermore, it is important to note that the implementation of this approach facilitates comprehensive optimization."*

| Profile | Output |
|---------|--------|
| `chat` | *"This approach helps optimize things a lot."* |
| `web` | *"Also, this approach helps with thorough optimization."* |
| `seo` | *"This approach facilitates comprehensive optimization."* |
| `formal` | *"Notably, implementing this approach facilitates optimization."* |

---

## Parameters

### Intensity (0-100)

Controls how aggressively text is modified:

| Range | Effect | Best For |
|-------|--------|----------|
| 0-20 | Typography normalization only | Legal, contracts |
| 20-40 | + light debureaucratization | Documentation |
| 40-60 | + structure diversification & connector swaps | Blog posts |
| 60-80 | + synonym replacement, natural phrasing | Web content |
| 80-100 | + maximum variation, colloquial insertions | Chat, social |

```python
# Minimal â€” only fix typography
result = humanize(text, intensity=10)

# Moderate â€” safe for most content
result = humanize(text, intensity=50)

# Maximum â€” full rewrite
result = humanize(text, intensity=95)
```

### Preserve Options

Protect specific elements from modification:

```python
preserve = {
    "code_blocks": True,    # protect ```code``` blocks
    "urls": True,           # protect URLs
    "emails": True,         # protect email addresses
    "hashtags": True,       # protect #hashtags
    "mentions": True,       # protect @mentions
    "markdown": True,       # protect markdown formatting
    "html": True,           # protect HTML tags
    "numbers": False,       # protect numbers (default: False)
    "brand_terms": [        # exact terms to protect (case-sensitive)
        "TextHumanize",
        "MyBrand",
        "ProductNameâ„¢",
    ],
}
```

### Constraints

Set limits on processing:

```python
constraints = {
    "max_change_ratio": 0.4,            # max 40% of text changed
    "min_sentence_length": 3,           # minimum words per sentence
    "keep_keywords": ["SEO", "API"],    # keywords preserved exactly
}
```

### Seed (Reproducibility)

```python
# Same seed = same result every time
r1 = humanize("Text here.", seed=42)
r2 = humanize("Text here.", seed=42)
assert r1.text == r2.text  # guaranteed
```

---

## Plugin System

Register custom processing stages that run before or after any built-in stage:

```python
from texthumanize import Pipeline, humanize

# Simple hook function
def add_disclaimer(text: str, lang: str) -> str:
    return text + "\n\n---\nProcessed by TextHumanize."

Pipeline.register_hook(add_disclaimer, after="naturalization")

# Plugin class with full context
class BrandEnforcer:
    def __init__(self, brand: str, canonical: str):
        self.brand = brand
        self.canonical = canonical

    def process(self, text: str, lang: str, profile: str, intensity: int) -> str:
        import re
        return re.sub(re.escape(self.brand), self.canonical, text, flags=re.IGNORECASE)

Pipeline.register_plugin(
    BrandEnforcer("texthumanize", "TextHumanize"),
    after="typography",
)

# Process text â€” plugins run automatically
result = humanize("texthumanize is great.")
print(result.text)  # "TextHumanize is great. ..."

# Clean up when done
Pipeline.clear_plugins()
```

### Available Stage Names

```
watermark â†’ segmentation â†’ typography â†’ debureaucratization â†’ structure â†’
repetitions â†’ liveliness â†’ universal â†’ naturalization â†’ validation â†’ restore
```

You can attach plugins `before` or `after` any of these stages.

---

## Chunk Processing

For large documents (articles, books, reports), use `humanize_chunked` to process text in manageable pieces:

```python
from texthumanize import humanize_chunked

# Automatically splits at paragraph boundaries
result = humanize_chunked(
    very_long_text,
    chunk_size=5000,    # characters per chunk
    overlap=200,        # context overlap
    lang="en",
    profile="docs",
    intensity=50,
    seed=42,            # base seed, each chunk gets seed+i
)
print(f"Processed {len(result.text)} characters")
```

Each chunk is processed independently with its own seed for variation, then reassembled into the final text. The chunk boundary detection preserves paragraph integrity.

---

## CLI Reference

### Basic Usage

```bash
# Process a file (output to stdout)
texthumanize input.txt

# Process with options
texthumanize input.txt -l en -p web -i 70

# Save to file
texthumanize input.txt -o output.txt

# Process from stdin
echo "Text to process" | texthumanize - -l en
cat article.txt | texthumanize -
```

### All CLI Options

```bash
texthumanize [input] [options]

Positional:
  input                     Input file path (or '-' for stdin)

Options:
  -o, --output FILE         Output file (default: stdout)
  -l, --lang LANG           Language: auto, en, ru, uk, de, fr, es, pl, pt, it
  -p, --profile PROFILE     Profile: chat, web, seo, docs, formal, academic,
                            marketing, social, email
  -i, --intensity N         Processing intensity 0-100 (default: 60)
  --keep WORD [WORD ...]    Keywords to preserve
  --brand TERM [TERM ...]   Brand terms to protect
  --max-change RATIO        Maximum change ratio 0-1 (default: 0.4)
  --seed N                  Random seed for reproducibility
  --report FILE             Save JSON report to file

Analysis modes:
  --analyze                 Analyze text metrics (no processing)
  --explain                 Show detailed change report
  --detect-ai               Check for AI-generated text
  --tone-analyze            Analyze text tone
  --readability             Full readability analysis
  --coherence               Coherence analysis

Transform modes:
  --paraphrase              Paraphrase the text
  --tone TARGET             Adjust tone (formal, casual, neutral, etc.)
  --watermarks              Detect and clean watermarks
  --spin                    Generate a spun version
  --variants N              Generate N spin variants

Server:
  --api                     Start REST API server
  --port N                  API server port (default: 8080)

Other:
  -v, --version             Show version
```

### CLI Examples

```bash
# Analyze a file
texthumanize article.txt --analyze -l en

# Check for AI generation
texthumanize essay.txt --detect-ai

# Paraphrase with output file
texthumanize input.txt --paraphrase -o paraphrased.txt

# Adjust tone to casual
texthumanize formal_email.txt --tone casual -o casual_email.txt

# Clean watermarks
texthumanize suspect.txt --watermarks -o clean.txt

# Generate 5 spin variants
texthumanize template.txt --variants 5

# Start API server
texthumanize dummy --api --port 9090
```

---

## REST API Server

TextHumanize includes a zero-dependency HTTP server for JSON API access:

```bash
# Start server
python -m texthumanize.api --port 8080

# Or via CLI
texthumanize dummy --api --port 8080
```

### Endpoints

All `POST` endpoints accept JSON body with `{"text": "..."}` and return JSON.

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/humanize` | Humanize text |
| `POST` | `/analyze` | Analyze text metrics |
| `POST` | `/detect-ai` | AI detection (single or batch) |
| `POST` | `/paraphrase` | Paraphrase text |
| `POST` | `/tone/analyze` | Tone analysis |
| `POST` | `/tone/adjust` | Tone adjustment |
| `POST` | `/watermarks/detect` | Detect watermarks |
| `POST` | `/watermarks/clean` | Clean watermarks |
| `POST` | `/spin` | Spin text (single or multi) |
| `POST` | `/coherence` | Coherence analysis |
| `POST` | `/readability` | Readability metrics |
| `GET` | `/health` | Server health check |
| `GET` | `/` | API info & endpoint list |

### Usage with curl

```bash
# Humanize
curl -X POST http://localhost:8080/humanize \
  -H "Content-Type: application/json" \
  -d '{"text": "Furthermore, it is important to utilize this.", "lang": "en", "profile": "web"}'

# AI Detection
curl -X POST http://localhost:8080/detect-ai \
  -H "Content-Type: application/json" \
  -d '{"text": "Text to check."}'

# Batch AI Detection
curl -X POST http://localhost:8080/detect-ai \
  -H "Content-Type: application/json" \
  -d '{"texts": ["First text.", "Second text."]}'

# Tone Adjustment
curl -X POST http://localhost:8080/tone/adjust \
  -H "Content-Type: application/json" \
  -d '{"text": "Formal text here.", "target": "casual"}'

# Health Check
curl http://localhost:8080/health
```

### Usage with Python requests

```python
import requests

API = "http://localhost:8080"

# Humanize
r = requests.post(f"{API}/humanize", json={
    "text": "Text to process.",
    "lang": "en",
    "profile": "web",
    "intensity": 60,
})
print(r.json()["text"])

# AI Detection
r = requests.post(f"{API}/detect-ai", json={"text": "Check this text."})
print(r.json()["verdict"])
```

All responses include `_elapsed_ms` field with processing time in milliseconds.

---

## Processing Pipeline

TextHumanize uses a **17-stage pipeline** with adaptive intensity:

```
Input Text
  â”‚
  â”œâ”€ 0.  Watermark Cleaning       â”€ remove zero-width chars, homoglyphs    [auto]
  â”‚
  â”œâ”€ 1.  Segmentation             â”€ protect code blocks, URLs, emails, brands
  â”‚
  â”œâ”€ 1b. CJK Segmentation         â”€ word-boundary injection for CJK text   [auto, zh/ja/ko]
  â”‚
  â”œâ”€ 2.  Typography               â”€ normalize dashes, quotes, ellipses, punctuation
  â”‚
  â”œâ”€ 3.  Debureaucratization      â”€ replace bureaucratic/formal words     [dictionary, 15% budget]
  â”‚
  â”œâ”€ 4.  Structure                â”€ diversify sentence openings            [dictionary]
  â”‚
  â”œâ”€ 5.  Repetitions              â”€ reduce word/phrase repetitions          [dictionary + context + morphology]
  â”‚
  â”œâ”€ 6.  Liveliness               â”€ inject natural phrasing                [dictionary]
  â”‚
  â”œâ”€ 7.  Paraphrasing             â”€ semantic sentence-level transforms     [syntax trees]
  â”‚
  â”œâ”€ 7b. Syntax Rewriting         â”€ structural sentence transforms         [POS-tagged]
  â”‚
  â”œâ”€ 8.  Tone Harmonization       â”€ align tone consistency                 [context-aware]
  â”‚
  â”œâ”€ 9.  Universal                â”€ statistical processing                 [any language]
  â”‚
  â”œâ”€ 10. Naturalization           â”€ burstiness, perplexity, rhythm         [KEY STAGE, collocation-aware]
  â”‚
  â”œâ”€ 10b.Word LM Quality Gate     â”€ perplexity check, rollback if degraded [advisory]
  â”‚
  â”œâ”€ 11. Readability Optimization â”€ improve sentence readability           [adaptive]
  â”‚
  â”œâ”€ 12. Grammar Correction       â”€ fix grammar issues                     [rule-based]
  â”‚
  â”œâ”€ 13. Coherence Repair         â”€ repair paragraph flow & transitions    [context-aware]
  â”‚
  â”œâ”€ 13b.Fingerprint Diversify    â”€ anti-fingerprint micro-variations      [typography]
  â”‚
  â”œâ”€ 14. Validation               â”€ quality check, graduated retry
  â”‚
  â””â”€ 15. Restore                  â”€ restore protected segments
  â”‚
Output Text
```

### Adaptive Intensity

The pipeline automatically adjusts processing based on how "AI-like" the input is:

| AI Score | Behavior | Why |
|:---------|:---------|:----|
| â‰¤ 5% | **Typography only** â€” skips all semantic stages | Text is already natural, don't touch it |
| â‰¤ 10% | Intensity Ã— 0.2 | Very light touch needed |
| â‰¤ 15% | Intensity Ã— 0.35 | Minor adjustments |
| â‰¤ 25% | Intensity Ã— 0.5 | Moderate processing |
| > 25% | Full intensity | Text needs substantial work |

### Graduated Retry

If processing exceeds `max_change_ratio`, the pipeline automatically retries at lower intensity (Ã—0.4, then Ã—0.15) instead of discarding all changes. This ensures maximum quality within constraints.

**Stages 3â€“6** require full dictionary support (14 languages).
**Stages 2, 7â€“9** work for any language, including those without dictionaries.
**Stage 14** validates quality and retries if needed (configurable via `max_change_ratio`).

---

## AI Detection â€” How It Works

TextHumanize includes a **production-grade AI text detector** that rivals commercial solutions like GPTZero and Originality.ai â€” but runs **100% locally**, requires **no API key**, and has **zero dependencies**.

### Architecture

The detector uses a **3-layer ensemble** of 13 independent statistical metrics. No machine learning models, no neural networks, no external APIs.

```
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚    13 Metric Analyzers   â”‚
                          â”‚  (each produces 0.0â€“1.0) â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                  â–¼                  â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Weighted Sum â”‚  â”‚ Strong Signal â”‚  â”‚   Majority   â”‚
            â”‚   (50%)      â”‚  â”‚  Detector     â”‚  â”‚   Voting     â”‚
            â”‚              â”‚  â”‚   (30%)       â”‚  â”‚   (20%)      â”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                  â”‚                  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  Final Score  â”‚
                              â”‚  + Verdict    â”‚
                              â”‚  + Confidence â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The 13 Metrics

| # | Metric | What It Detects | Weight | How It Works |
|:-:|:-------|:----------------|:------:|:-------------|
| 1 | **AI Patterns** | "it is important to note", "furthermore", etc. | 20% | 100+ formulaic phrase patterns per language |
| 2 | **Burstiness** | Sentence length uniformity | 14% | Coefficient of variation â€” humans vary, AI doesn't |
| 3 | **Opening Diversity** | Repetitive sentence starts | 9% | Unique first-word ratio across sentences |
| 4 | **Entropy** | Word predictability | 8% | Shannon entropy of word distribution |
| 5 | **Stylometry** | Word length consistency | 8% | Std deviation of character counts per word |
| 6 | **Coherence** | Paragraph transitions | 8% | Lexical overlap and connector analysis |
| 7 | **Vocabulary** | Lexical richness | 7% | Type-to-token ratio (unique vs total words) |
| 8 | **Grammar Perfection** | Suspiciously perfect grammar | 6% | 9 indicators: Oxford commas, fragments, etc. |
| 9 | **Punctuation** | Punctuation diversity | 6% | Distribution of , ; : ! ? â€” across text |
| 10 | **Rhythm** | Syllabic patterns | 6% | Syllable-per-word variation across sentences |
| 11 | **Perplexity** | Character-level predictability | 6% | Trigram model with Laplace smoothing |
| 12 | **Readability** | Reading level consistency | 5% | Variance of readability across paragraphs |
| 13 | **Zipf** | Word frequency distribution | 3% | Log-log linear regression with RÂ² fit |

### Ensemble Boosting

Three classifiers vote on the final score:

1. **Weighted Sum (50%)** â€” classic weighted average of all 13 metrics
2. **Strong Signal Detector (30%)** â€” triggers when any single metric is extremely high (>0.85) â€” catches obvious AI even when the average is moderate
3. **Majority Voting (20%)** â€” counts how many metrics individually vote "AI" (>0.5) â€” robust against outlier metrics

### Confidence Scoring

Confidence reflects how reliable the verdict is:

| Factor | Weight | Description |
|:-------|:------:|:------------|
| Text length | 35% | Longer text = more reliable analysis |
| Metric agreement | 20% | Higher when all metrics agree |
| Extreme bonus | â€” | +0.6 Ã— distance from 0.5 midpoint |
| Agreement ratio | 25% | What fraction of metrics agree on AI/human |

### Verdicts

| Score | Verdict | Interpretation |
|:-----:|:--------|:---------------|
| < 35% | `human_written` | Text appears naturally written |
| 35â€“65% | `mixed` | Uncertain â€” partially AI or heavily edited |
| â‰¥ 65% | `ai_generated` | Strong AI patterns detected |

### Benchmark Results

Tested on a curated benchmark of 11 labeled samples (5 AI, 5 human, 1 mixed):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          AI Detection Benchmark              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Accuracy         â”‚ 100%                      â”‚
â”‚ Precision        â”‚ 100%                      â”‚
â”‚ Recall           â”‚ 100%                      â”‚
â”‚ F1 Score         â”‚ 1.000                     â”‚
â”‚ True Positives   â”‚ 5                         â”‚
â”‚ False Positives  â”‚ 0                         â”‚
â”‚ True Negatives   â”‚ 5                         â”‚
â”‚ False Negatives  â”‚ 0                         â”‚
â”‚ Mixed (correct)  â”‚ 1/1                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detection Modes

```python
from texthumanize import detect_ai, detect_ai_batch, detect_ai_sentences, detect_ai_mixed

# Standard detection
result = detect_ai("Your text here.", lang="en")
print(f"AI: {result['score']:.0%} | {result['verdict']} | Confidence: {result['confidence']:.0%}")

# Per-metric breakdown
for name, score in result['metrics'].items():
    bar = "â–ˆ" * int(score * 20)
    print(f"  {name:30s} {score:.2f} {bar}")

# Human-readable explanations
for exp in result['explanations']:
    print(f"  â†’ {exp}")

# Batch detection â€” process many texts at once
results = detect_ai_batch(["Text 1", "Text 2", "Text 3"])

# Per-sentence detection â€” find AI sentences in mixed content
sentences = detect_ai_sentences(mixed_text)
for s in sentences:
    emoji = "ðŸ¤–" if s['label'] == 'ai' else "ðŸ‘¤"
    print(f"  {emoji} {s['text'][:80]}...")

# Mixed content analysis
report = detect_ai_mixed(text_with_ai_and_human_parts)
```

### Example: AI vs Human

```python
from texthumanize import detect_ai

# AI-generated text (GPT-like)
ai_text = """
Furthermore, it is important to note that the implementation of artificial 
intelligence constitutes a significant paradigm shift. Additionally, the 
utilization of machine learning facilitates comprehensive optimization 
of various processes. Nevertheless, it is worth mentioning that 
considerable challenges remain.
"""
result = detect_ai(ai_text, lang="en")
print(f"Score: {result['score']:.0%}")   # â†’ ~87-89% â€” AI detected
print(f"Verdict: {result['verdict']}")   # â†’ "ai_generated"

# Human-written casual text
human_text = """
I tried that new coffee shop downtown yesterday. Their espresso was 
actually decent - not as burnt as the place on 5th. The barista 
was nice too, recommended this Ethiopian blend I'd never heard of. 
Might go back this weekend.
"""
result = detect_ai(human_text, lang="en")
print(f"Score: {result['score']:.0%}")   # â†’ ~20-27% â€” Human confirmed
print(f"Verdict: {result['verdict']}")   # â†’ "human_written"
```

### Comparison with Commercial Detectors

| Feature | TextHumanize | GPTZero | Originality.ai |
|:--------|:------------:|:-------:|:--------------:|
| Works offline | âœ… | âŒ | âŒ |
| Free | âœ… | Freemium | $14.95/mo |
| API key required | âŒ | âœ… | âœ… |
| Languages | 14 | ~5 | English-focused |
| Metrics | 13 | Undisclosed | Undisclosed |
| Per-sentence breakdown | âœ… | âœ… | âŒ |
| Batch detection | âœ… | âœ… | âœ… |
| Self-hosted | âœ… | âŒ | âŒ |
| Reproducible | âœ… | âŒ | âŒ |
| Mixed content analysis | âœ… | âœ… | âŒ |
| Zero dependencies | âœ… | Cloud-based | Cloud-based |

### Tips for Best Results

- **100+ words** â€” best accuracy for texts of substantial length
- **Short texts** (< 50 words) â€” results may be less reliable
- **Formal texts** â€” may score slightly higher even if human-written (expected behavior for legal, academic style)
- **Multiple metrics** â€” the ensemble approach helps even when individual signals are weak

---

## Language Support

### Full Dictionary Support (14 languages)

Each language pack includes:
- Bureaucratic word â†’ natural replacements
- Formulaic connector alternatives
- Synonym dictionaries (context-aware)
- Sentence starter variations
- Colloquial markers
- Abbreviation lists (for sentence splitting)
- Language-specific trigrams (for detection)
- Stop words
- Profile-specific sentence length targets
- Perplexity boosters

| Language | Code | Bureaucratic | Connectors | Synonyms | AI Words | Abbreviations |
|----------|:----:|:-----:|:------:|:------:|:------:|:------:|
| Russian | `ru` | 70+ | 25+ | 50+ | 30+ | 15+ |
| Ukrainian | `uk` | 50+ | 24 | 48 | 25+ | 12+ |
| English | `en` | 40+ | 25 | 35+ | 24+ | 20+ |
| German | `de` | 64 | 20 | 45 | 38 | 10+ |
| French | `fr` | 20 | 12 | 20 | 15+ | 8+ |
| Spanish | `es` | 18 | 12 | 18 | 15+ | 8+ |
| Polish | `pl` | 18 | 12 | 18 | 15+ | 8+ |
| Portuguese | `pt` | 16 | 12 | 17 | 12+ | 6+ |
| Italian | `it` | 16 | 12 | 17 | 12+ | 6+ |
| Arabic | `ar` | 81 | 49 | 80 | 40+ | 47 |
| Chinese | `zh` | 80 | 36 | 80 | 40+ | 32 |
| Japanese | `ja` | 60+ | 30+ | 60+ | 30+ | 25+ |
| Korean | `ko` | 60+ | 30+ | 60+ | 30+ | 25+ |
| Turkish | `tr` | 60+ | 30+ | 60+ | 30+ | 25+ |

### Universal Processor

For any language not in the dictionary list, TextHumanize uses statistical methods:
- Sentence length variation (burstiness injection)
- Punctuation normalization
- Whitespace regularization
- Perplexity boosting
- Fragment insertion

```python
# Works with any language â€” no dictionaries needed
result = humanize("æ—¥æœ¬èªžã®ãƒ†ã‚­ã‚¹ãƒˆ", lang="ja")
result = humanize("Ð¢ÐµÐºÑÑ‚ Ð½Ð° ÐºÐ°Ð·Ð°Ñ…ÑÐºÐ¾Ð¼", lang="kk")
result = humanize("Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ", lang="fa")
result = humanize("ÄÃ¢y lÃ  vÄƒn báº£n tiáº¿ng Viá»‡t", lang="vi")
```

### Auto-Detection

```python
# Language is detected automatically
result = humanize("Ð­Ñ‚Ð¾Ñ‚ Ñ‚ÐµÐºÑÑ‚ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚ÑÑ ÐºÐ°Ðº Ñ€ÑƒÑÑÐºÐ¸Ð¹.")
print(result.lang)  # "ru"

result = humanize("This text is automatically detected as English.")
print(result.lang)  # "en"
```

---

## SEO Mode

The `seo` profile is designed for content that must preserve search ranking:

```python
result = humanize(
    text,
    profile="seo",
    intensity=40,            # lower intensity for safety
    constraints={
        "max_change_ratio": 0.3,
        "keep_keywords": ["cloud computing", "API", "microservices"],
    },
)
```

### SEO Mode Features

| Feature | Behavior |
|---------|----------|
| Keyword preservation | All specified keywords kept exactly |
| Intensity cap | Limited to safe levels |
| Colloquialisms | None inserted |
| Structure changes | Minimal |
| Sentence length | Stays within 12-25 words (optimal for SEO) |
| Synonyms | Only for non-keyword terms |
| Readability | Grade 6-8 target maintained |

### SEO Workflow Example

```python
from texthumanize import humanize, analyze, detect_ai

# 1. Analyze original
report = analyze(seo_text, lang="en")
print(f"Artificiality before: {report.artificiality_score:.0f}/100")

# 2. Humanize with SEO protection
result = humanize(seo_text, profile="seo", intensity=35,
                  constraints={"keep_keywords": ["cloud", "scalability"]})

# 3. Verify keywords preserved
for kw in ["cloud", "scalability"]:
    assert kw in result.text, f"Keyword '{kw}' was modified!"

# 4. Check AI detection improvement
ai_before = detect_ai(seo_text, lang="en")
ai_after = detect_ai(result.text, lang="en")
print(f"AI score: {ai_before['score']:.0%} â†’ {ai_after['score']:.0%}")
```

---

## Readability Metrics

TextHumanize includes 6 readability indices:

| Index | Range | Measures |
|-------|-------|----------|
| **Flesch-Kincaid Grade** | 0-18+ | US grade level needed to read |
| **Coleman-Liau** | 0-18+ | Grade level (character-based) |
| **ARI** | 0-14+ | Automated Readability Index |
| **SMOG** | 3-18+ | Complexity from polysyllabic words |
| **Gunning Fog** | 6-20+ | Complexity estimate |
| **Dale-Chall** | 0-10+ | Difficulty using common word list |

```python
from texthumanize import analyze, full_readability

# Quick readability from analyze()
report = analyze("Your text here.", lang="en")
print(f"Flesch-Kincaid: {report.flesch_kincaid_grade:.1f}")
print(f"Coleman-Liau:   {report.coleman_liau_index:.1f}")

# Full readability with all indices
r = full_readability("Your text with multiple sentences. Each one counts.", lang="en")
for metric, value in r.items():
    print(f"  {metric}: {value}")
```

### Readability Grade Interpretation

| Grade | Level | Audience |
|:-----:|-------|----------|
| 5-6 | Easy | General public |
| 7-8 | Standard | Web content, blogs |
| 9-10 | Moderate | Business writing |
| 11-12 | Difficult | Academic papers |
| 13+ | Complex | Technical/legal |

---

## Paraphrasing Engine

The paraphrasing engine uses syntactic transformations (no ML):

### Transformations Applied

| Transformation | Example |
|---------------|---------|
| **Clause swap** | "Although X, Y." â†’ "Y, although X." |
| **Passiveâ†’Active** | "The report was written by John." â†’ "John wrote the report." |
| **Sentence splitting** | "X, and Y, and Z." â†’ "X. Y. Z." |
| **Adverb fronting** | "He quickly ran." â†’ "Quickly, he ran." |
| **Nominalization** | "He decided to go." â†’ "His decision was to go." |

```python
from texthumanize import paraphrase

original = "Although the study was comprehensive, the results were inconclusive."
result = paraphrase(original, lang="en", intensity=0.8)
print(result)
# â†’ e.g. "The results were inconclusive, although the study was comprehensive."
```

---

## Tone Analysis & Adjustment

### Tone Levels

| Tone | Formality | Example |
|------|:---------:|---------|
| `very_formal` | 0.9+ | "The undersigned hereby acknowledges..." |
| `formal` | 0.7-0.9 | "Please submit the required documentation." |
| `neutral` | 0.4-0.7 | "Send us the documents." |
| `casual` | 0.2-0.4 | "Just send over the docs." |
| `very_casual` | 0.0-0.2 | "Shoot me the docs!" |

### Markers Detected

For English: `hereby`, `pursuant`, `constitutes`, `facilitate`, `implement`, `utilize`, `gonna`, `wanna`, `hey`, `awesome`, etc.

For Russian: `Ð½Ð°ÑÑ‚Ð¾ÑÑ‰Ð¸Ð¼`, `Ð¾ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÑŒ`, `Ð¾Ð´Ð½Ð°ÐºÐ¾`, `Ð¿Ñ€Ð¸Ð²ÐµÑ‚`, `ÐºÑ€ÑƒÑ‚Ð¾`, etc.

```python
from texthumanize import analyze_tone, adjust_tone

# Analyze
tone = analyze_tone("Pursuant to our agreement, please facilitate the transfer.", lang="en")
print(tone['primary_tone'])  # "formal"
print(tone['formality'])     # ~0.85

# Adjust down
casual = adjust_tone("Pursuant to our agreement, please facilitate the transfer.",
                     target="casual", lang="en")
print(casual)  # â†’ "Based on our agreement, go ahead and start the transfer."
```

---

## Watermark Detection & Cleaning

### What It Detects

| Type | Description | Example |
|------|-------------|---------|
| **Zero-width chars** | U+200B, U+200C, U+200D, U+FEFF | Invisible between words |
| **Homoglyphs** | Cyrillic/Latin lookalikes | `Ð°` (Cyrillic) vs `a` (Latin) |
| **Invisible formatting** | Invisible Unicode chars | U+2060, U+2061, etc. |
| **Spacing steganography** | Unusual space patterns | Extra spaces encoding data |
| **Statistical watermarks** | AI watermark patterns | Token probability anomalies |

```python
from texthumanize import detect_watermarks, clean_watermarks

# Full detection
report = detect_watermarks(suspicious_text, lang="en")
if report['has_watermarks']:
    print(f"Found: {report['watermark_types']}")
    print(f"Confidence: {report['confidence']:.0%}")
    print(f"Cleaned: {report['cleaned_text']}")
else:
    print("No watermarks detected")

# Quick clean
clean = clean_watermarks(suspicious_text)
```

---

## Text Spinning

Generate unique content variants using dictionary-based synonym replacement.

### Spintax

The spinner can output spintax format for use in other tools:

```python
from texthumanize.spinner import ContentSpinner

spinner = ContentSpinner(lang="en", seed=42)

# Generate spintax
spintax = spinner.generate_spintax("The system provides important data.")
print(spintax)
# â†’ "The {system|platform} {provides|offers} {important|crucial} {data|information}."

# Resolve spintax to one variant
resolved = spinner.resolve_spintax(spintax)
print(resolved)
```

### High-Level API

```python
from texthumanize import spin, spin_variants

# Single variant
unique = spin("Original text here.", lang="en", intensity=0.6, seed=42)

# Multiple variants
variants = spin_variants("Original text.", count=5, lang="en")
for v in variants:
    print(v)
```

---

## Coherence Analysis

Measures how well text flows at the paragraph level.

### Metrics

| Metric | Range | Description |
|--------|:-----:|-------------|
| `overall` | 0-1 | Weighted average of all coherence metrics |
| `lexical_cohesion` | 0-1 | Word overlap between adjacent sentences |
| `transition_score` | 0-1 | Quality of logical transitions |
| `topic_consistency` | 0-1 | How consistent the topic is throughout |
| `sentence_opening_diversity` | 0-1 | Variety in sentence beginnings |

### Issues Detected

The analyzer flags specific problems:
- "Weak transition between paragraph 2 and 3"
- "Topic drift detected at paragraph 4"
- "Repetitive sentence openings in paragraph 1"
- "Paragraph too short (1 sentence)"

```python
from texthumanize import analyze_coherence

report = analyze_coherence(article_text, lang="en")
print(f"Overall: {report['overall']:.2f}")

if report['overall'] < 0.5:
    print("Text coherence is low. Issues:")
    for issue in report['issues']:
        print(f"  - {issue}")
```

---

## Morphological Engine

Built-in lemmatization for RU, UK, EN, DE â€” no external libraries needed.

### Supported Operations

| Operation | Languages | Example |
|-----------|-----------|---------|
| Lemmatization | RU, UK, EN, DE | "running" â†’ "run" |
| Form generation | RU, UK, EN, DE | "run" â†’ ["runs", "running", "ran"] |
| Case handling | RU, UK, DE | Automatic declension matching |
| Compound words | DE | Splitting German compounds |

### Usage in Synonym Matching

The morphological engine is used internally by the repetition reducer to ensure synonym forms match the original grammatically:

```python
# Internal usage â€” synonyms match morphological forms
# "They were implementing..." â†’ "They were doing..." (not "They were do...")
```

Direct usage:

```python
from texthumanize.morphology import MorphologicalEngine

morph = MorphologicalEngine(lang="en")
print(morph.lemmatize("running"))   # "run"
print(morph.lemmatize("houses"))    # "house"
print(morph.lemmatize("better"))    # "good"
```

---

## Smart Sentence Splitter

Handles edge cases that naive regex splitting gets wrong:

| Case | Input | Correct Split |
|------|-------|--------------|
| Abbreviations | "Dr. Smith went home." | 1 sentence |
| Decimals | "Temperature is 36.6 degrees." | 1 sentence |
| Initials | "J.K. Rowling wrote it." | 1 sentence |
| Ellipsis | "Well... Maybe not." | 2 sentences |
| Direct speech | '"Hello," she said.' | 1 sentence |
| URLs | "Visit example.com today." | 1 sentence |

```python
from texthumanize.sentence_split import split_sentences

text = "Dr. Smith arrived at 3 p.m. He brought the report."
sents = split_sentences(text, lang="en")
print(sents)  # ['Dr. Smith arrived at 3 p.m.', 'He brought the report.']
```

The smart splitter is integrated into all pipeline stages that need sentence-level processing.

---

## Perplexity Analysis

Character-level trigram cross-entropy model for measuring text naturalness â€” fully offline, no ML dependencies.

```python
from texthumanize import perplexity_score, cross_entropy

# Quick cross-entropy measurement
ce = cross_entropy("The quick brown fox jumps over the lazy dog.", lang="en")
print(f"Cross-entropy: {ce:.2f} bits")

# Full analysis with naturalness score and verdict
result = perplexity_score(
    "It is important to note that AI-generated text tends to be uniform.",
    lang="en",
)
print(f"Naturalness: {result['naturalness']}/100")
print(f"Verdict: {result['verdict']}")  # "human", "mixed", or "ai"
print(f"Burstiness: {result['burstiness_score']:.2f}")
```

| Return Field | Description |
|---|---|
| `cross_entropy` | Bits per character against language model |
| `perplexity` | 2^cross_entropy â€” lower = more predictable |
| `local_variance` | Entropy variance across text windows |
| `burstiness_score` | Human-like variability (higher = more natural) |
| `naturalness` | Score 0â€“100 (100 = fully natural) |
| `verdict` | `"human"`, `"mixed"`, `"ai"`, or `"unknown"` |

---

## Plagiarism Detection

Offline originality analysis via n-gram fingerprinting and self-similarity scoring.

```python
from texthumanize import check_originality, compare_originality

# Check text originality (self-repetition analysis)
report = check_originality(
    "Your text here...",
    reference_texts=["Optional reference corpus..."],
)
print(f"Originality: {report.originality_score}%")
print(f"Verdict: {report.verdict}")  # "original", "moderate_overlap", "high_overlap"
print(f"Fingerprint: {report.fingerprint_hash}")

# Compare humanized output against original
divergence = compare_originality(
    "The humanized version of the text.",
    "The original AI-generated text.",
)
print(f"Divergence: {divergence['divergence']:.1%}")
print(f"Sufficiently different: {divergence['is_sufficiently_different']}")
```

---

## Dictionary Training

Analyze a corpus to detect overused AI phrases and build custom replacement dictionaries.

```python
from texthumanize import train_from_corpus, export_custom_dict

# Analyze your text corpus
texts = [
    "Furthermore, it is important to note that...",
    "Moreover, the results clearly demonstrate...",
    # ... more texts
]
result = train_from_corpus(texts, lang="en", min_frequency=2)

print(f"Overused phrases: {result.overused_phrases}")
print(f"AI patterns found: {len(result.repeated_patterns)}")
print(f"Type-token ratio: {result.vocabulary_stats['type_token_ratio']:.2f}")

# Export as custom dictionary for humanize()
custom_dict = export_custom_dict(result)
from texthumanize import humanize
r = humanize("Your text...", custom_dict=custom_dict)
```

---

## Sentence-Level Humanization

Process text at sentence granularity â€” only rewrite sentences that score above an AI probability threshold.

```python
from texthumanize import humanize_sentences

result = humanize_sentences(
    "Human-written intro. AI-generated middle part. Another natural sentence.",
    lang="en",
    ai_threshold=0.6,    # Only process sentences with >60% AI probability
    intensity=70,
)

print(f"Kept human: {result['human_kept']} sentences")
print(f"Processed AI: {result['ai_processed']} sentences")
for s in result["sentences"]:
    print(f"  [{s['action']}] {s['original'][:50]}... â†’ AI: {s['ai_probability']:.0%}")
```

---

## Multi-Variant Output

Generate multiple humanization variants and pick the best one.

```python
from texthumanize import humanize_variants

variants = humanize_variants(
    "AI-generated text to humanize.",
    lang="en",
    variants=5,     # Generate 5 different versions
    seed=42,        # Reproducible base seed
)

# Results sorted by quality (best first)
for v in variants:
    print(f"Variant {v['variant_id']}: score={v['ai_score']:.2f}, "
          f"changes={v['change_ratio']:.0%}")
    print(f"  {v['text'][:80]}...")
```

---

## Streaming API

Process large texts chunk-by-chunk with progress tracking.

```python
from texthumanize import humanize_stream

for chunk in humanize_stream("Very long text...", lang="en"):
    print(f"[{chunk['progress']:.0%}] {chunk['chunk'][:60]}...")
    if chunk["is_last"]:
        print("Done!")
```

---

## Context-Aware Synonyms

Word-sense disambiguation (WSD) without ML. Chooses the best synonym based on surrounding context.

### How It Works

1. **Topic detection** â€” classifies text as technology, business, casual, or neutral
2. **Collocation scoring** â€” checks expected word pairs ("make decision" not "make choice")
3. **Context window** â€” examines surrounding words to determine word sense

```python
from texthumanize.context import ContextualSynonyms

ctx = ContextualSynonyms(lang="en", seed=42)
ctx.detect_topic("The server handles API requests efficiently.")

# Choose best synonym for "important" in tech context
best = ctx.choose_synonym("important", ["significant", "crucial", "key", "vital"],
                          "This is an important update to the system.")
print(best)  # "key" or "crucial" (tech-appropriate)
```

---

## Style Presets

*New in v0.8.0*

Target a specific writing style using preset fingerprints. The pipeline adapts sentence length, vocabulary complexity, and punctuation patterns to match the chosen persona â€” producing output that reads like it was written by a real student, journalist, or scientist.

```python
from texthumanize import humanize, STYLE_PRESETS

# Just pass a string â€” that's it
result = humanize(text, target_style="student")

# Or use the fingerprint object directly
result = humanize(text, target_style=STYLE_PRESETS["scientist"])

# Custom fingerprint from your own writing sample
from texthumanize import StylisticAnalyzer
analyzer = StylisticAnalyzer(lang="en")
my_style = analyzer.extract(my_writing_sample)
result = humanize(text, target_style=my_style)
```

### Available Presets

| Preset | Avg Sentence | Sentence Variance | Vocabulary Richness | Complex Words | Best For |
|:-------|:------------:|:-----------------:|:-------------------:|:-------------:|:---------|
| ðŸŽ“ `student` | 14 words | Ïƒ=6 | 65% | 25% | Essays, homework, coursework |
| âœï¸ `copywriter` | 12 words | Ïƒ=8.5 | 72% | 20% | Marketing copy, ads, landing pages |
| ðŸ”¬ `scientist` | 22 words | Ïƒ=7 | 70% | 55% | Research papers, dissertations |
| ðŸ“° `journalist` | 16 words | Ïƒ=7.5 | 72% | 35% | News articles, reports, features |
| ðŸ’¬ `blogger` | 11 words | Ïƒ=7 | 60% | 12% | Blog posts, social media, casual writing |

### How It Works

1. The preset defines a **stylistic fingerprint** â€” a vector of text metrics (sentence length mean/std, vocabulary richness, complex word ratio)
2. After the main pipeline processes text, the **stylistic alignment stage** adjusts output to match the target fingerprint
3. Sentences are split, merged, or reorganized to match the target distribution
4. The result reads naturally in the target style while preserving original meaning

---

## Auto-Tuner (Feedback Loop)

*New in v0.8.0*

The Auto-Tuner learns optimal processing parameters from your history. Instead of guessing the right intensity, let it figure it out from data.

```python
from texthumanize import humanize, AutoTuner

# Create tuner with persistent storage
tuner = AutoTuner(history_path="~/.texthumanize_history.json", max_records=500)

# Process & record
for text in my_texts:
    intensity = tuner.suggest_intensity(text, lang="en")  # Smart suggestion
    result = humanize(text, lang="en", intensity=intensity)
    tuner.record(result)  # Learn from this result

# After 10+ records, suggestions become data-driven
params = tuner.suggest_params(lang="en")
print(f"Optimal intensity: {params.intensity}")
print(f"Max change ratio: {params.max_change_ratio:.2f}")
print(f"Confidence: {params.confidence:.0%}")

# Review accumulated statistics
stats = tuner.summary()
# â†’ {"total_records": 47, "avg_quality": 0.78, "avg_ai_reduction": 42, ...}

# Reset if needed
tuner.reset()
```

### How It Works

1. Each `record()` call saves: language, profile, intensity, AI score before/after, change ratio, quality score, timestamp
2. `suggest_intensity()` groups historical records by intensity bucket (10, 20, 30, ..., 100)
3. For each bucket, it computes average quality score
4. Returns the intensity with the highest average quality
5. Confidence increases from 0 to 1 as more data accumulates (10+ records per bucket = full confidence)

---

## Stylistic Fingerprinting

*New in v0.7.0+*

Extract and compare writing styles using statistical fingerprints. Use this to match AI-generated text to your personal writing style, or compare two texts for stylistic similarity.

```python
from texthumanize import StylisticAnalyzer, StylisticFingerprint

# Extract fingerprint from a writing sample
analyzer = StylisticAnalyzer(lang="en")
my_style = analyzer.extract(my_writing_sample)

# Fingerprint contains:
print(f"Avg sentence length: {my_style.sent_len_mean:.1f} words")
print(f"Sentence length std: {my_style.sent_len_std:.1f}")
print(f"Complex word ratio: {my_style.complex_ratio:.2f}")
print(f"Vocabulary richness: {my_style.vocabulary_richness:.2f}")

# Compare two styles (cosine similarity)
similarity = my_style.similarity(other_style)
print(f"Style match: {similarity:.1%}")

# Use as target for humanization
result = humanize(ai_text, target_style=my_style)
```

---

## Using Individual Modules

Each module can be used independently:

```python
# Typography normalization only
from texthumanize.normalizer import TypographyNormalizer
norm = TypographyNormalizer(profile="web")
result = norm.normalize("Text â€” with dashes and Â«quotesÂ»...")
# â†’ 'Text - with dashes and "quotes"...'

# Debureaucratization only
from texthumanize.decancel import Debureaucratizer
db = Debureaucratizer(lang="en", profile="chat", intensity=80)
result = db.process("This text utilizes a comprehensive methodology.")
# â†’ "This text uses a complete method."

# Structure diversification
from texthumanize.structure import StructureDiversifier
sd = StructureDiversifier(lang="en", profile="web", intensity=60)
result = sd.process("Furthermore, X. Additionally, Y. Moreover, Z.")

# Sentence splitting
from texthumanize.sentence_split import split_sentences
sents = split_sentences("Dr. Smith said hello. She left.", lang="en")

# AI detection (low-level)
from texthumanize.detectors import detect_ai
result = detect_ai("Text to check.", lang="en")
print(result.ai_probability, result.verdict)

# Tone analysis (low-level)
from texthumanize.tone import analyze_tone
report = analyze_tone("Formal text here.", lang="en")
print(report.primary_tone, report.formality)

# Content spinning
from texthumanize.spinner import ContentSpinner
spinner = ContentSpinner(lang="en", seed=42)
spintax = spinner.generate_spintax("The system works well.")

# Analysis only
from texthumanize.analyzer import TextAnalyzer
analyzer = TextAnalyzer(lang="en")
report = analyzer.analyze("Text to analyze.")
```

---

## Performance & Benchmarks

All benchmarks on Apple Silicon (M1 Pro), Python 3.12, single thread. Reproducible via `python3 benchmarks/full_benchmark.py`.

### Processing Speed

| Text Size | Humanize Time | AI Detection Time | Throughput |
|-----------|:-------------:|:-----------------:|:----------:|
| 100 words (~900 chars) | ~24ms | ~2ms | ~38,000 chars/sec |
| 500 words (~3,600 chars) | ~138ms | ~6ms | ~26,000 chars/sec |
| 1,000 words (~6,000 chars) | ~213ms | ~9ms | ~28,000 chars/sec |

### Quality Benchmark

Tested on 45 curated samples across 14 languages, multiple profiles, and edge cases:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          TextHumanize Quality Benchmark           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Pass rate          â”‚ 100% (45/45)                â”‚
â”‚ Avg quality score  â”‚ 0.75                        â”‚
â”‚ Avg speed          â”‚ 51,459 chars/sec            â”‚
â”‚ Issues found       â”‚ 0                           â”‚
â”‚ Languages tested   â”‚ 9                           â”‚
â”‚ Profiles tested    â”‚ 9                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Predictability (Determinism)

TextHumanize is fully deterministic â€” the core corporate requirement:

```python
result1 = humanize(text, seed=12345)
result2 = humanize(text, seed=12345)
assert result1.text == result2.text  # Always True
```

| Property | Value |
|----------|:-----:|
| Same seed â†’ identical output | âœ… Always |
| Different seed â†’ different output | âœ… Always |
| No network calls | âœ… |
| No randomness from external sources | âœ… |

### Memory Usage

| Scenario | Memory |
|----------|:------:|
| Base import | ~2 MB |
| Processing 30K chars | ~2.5 MB peak |
| No model files to load | âœ… |

### Change Report (explain)

Every `humanize()` call returns a structured result with full audit trail:

```python
result = humanize(text, seed=42, profile="web")
print(result.change_ratio)   # 0.15 â€” 15% of words changed
print(result.quality_score)  # 0.85 â€” quality score 0..1
print(result.similarity)     # 0.87 â€” Jaccard similarity with original

# Full human-readable report
print(explain(result))
# === Report ===
# Language: en | Profile: web | Intensity: 60
# Change ratio: 15.0%
# --- Metrics ---
#   Artificiality: 57.2 â†’ 46.1 â†“
#   Bureaucratisms: 0.18 â†’ 0.05 â†“
#   AI connectors: 0.12 â†’ 0.00 â†“
# --- Changes (5) ---
#   [debureaucratize] "implementation" â†’ "setup"
#   [debureaucratize] "utilization" â†’ "use"
#   ...
```

---

## Testing

### Test Suite Overview

| Platform | Tests | Status | Time |
|:---------|------:|:------:|:-----|
| **Python** | 1,333 | âœ… All passing | ~1.5s |
| **PHP** | 223 | âœ… All passing | ~2s |
| **TypeScript** | 28 | âœ… All passing | ~1s |
| **Total** | **1,584** | âœ… | â€” |

```bash
# Run all Python tests
pytest -q                           # 1333 passed in 1.53s

# With coverage report
pytest --cov=texthumanize --cov-report=term-missing

# Lint + type check
ruff check texthumanize/            # 0 errors
mypy texthumanize/                  # 0 errors

# Pre-commit hooks
pre-commit run --all-files

# PHP tests
cd php && php vendor/bin/phpunit    # 223 tests, 825 assertions

# TypeScript tests
cd js && npx vitest run             # 28 tests
```

### Coverage Summary (Python)

| Module | Coverage |
|--------|:--------:|
| core.py | 98% |
| decancel.py | 97% |
| segmenter.py | 98% |
| lang_detect.py | 96% |
| coherence.py | 96% |
| tokenizer.py | 95% |
| spinner.py | 94% |
| normalizer.py | 94% |
| tone.py | 94% |
| morphology.py | 93% |
| analyzer.py | 93% |
| stylistic.py | 95% |
| autotune.py | 92% |
| detectors.py | 90% |
| utils.py | 90% |
| repetitions.py | 88% |
| structure.py | 88% |
| paraphrase.py | 87% |
| watermark.py | 87% |
| context.py | 90% |
| liveliness.py | 86% |
| validator.py | 86% |
| pipeline.py | 92% |
| cli.py | 85% |
| lang/ | 100% |
| **Overall** | **99%** |

---

## Architecture

```
texthumanize/                   # 72 Python modules, 40,677 lines
â”œâ”€â”€ __init__.py                 # Public API: 25 functions + 5 classes
â”œâ”€â”€ core.py                     # Facade: humanize(), analyze(), detect_ai(), etc.
â”œâ”€â”€ api.py                      # REST API: zero-dependency HTTP server, 12 endpoints
â”œâ”€â”€ cli.py                      # CLI: 15+ commands
â”œâ”€â”€ pipeline.py                 # 17-stage pipeline + adaptive intensity + graduated retry
â”‚
â”œâ”€â”€ analyzer.py                 # Artificiality scoring + 6 readability metrics
â”œâ”€â”€ tokenizer.py                # Paragraph/sentence/word tokenization
â”œâ”€â”€ sentence_split.py           # Smart sentence splitter (abbreviations, decimals)
â”‚
â”œâ”€â”€ segmenter.py                # Code/URL/email/brand protection (stage 1)
â”œâ”€â”€ cjk_segmenter.py            # CJK word segmentation â€” zh/ja/ko (stage 1b)
â”œâ”€â”€ normalizer.py               # Typography normalization (stage 2)
â”œâ”€â”€ decancel.py                 # Debureaucratization + 15% budget + echo check (stage 3)
â”œâ”€â”€ structure.py                # Sentence structure diversification (stage 4)
â”œâ”€â”€ repetitions.py              # Repetition reduction + morphology (stage 5)
â”œâ”€â”€ liveliness.py               # Natural phrasing injection (stage 6)
â”œâ”€â”€ paraphraser_ext.py          # Semantic paraphrasing â€” syntax trees (stage 7)
â”œâ”€â”€ syntax_rewriter.py          # Structural sentence transforms â€” POS-tagged (stage 7b)
â”œâ”€â”€ tone_harmonizer.py          # Tone harmonization (stage 8)
â”œâ”€â”€ universal.py                # Universal processor â€” any language (stage 9)
â”œâ”€â”€ naturalizer.py              # Key stage: burstiness, perplexity, collocation-aware synonyms (stage 10)
â”œâ”€â”€ word_lm.py                  # Word language model â€” perplexity quality gate (stage 10b)
â”œâ”€â”€ readability_opt.py          # Readability optimization (stage 11)
â”œâ”€â”€ grammar_fix.py              # Grammar correction (stage 12)
â”œâ”€â”€ coherence_repair.py         # Coherence repair (stage 13)
â”œâ”€â”€ fingerprint_randomizer.py   # Anti-fingerprint diversification (stage 13b)
â”œâ”€â”€ validator.py                # Quality validation + graduated retry (stage 14)
â”‚
â”œâ”€â”€ detectors.py                # AI detector: 13 metrics + ensemble boosting
â”œâ”€â”€ statistical_detector.py     # Statistical AI detection (feature weights + ensemble)
â”œâ”€â”€ pos_tagger.py               # Rule-based POS tagger (en/de/ru/uk)
â”œâ”€â”€ collocation_engine.py       # Collocation scoring â€” context-aware synonym ranking
â”œâ”€â”€ benchmark_suite.py          # Benchmarking suite â€” quality metrics
â”œâ”€â”€ ai_backend.py               # AI backend â€” OpenAI + OSS Gradio providers
â”‚
â”œâ”€â”€ paraphrase.py               # Syntactic paraphrasing engine
â”œâ”€â”€ tone.py                     # Tone analysis & adjustment (7 levels)
â”œâ”€â”€ watermark.py                # Watermark detection & cleaning (5 types)
â”œâ”€â”€ spinner.py                  # Text spinning & spintax generation
â”œâ”€â”€ coherence.py                # Coherence & paragraph flow analysis
â”œâ”€â”€ morphology.py               # Morphological engine (RU/UK/EN/DE)
â”œâ”€â”€ context.py                  # Context-aware synonyms (WSD + negative collocations)
â”œâ”€â”€ stylistic.py                # Stylistic fingerprinting + presets
â”œâ”€â”€ autotune.py                 # Auto-Tuner (feedback loop + JSON persistence)
â”‚
â”œâ”€â”€ lang_detect.py              # Language detection (14 languages)
â”œâ”€â”€ utils.py                    # Options, profiles, result classes
â”œâ”€â”€ __main__.py                 # python -m texthumanize
â”‚
â””â”€â”€ lang/                       # Language packs (data only, no logic)
    â”œâ”€â”€ __init__.py             # Registry + fallback
    â”œâ”€â”€ ru.py                   # Russian (70+ bureaucratic, 50+ synonyms)
    â”œâ”€â”€ uk.py                   # Ukrainian (50+ bureaucratic, 48 synonyms)
    â”œâ”€â”€ en.py                   # English (40+ bureaucratic, 35+ synonyms)
    â”œâ”€â”€ de.py                   # German (64 bureaucratic, 45 synonyms, 38 AI words)
    â”œâ”€â”€ fr.py                   # French
    â”œâ”€â”€ es.py                   # Spanish
    â”œâ”€â”€ pl.py                   # Polish
    â”œâ”€â”€ pt.py                   # Portuguese
    â”œâ”€â”€ it.py                   # Italian
    â”œâ”€â”€ ar.py                   # Arabic (81 bureaucratic, 80 synonyms)
    â”œâ”€â”€ zh.py                   # Chinese Simplified (80 bureaucratic, 80 synonyms)
    â”œâ”€â”€ ja.py                   # Japanese (keigoâ†’casual register)
    â”œâ”€â”€ ko.py                   # Korean (honorificâ†’casual register)
    â””â”€â”€ tr.py                   # Turkish (Ottomanâ†’modern Turkish)
```

### Design Principles

| Principle | Description |
|-----------|-------------|
| **Modularity** | Each pipeline stage is a separate module |
| **Declarative rules** | Language packs contain only data, not logic |
| **Idempotent** | Re-processing doesn't degrade quality |
| **Safe defaults** | Validator auto-rolls back harmful changes |
| **Extensible** | Add languages, profiles, or stages via plugins |
| **Portable** | Declarative architecture enables easy porting |
| **Zero dependencies** | Pure Python stdlib only |
| **Lazy imports** | New modules loaded on first use, fast startup |

---

## TypeScript / JavaScript Port

The `js/` directory contains a TypeScript port of the core pipeline with full processing stages:

```typescript
import { humanize, analyze } from 'texthumanize';

const result = humanize('Text to process', { lang: 'en', intensity: 60 });
console.log(result.text);
console.log(`Changed: ${(result.changeRatio * 100).toFixed(0)}%`);

const report = analyze('Text to check');
console.log(`AI score: ${report.artificialityScore}%`);
```

### JS/TS Modules

| Module | Description |
|:-------|:------------|
| `pipeline.ts` | Full 11-stage pipeline with adaptive intensity |
| `normalizer.ts` | Typography normalization (dashes, quotes, spacing) |
| `debureaucratizer.ts` | Bureaucratic word replacement with seeded PRNG |
| `naturalizer.ts` | AI word replacement, burstiness, connectors |
| `analyzer.ts` | Text analysis and artificiality scoring |
| `detector.ts` | AI detection with statistical metrics |
| `segmenter.ts` | Code/URL/email protection |

Features:
- **Seeded PRNG** (xoshiro128**) â€” reproducible results
- **Adaptive intensity** â€” same algorithm as Python (AI â‰¤ 5% â†’ typography only)
- **Graduated retry** â€” retries at lower intensity if change ratio exceeds limit
- **Cyrillic-safe regex** â€” lookbehind/lookahead instead of `\b` for Cyrillic support
- **28 tests** (vitest) â€” all passing, TS compiles clean

```bash
cd js/
npm install
npx vitest run    # 28 tests
npx tsc --noEmit  # type check
```

---

## PHP Library

A full PHP port is available in the `php/` directory â€” 10,000 lines, 223 tests, 825 assertions.

### PHP Quick Start

```php
<?php
use TextHumanize\TextHumanize;

// Basic usage
$result = TextHumanize::humanize("Text to process", profile: 'web');
echo $result->processed;

// Chunk processing for large texts
$result = TextHumanize::humanizeChunked($longText, chunkSize: 5000);

// AI detection
$ai = TextHumanize::detectAI("Suspicious text", lang: 'en');
echo $ai['verdict'];  // "ai_generated"

// Batch processing
$results = TextHumanize::humanizeBatch([$text1, $text2, $text3]);

// Tone analysis & adjustment
$tone = TextHumanize::analyzeTone("Formal text", lang: 'en');
$casual = TextHumanize::adjustTone("Formal text", target: 'casual');
```

### PHP Modules

| Module | PHP Class | Tests |
|:-------|:----------|:-----:|
| Core Pipeline | `TextHumanize`, `Pipeline` | âœ… |
| AI Detection | `AIDetector` | âœ… |
| Sentence Splitting | `SentenceSplitter` | âœ… |
| Paraphrasing | `Paraphraser` | âœ… |
| Tone Analysis | `ToneAnalyzer` | âœ… |
| Watermark Detection | `WatermarkDetector` | âœ… |
| Content Spinning | `ContentSpinner` | âœ… |
| Coherence Analysis | `CoherenceAnalyzer` | âœ… |
| Language Packs | 14 languages | âœ… |

```bash
cd php/
composer install
php vendor/bin/phpunit  # 223 tests, 825 assertions
```

See [php/README.md](php/README.md) for full PHP documentation.

---

## What's New in v0.8.0

A summary of everything added since v0.5.0:

### v0.8.0 â€” Style Presets, Auto-Tuner, Semantic Guards

| Feature | Description |
|:--------|:------------|
| ðŸŽ­ Style Presets | 5 personas: student, copywriter, scientist, journalist, blogger |
| ðŸ“Š Auto-Tuner | Feedback loop â€” learns optimal intensity from history |
| ðŸ›¡ï¸ Semantic Guards | Echo check prevents introducing duplicate words; 20+ context patterns |
| âš¡ Typography fast path | AI â‰¤ 5% â†’ skip all semantic stages, apply typography only |
| ðŸŸ¦ JS/TS full pipeline | Normalizer, Debureaucratizer, Naturalizer â€” full adaptive pipeline |
| ðŸ“– Documentation | API Reference, 14-recipe Cookbook, updated README |
| ðŸ‡©ðŸ‡ª German expanded | Bureaucratic 22â†’64, synonyms 26â†’45, AI words 20â†’38 |
| ðŸ”§ change_ratio fix | SequenceMatcher replaces broken positional comparison |
| â™»ï¸ Graduated retry | Pipeline retries at Ã—0.4, Ã—0.15 instead of full rollback |

### v0.7.0 â€” AI Detection 2.0, C2PA, Streaming

| Feature | Description |
|:--------|:------------|
| ðŸ§  13th metric | Perplexity score (character-level trigram model) |
| ðŸŽ¯ Ensemble boosting | 3-classifier aggregation: weighted + strong signal + majority |
| ðŸ“ˆ Benchmark suite | 11 labeled samples, 100% accuracy |
| ðŸ”Œ CLI `detect` | `texthumanize detect file.txt --verbose --json` |
| ðŸ“¡ Streaming callback | `on_progress(index, total, result)` for batch processing |
| ðŸ·ï¸ C2PA watermarks | Detect content provenance markers (C2PA, IPTC, XMP) |
| ðŸ—£ï¸ Tone: 4 new langs | UK, DE, FR, ES tone replacement pairs |
| ðŸ“Š Zipf rewrite | Log-log regression with RÂ² goodness-of-fit |

### v0.6.0 â€” Batch Processing, Quality Metrics, 99% Coverage

| Feature | Description |
|:--------|:------------|
| ðŸ“¦ Batch processing | `humanize_batch()` with unique seeds per text |
| ðŸ“ Quality score | Balances sufficient change with meaning preservation |
| ðŸ“ Similarity metric | Jaccard similarity (0..1) original vs processed |
| ðŸ§ª 1,255 Python tests | Up from 500, 99% coverage |
| ðŸ˜ 223 PHP tests | Up from 30, covering all modules |
| ðŸ”’ mypy clean | 0 type errors across all 38 source files |

### v0.5.0 â€” Code Quality, Pre-commit, PEP 561

| Feature | Description |
|:--------|:------------|
| ðŸ§¹ 0 lint errors | 67 ruff errors fixed |
| âœ… PEP 561 | `py.typed` marker for downstream type checkers |
| ðŸª Pre-commit hooks | Ruff lint/format, trailing whitespace, YAML/TOML checks |
| ðŸ”¬ conftest.py | 12 reusable pytest fixtures |

---

## Code Quality & Tooling

### Linting

TextHumanize enforces strict code quality with [ruff](https://github.com/astral-sh/ruff):

```bash
# Check all code (0 errors)
ruff check texthumanize/

# Auto-fix safe issues
ruff check --fix texthumanize/
```

Rules enabled: `E` (pycodestyle), `F` (Pyflakes), `W` (warnings), `I` (isort). Line length: 100 chars.

### Type Checking

PEP 561 compliant â€” ships `py.typed` marker for downstream type checkers:

```bash
mypy texthumanize/
```

Configuration in `pyproject.toml`:
- `python_version = "3.9"` â€” minimum supported version
- `check_untyped_defs = true` â€” checks function bodies even without annotations
- `warn_return_any = true` â€” warns on `Any` return types

### Pre-commit Hooks

Automatic quality checks on every commit:

```bash
pre-commit install        # one-time setup
pre-commit run --all-files # manual run
```

Hooks configured:
- Trailing whitespace removal
- End-of-file fixer
- YAML/TOML validation
- Large file prevention
- Merge conflict detection
- Ruff lint + format check

### CI/CD Pipeline

GitHub Actions runs on every push/PR:

| Step | Description |
|------|-------------|
| **Lint** | `ruff check` â€” zero errors enforced |
| **Test** | `pytest` across Python 3.9â€“3.12 + PHP 8.1â€“8.3 |
| **Coverage** | `pytest-cov` â€” 85% minimum |
| **Types** | `mypy` on Python 3.12 (non-blocking) |

---

## FAQ & Troubleshooting

### General

**Q: Does TextHumanize use the internet?**
No. All processing is 100% local. No API calls, no data sent anywhere.

**Q: Does it require GPU or large models?**
No. Pure algorithmic processing using Python standard library only. Starts in <100ms.

**Q: What makes it better than online humanizers?**
Speed (56K chars/sec vs 2-10 seconds), privacy (offline), control (intensity, profiles, seeds), and it's free.

**Q: Which Python versions are supported?**
Python 3.9 through 3.12+ (tested in CI/CD matrix).

### Processing

**Q: My text isn't changing much. Why?**
Increase `intensity` (e.g., 80-100) or use a more aggressive profile like `chat`. The `seo` and `formal` profiles intentionally make fewer changes. Also check if the text already has a low AI score â€” the adaptive pipeline deliberately reduces changes for natural text.

**Q: How do I target a specific writing style?**
Use `target_style="student"` (or `copywriter`, `scientist`, `journalist`, `blogger`). You can also extract a custom fingerprint from your writing sample with `StylisticAnalyzer`.

**Q: Can I undo changes?**
The `explain(result)` function shows all changes. The original text is always in `result.original`.

**Q: How do I protect specific words from changing?**
Use `constraints={"keep_keywords": ["word1", "word2"]}` or `preserve={"brand_terms": ["Brand"]}`.

### AI Detection

**Q: How accurate is the AI detector?**
100% on our benchmark (11 samples: 5 AI, 5 human, 1 mixed). Uses 13 independent metrics with ensemble boosting. Best results with 100+ words.

**Q: Does it detect ChatGPT/GPT-4/Claude?**
It detects statistical patterns common to all LLMs, not any specific model. Works for GPT-3.5, GPT-4, Claude, Gemini, Llama, etc.

**Q: Can I use the detector and humanizer together?**
Yes â€” the typical pipeline is: detect (score high) â†’ humanize â†’ detect again (score low).

### Languages

**Q: My language isn't supported.**
Use `lang="xx"` â€” the universal processor handles typography, sentence variation, and burstiness without dictionaries. Adding a full language pack is easy â€” just create a file in `texthumanize/lang/`.

### API

**Q: How do I start the REST API?**
```bash
python -m texthumanize.api --port 8080
```

---

## Contributing

Contributions are welcome:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/my-feature`
3. Write tests for new functionality
4. Ensure all tests pass: `pytest`
5. Commit changes: `git commit -m 'Add my feature'`
6. Push: `git push origin feature/my-feature`
7. Open a Pull Request

### Areas for Improvement

- **Dictionaries** â€” expand bureaucratic and synonym dictionaries for all languages
- **Languages** â€” add new language packs (Japanese, Chinese, Arabic, Korean, etc.)
- **Tests** â€” more edge cases and golden tests, push coverage past 90%
- **Documentation** â€” tutorials, video walkthroughs, blog posts
- **Ports** â€” Node.js, Go, Rust implementations
- **API** â€” WebSocket support, authentication, rate limiting
- **Morphology** â€” expand to more languages (FR, ES, PL, PT, IT)
- **AI Detector** â€” larger benchmark suite, more metrics

### Development Setup

```bash
git clone https://github.com/ksanyok/TextHumanize.git
cd TextHumanize
python -m venv .venv
source .venv/bin/activate
pip install -e ".[dev]"
pre-commit install
ruff check texthumanize/
pytest --cov=texthumanize
```

---

## Security & Limits

### Input Limits

| Parameter | Default | Configurable | Notes |
|:----------|:-------:|:------------:|:------|
| Max input length | 500 KB | Yes (`max_input_size`) | Texts above this limit should be processed via chunk API |
| Max sentence length | 5,000 chars | Internal | Sentences exceeding this are passed through unchanged |
| Max paragraph count | None | â€” | No hard limit; memory usage scales linearly |

### Resource Consumption

- **Memory**: ~2.5 MB peak for a 10 KB text; scales linearly with input size  
- **CPU**: Single-threaded, no background workers or child processes  
- **Disk**: Zero disk I/O during processing (all dictionaries are in-memory)  
- **Network**: Zero network calls. Ever. No telemetry, no analytics, no phone-home  

### Regex Safety (ReDoS)

All regular expressions in the library are:

1. **Bounded** â€” no unbounded repetitions on overlapping character classes  
2. **Linear-time** â€” worst-case O(n) execution for any input string  
3. **Fuzz-tested** â€” CI runs property-based tests with random Unicode strings up to 100 KB  

No user input is ever compiled into a regex pattern.

### Sandboxing Recommendations

For production deployments processing untrusted input:

```python
import resource, signal

# Limit memory to 256 MB
resource.setrlimit(resource.RLIMIT_AS, (256 * 1024 * 1024, 256 * 1024 * 1024))

# Limit CPU time to 10 seconds per call
signal.alarm(10)

result = humanize(untrusted_text, lang="en")

signal.alarm(0)  # Cancel alarm after success
```

### Threat Model

| Threat | Mitigation |
|:-------|:-----------|
| Denial of service via large input | Use chunk API or enforce `max_input_size` |
| ReDoS via crafted patterns | All regexes are linear-time; no user input compiled to regex |
| Data exfiltration | Zero network calls; all processing is local |
| Supply-chain attack | Zero runtime dependencies; pure stdlib |
| Non-deterministic output in audit | Seed-based PRNG guarantees reproducibility |

### Testing & Quality Assurance

- **1,584 tests** across Python, PHP, and TypeScript  
- **99% code coverage** (Python)  
- **Property-based fuzzing** with random Unicode, empty strings, extremely long inputs  
- **Golden tests** â€” reference outputs checked against known-good baselines  
- **CI/CD** â€” ruff linting + mypy type checking on every commit  

---

## For Business & Enterprise

TextHumanize is designed for production use in corporate environments:

| Corporate Requirement | How TextHumanize Delivers |
|:----------------------|:-------------------------|
| **Predictability** | Seed-based PRNG â€” same input + seed = identical output. Always. |
| **Privacy & Security** | 100% local processing. Zero network calls. No data leaves your server. |
| **Auditability** | Every call returns `change_ratio`, `quality_score`, `similarity`, and a full `explain()` report of what was changed and why. |
| **Modes** | `normalize` (typography only) Â· `style_soft` (mild humanization) Â· `rewrite` (full pipeline). Control via `intensity` (0â€“100) and `profile` (9 options). |
| **Integration** | Python SDK Â· TypeScript/JavaScript SDK Â· PHP SDK Â· CLI Â· REST API. Drop into any pipeline. |
| **Reliability** | 1,584 tests across 3 platforms, 99% code coverage, CI/CD with ruff + mypy. |
| **No vendor lock-in** | Zero dependencies. Pure stdlib. No cloud APIs, no API keys, no rate limits. |
| **Language coverage** | 9 full language packs + universal statistical processor for any language. |
| **Licensing** | Clear dual license. [Commercial tiers from $199/year â†’](COMMERCIAL.md) |

### Processing Modes

```python
# Mode 1: Typography only (normalize) â€” safest, no semantic changes
result = humanize(text, intensity=5)  # Only fixes quotes, dashes, spaces

# Mode 2: Soft style (style_soft) â€” light humanization
result = humanize(text, intensity=30, profile="docs")

# Mode 3: Full rewrite â€” maximum humanization
result = humanize(text, intensity=80, profile="web")

# Every mode returns an audit trail
print(result.change_ratio)   # What % was changed
print(result.quality_score)  # Quality metric
print(explain(result))       # Detailed diff report
```

---

## Support the Project

If you find TextHumanize useful, consider supporting the development:

[![PayPal](https://img.shields.io/badge/PayPal-Donate-blue.svg?logo=paypal)](https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=ksanyok%40me.com&item_name=TextHumanize&currency_code=USD)

- Star the repository
- Report bugs and suggest features
- Improve documentation
- Add language packs

---

## License & Pricing

TextHumanize uses a **dual license model**:

| Use Case | License | Cost |
|:---------|:--------|:----:|
| Personal projects | Free License | **Free** |
| Academic / Research | Free License | **Free** |
| Open-source (non-commercial) | Free License | **Free** |
| Evaluation / Testing | Free License | **Free** |
| Commercial â€” 1 dev, 1 project | Indie | **$199/year** |
| Commercial â€” up to 5 devs | Startup | **$499/year** |
| Commercial â€” up to 20 devs | Business | **$1,499/year** |
| Enterprise / On-prem / SLA | Enterprise | [Contact us](mailto:ksanyok@me.com) |

All commercial licenses include full source code, updates for 1 year, and email support.

ðŸ‘‰ **[Full licensing details & FAQ â†’](COMMERCIAL.md)**

See [LICENSE](LICENSE) for the complete legal text.

**Contact:** [ksanyok@me.com](mailto:ksanyok@me.com)

---

<p align="center">
  <a href="https://github.com/ksanyok/TextHumanize">GitHub</a> Â·
  <a href="https://github.com/ksanyok/TextHumanize/issues">Issues</a> Â·
  <a href="https://github.com/ksanyok/TextHumanize/discussions">Discussions</a> Â·
  <a href="COMMERCIAL.md">Commercial License</a>
</p>
